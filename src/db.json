{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"source/images/ALM.png","path":"images/ALM.png","modified":0,"renderable":0},{"_id":"source/images/BuildTasks.png","path":"images/BuildTasks.png","modified":0,"renderable":0},{"_id":"source/images/AddNewToken.png","path":"images/AddNewToken.png","modified":0,"renderable":0},{"_id":"source/images/BadDesign.png","path":"images/BadDesign.png","modified":0,"renderable":0},{"_id":"source/images/Donald.png","path":"images/Donald.png","modified":0,"renderable":0},{"_id":"source/images/3simplerules.jpg","path":"images/3simplerules.jpg","modified":0,"renderable":0},{"_id":"source/images/Chocolatey.jpg","path":"images/Chocolatey.jpg","modified":0,"renderable":0},{"_id":"source/images/GettingToWorkArea.png","path":"images/GettingToWorkArea.png","modified":0,"renderable":0},{"_id":"source/images/MyNewToken.png","path":"images/MyNewToken.png","modified":0,"renderable":0},{"_id":"source/images/NewButton.png","path":"images/NewButton.png","modified":0,"renderable":0},{"_id":"source/images/Done.jpg","path":"images/Done.jpg","modified":0,"renderable":0},{"_id":"source/images/NewIterationChild.png","path":"images/NewIterationChild.png","modified":0,"renderable":0},{"_id":"source/images/RedGateInstalled.png","path":"images/RedGateInstalled.png","modified":0,"renderable":0},{"_id":"source/images/OpenProfile.png","path":"images/OpenProfile.png","modified":0,"renderable":0},{"_id":"source/images/NoTesting.jpg","path":"images/NoTesting.jpg","modified":0,"renderable":0},{"_id":"source/images/SaveNewIteration.png","path":"images/SaveNewIteration.png","modified":0,"renderable":0},{"_id":"source/images/TestResults.png","path":"images/TestResults.png","modified":0,"renderable":0},{"_id":"source/images/TFSLogo.jpg","path":"images/TFSLogo.jpg","modified":0,"renderable":0},{"_id":"source/images/LackOfTesting.jpg","path":"images/LackOfTesting.jpg","modified":0,"renderable":0},{"_id":"source/images/WebDeploy.jpg","path":"images/WebDeploy.jpg","modified":0,"renderable":0},{"_id":"source/images/SoapBox.jpg","path":"images/SoapBox.jpg","modified":0,"renderable":0},{"_id":"source/images/attack.jpg","path":"images/attack.jpg","modified":0,"renderable":0},{"_id":"source/images/go-vegan.png","path":"images/go-vegan.png","modified":0,"renderable":0},{"_id":"source/images/changed.jpg","path":"images/changed.jpg","modified":0,"renderable":0},{"_id":"source/images/internal.png","path":"images/internal.png","modified":0,"renderable":0},{"_id":"source/images/master_branch.png","path":"images/master_branch.png","modified":0,"renderable":0},{"_id":"source/images/hexo.jpg","path":"images/hexo.jpg","modified":0,"renderable":0},{"_id":"source/images/git-logo.jpg","path":"images/git-logo.jpg","modified":0,"renderable":0},{"_id":"source/images/unicorn.jpg","path":"images/unicorn.jpg","modified":0,"renderable":0},{"_id":"source/images/version.jpg","path":"images/version.jpg","modified":0,"renderable":0},{"_id":"source/images/redgate.jpg","path":"images/redgate.jpg","modified":0,"renderable":0},{"_id":"source/images/vstsLogo.png","path":"images/vstsLogo.png","modified":0,"renderable":0},{"_id":"source/images/stackofpapers.jpg","path":"images/stackofpapers.jpg","modified":0,"renderable":0},{"_id":"source/images/waterfall.jpg","path":"images/waterfall.jpg","modified":0,"renderable":0},{"_id":"source/images/BuildThroughEnvironment.png","path":"images/BuildThroughEnvironment.png","modified":0,"renderable":0},{"_id":"source/images/DonaldFunny.jpg","path":"images/DonaldFunny.jpg","modified":0,"renderable":0},{"_id":"source/images/NewToDone.png","path":"images/NewToDone.png","modified":0,"renderable":0},{"_id":"source/images/startreck.jpg","path":"images/startreck.jpg","modified":0,"renderable":0},{"_id":"themes/hexo-theme-bootstrap-blog/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/callouts.css","path":"css/callouts.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/hexo-base.css","path":"css/hexo-base.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/highlight-js.css","path":"css/highlight-js.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/custom.css","path":"css/custom.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/share-box.css","path":"css/share-box.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/styles.styl","path":"css/styles.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"source/images/Forks_Over_Knives_movie_poster.png","path":"images/Forks_Over_Knives_movie_poster.png","modified":0,"renderable":0},{"_id":"source/images/Donald_2019.jpg","path":"images/Donald_2019.jpg","modified":0,"renderable":0},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"source/images/V__FD25.jpg","path":"images/V__FD25.jpg","modified":0,"renderable":0},{"_id":"source/images/Lumina-950XL.png","path":"images/Lumina-950XL.png","modified":0,"renderable":0}],"Cache":[{"_id":"themes/hexo-theme-bootstrap-blog/LICENSE","hash":"30328fabb2124d0748e58efe8ce363f7a6414640","modified":1578184565148},{"_id":"source/favicon.ico","hash":"1294ddcb25e1fceb6c2b51c23d4e3c65480d241d","modified":1578184565109},{"_id":"themes/hexo-theme-bootstrap-blog/README.md","hash":"ba9b2ea72c7992b0dc0b422719b62fb98a0c216f","modified":1578184565149},{"_id":"source/About-Me/index.md","hash":"881c477741ffb81f02a1c04306557bd92f45cd0c","modified":1578184564963},{"_id":"source/_posts/A-New-Start-on-an-Old-Blog.md","hash":"db81787b7d5d49c5799dc87f6def9916c96a2566","modified":1578184564964},{"_id":"source/_posts/C-net-or-VB-net.md","hash":"7bdcf27ad823f1a3348a976a6ce0d5540f46516b","modified":1578184564968},{"_id":"source/_posts/An-Argument-against-the-Date-Based-Version-Number.md","hash":"2e64593bef5e4e6d512e42a6a417a5d7924a051c","modified":1578184564968},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile.md","hash":"ee74b373d7c1b93e98445aebc5a52352f1bbb409","modified":1578184564969},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning.md","hash":"5cb5ff19c30dc16eece58a3fd70a37bfa57dce77","modified":1578184564983},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment.md","hash":"6d0ba54f3e9a07953580ec4dee901fc19ef7c4e8","modified":1578184565000},{"_id":"source/_posts/Goal-Tracking.md","hash":"285c2ccfa17be098c6b1b591530083bd8eee4870","modified":1578184565009},{"_id":"themes/hexo-theme-bootstrap-blog/_config.yml","hash":"96c47bec2cb1d43914f52e32535ed61855a1ef71","modified":1578184565149},{"_id":"source/_posts/How-I-Use-Chocolatey-in-my-Releases.md","hash":"d04056b3a6b5b45fc4c2d7deaa3013f772a1b5c7","modified":1578184565012},{"_id":"source/_posts/Let-the-Test-Plan-Tell-the-Story.md","hash":"04d404c379c5e2ad6a479c09240adfae76a59dcf","modified":1578184565014},{"_id":"source/_posts/Living-on-a-Vegan-Diet.md","hash":"4d066a69c9d1d76299dfd5f98846ff709b5bc2b7","modified":1578184565018},{"_id":"source/_posts/Linking-the-Iterations-to-all-your-Teams.md","hash":"07e2406715c8ff202949c0ebb8ab8ae72379846f","modified":1578184565018},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement.md","hash":"e8aa508f6a0af408d9db666f422c8bf85ba4e925","modified":1578184565019},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History.md","hash":"b9067aa32c7148947e94d8545a098818c6d06d2a","modified":1578184565023},{"_id":"source/_posts/My-Experience-with-Git-Sub-modules.md","hash":"48781b048f7e3e63880fb197bb68f3e14232875b","modified":1578184565027},{"_id":"source/_posts/My-New-3-Rules-for-Releases.md","hash":"783cdc5582f3c4b2fc2d2b3d3b22bb3f6c05c0e6","modified":1578184565028},{"_id":"source/_posts/No-no-he-s-not-dead-he-s-he-s-restin.md","hash":"85593c360723e52edf23b3f1faf651d76127b9ba","modified":1578184565030},{"_id":"source/_posts/One-Build-Definition-to-Support-Multiple-Branches.md","hash":"b0f9a965644fb13fde33e6afbbcda3125b15db11","modified":1578184565031},{"_id":"source/_posts/Security-Configuration-for-Teams.md","hash":"5303f2423cdfda740e9415e0951d948b5117e3f0","modified":1578184565038},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools.md","hash":"9d7aa5a6703604d066ad88a7d93a130c0ec24d0a","modified":1578184565033},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed.md","hash":"2982b0c069b715d5d29e9f867ba3d4316817f81d","modified":1578184565061},{"_id":"source/_posts/Some-MSDeploy-Tricks-I-ve-Learned.md","hash":"dcb2e506383d94076c60bfc5a271d1c2a654542b","modified":1578184565066},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s.md","hash":"5d026578854649fdc3c4ee89d771163a95366484","modified":1578184565069},{"_id":"source/_posts/The-Power-of-Time-Tracking.md","hash":"2ede5399dcda12464b0c2e1adeced8d0572f73e0","modified":1578184565103},{"_id":"source/_posts/The-Two-Opposite-IT-Agenda-s.md","hash":"9f1bd0441ef25352d71381615a248a231ce47723","modified":1578184565105},{"_id":"source/_posts/What-is-The-Web-We-Weave-Inc.md","hash":"805f412d4b3bebd74e470b58d0c01489858cfec3","modified":1578184565106},{"_id":"source/_posts/When-is-Waterfall-a-Good-Choice.md","hash":"b955446657c352cc02632b23ce042ecef9d8ce13","modified":1578184565107},{"_id":"source/_posts/When-Should-we-Move-the-Work-Items-to-DONE.md","hash":"0460e3bb1f78e6f2b137bc70ac3c635100c0fc19","modified":1578184565106},{"_id":"source/_posts/Who-left-the-Developers-in-the-design-room-and-who-is-testing-this-stuff.md","hash":"ba186479307e71c3ab7e36386cd27994c264e603","modified":1578184565108},{"_id":"source/_posts/Who-s-the-Boss.md","hash":"4351a431a906007874e172a4c43fb77eb2d1f81e","modified":1578184565108},{"_id":"source/images/ALM.png","hash":"efa1ddd6fb0a170053c2a129ffee1c5a12f0d618","modified":1578184565111},{"_id":"source/images/BuildTasks.png","hash":"4216bf97d8012a4d6413145a133bdd3aa324bd50","modified":1578184565113},{"_id":"source/images/AddNewToken.png","hash":"ccd484978fabce871a89cbda3ecc7b8db5aed102","modified":1578184565112},{"_id":"source/images/BadDesign.png","hash":"9746b2a260a57c50c11657893abe35f884ad618e","modified":1578184565113},{"_id":"source/images/Donald.png","hash":"2a980fd94a7d7be09d3dd7f6eae58db433b95554","modified":1578184565116},{"_id":"source/images/3simplerules.jpg","hash":"5f6e9486c560a8ce1e20864c9e427a865f7a243a","modified":1578184565111},{"_id":"source/images/Chocolatey.jpg","hash":"468b6217be1af8b9a55b825345b6348858fda723","modified":1578184565115},{"_id":"source/images/GettingToWorkArea.png","hash":"07df22038a5dae800268213c0d957234bca78908","modified":1578184565119},{"_id":"source/images/MyNewToken.png","hash":"525e9deafee6f540377a47589eb483c4568bc1d5","modified":1578184565123},{"_id":"source/images/NewButton.png","hash":"647d823d21483585cf09b61a1c7b1d75163494bb","modified":1578184565124},{"_id":"source/images/Done.jpg","hash":"595a34ece1fda60e39e4ac95cfe91411a80c200c","modified":1578184565117},{"_id":"source/images/NewIterationChild.png","hash":"1bbff0b3ce7af056afc17088bf8ef7ce9617dded","modified":1578184565125},{"_id":"source/images/RedGateInstalled.png","hash":"5b93b7d370eca6b10162e16b3259863302882f98","modified":1578184565129},{"_id":"source/images/OpenProfile.png","hash":"bd5216c305376b9569c6e121d8425ef4fb942c84","modified":1578184565128},{"_id":"source/images/NoTesting.jpg","hash":"86eb6c3623044fe0b9b5c078c85ed60874d973a9","modified":1578184565127},{"_id":"source/images/SaveNewIteration.png","hash":"52f2180e360b471b0c308296b4ac6d1a0b3b315b","modified":1578184565130},{"_id":"source/images/TestResults.png","hash":"10e5037602163b3a4898eda03ec72b8a599d29c3","modified":1578184565132},{"_id":"source/images/TFSLogo.jpg","hash":"7710f4668428d01102d7f8f95cb297aae4bb3631","modified":1578184565131},{"_id":"source/images/LackOfTesting.jpg","hash":"a0e0726f0cb1e17b00c02659d8031af3bc72fe94","modified":1578184565120},{"_id":"source/images/WebDeploy.jpg","hash":"f2bd9557115ff7189a2f7c313c9ed3caebb9ec0c","modified":1578184565136},{"_id":"source/images/SoapBox.jpg","hash":"f2525688d839689cc6720bb8c273939db7805ef6","modified":1578184565130},{"_id":"source/images/attack.jpg","hash":"16922585a221bd8a78ce54dd12b5c0ec966ec69c","modified":1578184565136},{"_id":"source/images/go-vegan.png","hash":"f086fc74fdcb77614bb3a17ae4bab71f4caf0e4a","modified":1578184565139},{"_id":"source/images/changed.jpg","hash":"e05d00e7adc3b4830849fb9840ac823449e5d289","modified":1578184565137},{"_id":"source/images/internal.png","hash":"9f5bae534921a5cc00b679fccf9e7d3bbbabf151","modified":1578184565141},{"_id":"source/images/master_branch.png","hash":"3e65b386067058ed8e7ff1522ebd7d0f8b8b4681","modified":1578184565142},{"_id":"source/images/hexo.jpg","hash":"b14bb01ec6a92caa53b4fc628d8d578815683f53","modified":1578184565140},{"_id":"source/images/git-logo.jpg","hash":"2a5236a764cb352fa5ca1990bd89fc41325c3fec","modified":1578184565138},{"_id":"source/images/unicorn.jpg","hash":"8801bdeaa462161281042a35bf3d43c0720aac01","modified":1578184565145},{"_id":"source/images/version.jpg","hash":"a1a4eb9c8e21ac48cbdadba16394862cc2167a12","modified":1578184565146},{"_id":"source/images/redgate.jpg","hash":"849b5dde0e0f2090e0671ee1d2b7739879ac3c62","modified":1578184565142},{"_id":"source/images/vstsLogo.png","hash":"ad1a5c7c0242a97a86fee06e5e117e8b7340472d","modified":1578184565146},{"_id":"source/images/stackofpapers.jpg","hash":"f5f3d69c953e6d60e387cca188a7fede3a9f60b8","modified":1578184565143},{"_id":"themes/hexo-theme-bootstrap-blog/languages/ru.yml","hash":"bd842b6b237bd349e29a0c6935426413974a30aa","modified":1578184565151},{"_id":"themes/hexo-theme-bootstrap-blog/languages/default.yml","hash":"feb1dd022dc8897d65baa5b927de2a3d4178d798","modified":1578184565150},{"_id":"source/images/waterfall.jpg","hash":"770590863a4482962ec83570aaa89e44ef6a20d4","modified":1578184565147},{"_id":"themes/hexo-theme-bootstrap-blog/languages/no.yml","hash":"d9a1b34d590f94ab5c03809754e62bc4cf0e8d0f","modified":1578184565150},{"_id":"themes/hexo-theme-bootstrap-blog/scripts/callout.js","hash":"0d80bf4d275c8927a81abf3ab13be167f40060a8","modified":1578184565169},{"_id":"themes/hexo-theme-bootstrap-blog/languages/zh-CN.yml","hash":"26c2e94093b2b6228c0f26be5f9f238daa6c5d1c","modified":1578184565152},{"_id":"themes/hexo-theme-bootstrap-blog/scripts/bs_paginator.js","hash":"b3a84f4758dc3e92c928dc648ab6f17754b9caca","modified":1578184565168},{"_id":"themes/hexo-theme-bootstrap-blog/scripts/fancybox.js","hash":"d3f846104c45541f7a45702f937d540dd988a817","modified":1578184565169},{"_id":"themes/hexo-theme-bootstrap-blog/layout/archive.ejs","hash":"ae0149112760550ec5b5791aed25a9f558008969","modified":1578184565165},{"_id":"themes/hexo-theme-bootstrap-blog/languages/zh-TW.yml","hash":"b2474b775a8fa0fa3e9e3c58ddb11b20cf65dbc5","modified":1578184565152},{"_id":"themes/hexo-theme-bootstrap-blog/layout/category.ejs","hash":"6bb634b555083f54904536b0f159c7b3e8febdd7","modified":1578184565165},{"_id":"themes/hexo-theme-bootstrap-blog/layout/page.ejs","hash":"f93d8ea38272ca1956134f13760a5a59a01657cf","modified":1578184565166},{"_id":"themes/hexo-theme-bootstrap-blog/layout/index.ejs","hash":"ec498c6c0606acde997ce195dad97b267418d980","modified":1578184565165},{"_id":"themes/hexo-theme-bootstrap-blog/layout/post.ejs","hash":"f93d8ea38272ca1956134f13760a5a59a01657cf","modified":1578184565167},{"_id":"source/images/BuildThroughEnvironment.png","hash":"1810b90eb35806ff42076736e9c8750b3eec1882","modified":1578184565115},{"_id":"source/images/DonaldFunny.jpg","hash":"759a4dbfc807ad14f67f8288dc28e344acd4f59a","modified":1578184565117},{"_id":"source/images/NewToDone.png","hash":"0b327bee10fdb6ff6d0bd4e963d8a4fbae2144e9","modified":1578184565127},{"_id":"themes/hexo-theme-bootstrap-blog/layout/tag.ejs","hash":"8c37543c3fb6cbb333f7942f9f9bb8d9c03fb3c2","modified":1578184565167},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image10.png","hash":"978a97cffd4e33b4bec787c7fc448fe6fb2eb58e","modified":1578184564972},{"_id":"source/_posts/A-New-Start-on-an-Old-Blog/hexo.jpg","hash":"b14bb01ec6a92caa53b4fc628d8d578815683f53","modified":1578184564967},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image2.png","hash":"e5c8056c7393755ec32369469a479d10365f96b0","modified":1578184564976},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image4.png","hash":"c9e78eaee3a4573548da2a03c3cf295cae3f0823","modified":1578184564979},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image6.png","hash":"fd613828d8d5a6ade8026b5b2da45731127732e2","modified":1578184564981},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image8.png","hash":"b0cf579c0f4ef3d8fc5b6156e8eae41a997bd945","modified":1578184564982},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image7.png","hash":"08fbbe9d210629ddd6ff70ec1a27a613bbd4cbaf","modified":1578184564981},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image9.png","hash":"a85e5fec5d522bba8bc8197bb28efb3fa40d36aa","modified":1578184564983},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image10.png","hash":"853e65bac63e32080f70e248298ada83b6874e22","modified":1578184564986},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image12.png","hash":"97fa9f90b5283d47b46ecbaf0e7b37a1e0b293d2","modified":1578184564987},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image11.png","hash":"24d30d4a4ccb4789e26de4ee82d016200b978c90","modified":1578184564986},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image14.png","hash":"6c890ecbea2f800bec8b79247952ffc2bac94b66","modified":1578184564990},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image13.png","hash":"4c52dd07b109d7133b8f9cecd03df322aac1f2fd","modified":1578184564988},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image15.png","hash":"e2970cd88b9d15eaf5b7dca6fcb4de946535a7b5","modified":1578184564991},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image5.png","hash":"2bf6480b9191a7166f05268533217c69b697a2ae","modified":1578184564995},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image4.png","hash":"d8f90208943dc42b2346e9f617b4afd1932239ae","modified":1578184564994},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image7.png","hash":"b50fca5237aa05aed172629d28fb0cd7452a394a","modified":1578184564997},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image9.png","hash":"3ade4b8135cd1b24590771269ee418328248022d","modified":1578184564999},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/IISWebAppDeployJsonTask.png","hash":"a924106970eb50ebb4e66db4cb21d208a8317668","modified":1578184565002},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/Json.png","hash":"977a977de39fb163cb798b38056c28559bd87193","modified":1578184565004},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/variable-json.png","hash":"be02db2410d0e8a009f4508294cee18fd952c395","modified":1578184565005},{"_id":"source/_posts/How-I-Use-Chocolatey-in-my-Releases/FileStructure.png","hash":"794861afb9169290c308aab87125ba71483e6e4b","modified":1578184565014},{"_id":"themes/hexo-theme-bootstrap-blog/layout/layout.ejs","hash":"ca305473e090c6fc606706c986c37a0e000cbd54","modified":1578184565166},{"_id":"source/_posts/Goal-Tracking/image.png","hash":"d9803ffef0fc3bb5ee9546a59e646b204b55d2ab","modified":1578184565011},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/ConfigureScreen.png","hash":"a5a8be753f70eab469b28d510081ce9d24cdb8a1","modified":1578184565021},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/FinishResults.png","hash":"0e8c465abd7a580f69f1de60e75d5134d63de123","modified":1578184565024},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/ConditionOption.png","hash":"fd3717f256911752012299bb337ddaa912923fb8","modified":1578184565020},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/NewReleaseEditor.png","hash":"9c1a9afd68cd295442434f0eae7600415f2bb5a9","modified":1578184565022},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/NewRepoDialog.png","hash":"4354309bd4810e9f2f9cc8a2b33b154ca1494b20","modified":1578184565025},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/TFSNewRepo.png","hash":"3a6b356e27d78b15ae66e390b5a481c1d1a676dc","modified":1578184565026},{"_id":"source/_posts/One-Build-Definition-to-Support-Multiple-Branches/CIBranchFilters.png","hash":"c134d4219a7c5bc47085ad91957d51ee38d9f35f","modified":1578184565033},{"_id":"source/_posts/My-New-3-Rules-for-Releases/MasterBranchOnly.png","hash":"2e347939343a9bc0330083c348f05a531368d84f","modified":1578184565029},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/DLMBuildTask.png","hash":"429b563b97ca1a6161d462345f3491a9cebf472c","modified":1578184565035},{"_id":"source/_posts/No-no-he-s-not-dead-he-s-he-s-restin/blog_grim_reaper.gif","hash":"0fbd3ae8862f9ec2509f2d538f177fb9d64095c2","modified":1578184565031},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/ImportDb.png","hash":"bec3fdda43ec630c7fc45cb8bca15b7c7edcabfc","modified":1578184565036},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/pushRepoCopyButton.png","hash":"42dce02d3c7f921a92c974c1638411598571b953","modified":1578184565038},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/NewWidgitRepo.png","hash":"98a9e1833807b2b692ef7248aa6c0e1cff9be072","modified":1578184565036},{"_id":"source/_posts/Security-Configuration-for-Teams/10.png","hash":"3d286df2ebd6c3c2f07c3c351ae162350ae01948","modified":1578184565041},{"_id":"source/_posts/Security-Configuration-for-Teams/11.png","hash":"77f194c0b83bc02066ddea84a7219d2e41d80167","modified":1578184565042},{"_id":"source/_posts/Security-Configuration-for-Teams/12.png","hash":"288cae5cf899160019c6b6ee2e5f3cbe2d3000de","modified":1578184565043},{"_id":"source/_posts/Security-Configuration-for-Teams/13.png","hash":"261aa89ef80bae02b8493d59dbc4f10397345110","modified":1578184565044},{"_id":"source/_posts/Let-the-Test-Plan-Tell-the-Story/testImpactResults.jpg","hash":"1e56aab0786c573f9751a8c4af3e7dc2430207e8","modified":1578184565017},{"_id":"source/_posts/Security-Configuration-for-Teams/14.png","hash":"d254895439d54b0c32ba1a4a753808d6d1d18cfa","modified":1578184565045},{"_id":"source/_posts/Security-Configuration-for-Teams/17.png","hash":"bd6f9e7aef5e4c0f19634d6b81de2ada10ba62b2","modified":1578184565048},{"_id":"source/_posts/Security-Configuration-for-Teams/15.png","hash":"d877b659d241c060a23c6ece74b4668e1bb2035e","modified":1578184565046},{"_id":"source/_posts/Security-Configuration-for-Teams/16.png","hash":"f0e585808fef8021a7a8c386ed39106367374ad7","modified":1578184565047},{"_id":"source/_posts/Security-Configuration-for-Teams/18.png","hash":"c17c41ea68cd5f88f5a7a4d7dbf46f5260d64676","modified":1578184565049},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/webconfig.jpg","hash":"63dba06c3e7d99318cb9caddb4523b5b36104555","modified":1578184565009},{"_id":"source/_posts/Security-Configuration-for-Teams/19.png","hash":"c14db9beb3c5b0d4e41df8f5406e31229df4ce80","modified":1578184565049},{"_id":"source/_posts/Security-Configuration-for-Teams/21.png","hash":"bb918270b79e8d9f1e9eda298a78a12db9fa97c2","modified":1578184565052},{"_id":"source/_posts/Security-Configuration-for-Teams/2.png","hash":"fa3647c714d1f42e1b79929e0a97018fa99ee4a8","modified":1578184565050},{"_id":"source/_posts/Security-Configuration-for-Teams/20.png","hash":"5ee65cb8431fcf499b1e7683b085000f8f1163e5","modified":1578184565051},{"_id":"source/_posts/Security-Configuration-for-Teams/22.png","hash":"a8e135791987e0b95f5724f4428f28f8437d345e","modified":1578184565053},{"_id":"source/_posts/Security-Configuration-for-Teams/23.png","hash":"445e6dfe3509cdf557785640276e27ab0ee3e6a7","modified":1578184565054},{"_id":"source/_posts/Security-Configuration-for-Teams/3.png","hash":"bb2bddbe4a6d9c6d1d256ecd889daa8086ef3535","modified":1578184565055},{"_id":"source/_posts/Security-Configuration-for-Teams/4.png","hash":"03a3b1eaf88dff1df8790250e078885fa4bf8988","modified":1578184565056},{"_id":"source/_posts/Security-Configuration-for-Teams/6.png","hash":"d4325a4dc7995326d6617f41c6bb4cd1ca87d766","modified":1578184565058},{"_id":"source/_posts/Security-Configuration-for-Teams/8.png","hash":"ad5d8763df808ba81bec67f313414bd233a91d27","modified":1578184565060},{"_id":"source/_posts/Security-Configuration-for-Teams/7.png","hash":"16eef7eeed511ed835db2a0409cb5f2a160f7550","modified":1578184565059},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/GearIcon.png","hash":"67655f165673f10cd057b498ea9d06b6b89b8e4d","modified":1578184565062},{"_id":"source/_posts/Security-Configuration-for-Teams/9.png","hash":"44dd02799e387614fe78b958ff6ace461470e5e0","modified":1578184565061},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NewNotification.png","hash":"5faacb4376649201b11c6ea27ef291f720ab0ef6","modified":1578184565063},{"_id":"source/_posts/Some-MSDeploy-Tricks-I-ve-Learned/CreatePackageFromManifest.png","hash":"85bffd9b1b589fa7078ec8f4d1e851339fae9d34","modified":1578184565067},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/Notifications.png","hash":"bc39de74379a5bb1b4712a25ae82c171d954531e","modified":1578184565065},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NotificationDetails.png","hash":"e86f05fafe86b51e9f5ee6389ca3851d08903f76","modified":1578184565064},{"_id":"source/_posts/Some-MSDeploy-Tricks-I-ve-Learned/DeployWebsiteAzure.png","hash":"587dddca5a734891571a6785f175938fd2097671","modified":1578184565068},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AddTeamButton.jpg","hash":"6198a9db290993ff875d2aca6aa5811a17421610","modified":1578184565071},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AreaDropDown.jpg","hash":"eb27e7718dbcf86f129dbb90aa15aed0a241969d","modified":1578184565071},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AreasLink.jpg","hash":"53501ebd83c50692c8e928a1c02db5a8bb194ac5","modified":1578184565072},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickAreas.jpg","hash":"9a203ba35a09a03b0bc9877c4fe0d8646f8afd5d","modified":1578184565077},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickSelectAreasButton.jpg","hash":"3d576aa21607aebb799c590341ebb4e2d9740750","modified":1578184565078},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ChooseBug.jpg","hash":"746e1a005edd0cffe3dd54acbf2ddd78bd98b947","modified":1578184565076},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/BoardsToWorkItems.jpg","hash":"b5c469915f66b9967257bc71fd518cd9101bc1bf","modified":1578184565075},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg","hash":"61c637492f7e3c8bd513ee1e61b85615d7a12874","modified":1578184565079},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickSaveAndClose.jpg","hash":"c329e1980d3a5b3d119df9953470f63665990fe6","modified":1578184565077},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickTeam-1Area.jpg","hash":"1d593f0d44029691b6493c6444e7ba38b1f7fc3a","modified":1578184565079},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateTheBug.jpg","hash":"fc0904837c1cebf57ef4d843205d385d1f6ede8c","modified":1578184565084},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/DeleteTeam.jpg","hash":"89dc5631b2d4c8af13ddaaa52ac4ce6d440c1596","modified":1578184565085},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/DeleteButton.jpg","hash":"3f153feb19810008a87e6d93eac7fbabe51b69e9","modified":1578184565085},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ErrorMessage.jpg","hash":"913f83d5b7a2159687ef1892103f21ccccaaa92b","modified":1578184565086},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/FilterToolbar.jpg","hash":"243e89a9282f6227f70923fdb6fa316e64afd4dc","modified":1578184565087},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/GetToWebBacklog.jpg","hash":"18e4a3ef7183646060a5ed07d2b9b1ffeea0c4ee","modified":1578184565088},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewChildButton.jpg","hash":"bc8cf8db8cb703b89f7876550536a36e397ecf7b","modified":1578184565090},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewSecondChildProcess.jpg","hash":"c869c5fdf75a58d6e878dda3225e3854088df58e","modified":1578184565091},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/IncludeSubAreas.jpg","hash":"31084bcb8b5df819868cdfcb29da729cc79aa480","modified":1578184565089},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-1.jpg","hash":"76ec87616f4f374fcdf77378d3af7a15d5d366bb","modified":1578184565092},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-2.jpg","hash":"7d70fff46f607f1764ca2cceb4732c744a9ca0bb","modified":1578184565093},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewWebTeam-1.jpg","hash":"2a752766bf3215d6102e30846e6d5d784c6ad845","modified":1578184565094},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/OnlyTeam-2.jpg","hash":"d68736ba5893762c58a0fff0e32aeaea1945d35a","modified":1578184565095},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ProjectConfigurationButton.jpg","hash":"4f8125159dcfd656e0a32c688f41983e067a9628","modified":1578184565095},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ProjectSettings.jpg","hash":"383388cf810823d72a811e282bd1bb6f599d8304","modified":1578184565096},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/Team-2AreaPath.jpg","hash":"6d56af1a6b7837e28f3ce7fe1312a8f05ba836f6","modified":1578184565099},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/Team-1AreaPath.jpg","hash":"3d54154a88038b95216fe65651f8ad11473aa56e","modified":1578184565098},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TeamConfiguration.jpg","hash":"8d74272bcf8069762c26156387d69d04c91ef499","modified":1578184565100},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TeamConfigurationClick.jpg","hash":"dcae63da2f0ef3812df3e7a71f347d01d8f37f11","modified":1578184565100},{"_id":"source/images/startreck.jpg","hash":"dbace5f55b96279b9ee65ec285110a3f0d8351fb","modified":1578184565145},{"_id":"themes/hexo-theme-bootstrap-blog/source/js/script.js","hash":"0f5f799f3d059fa56a3eddd3d1eb5236e636e12d","modified":1578184565184},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TwoBugs.jpg","hash":"ac780705e65fff683e2772d468b0c61e1a2bd509","modified":1578184565101},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/callouts.css","hash":"a9853df2c53b6a8dd4e8956c26191f0e241b312f","modified":1578184565170},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/subareaincludedmessage.jpg","hash":"bb4ea817f0d96bce0ed68761450e0a9b2bfb8ce5","modified":1578184565102},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/hexo-base.css","hash":"9fc6a81b54fc530a8e9365e7eee5114fcd215f00","modified":1578184565172},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/highlight-js.css","hash":"2b78bf52a38b8aaecef60d64f328217b34fc7665","modified":1578184565173},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1578184565175},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1578184565175},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1578184565176},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1578184565176},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/custom.css","hash":"e0c9de7b7affb81e223133b53eca4f233e40e7d4","modified":1553996316583},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1578184565177},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1578184565177},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/share-box.css","hash":"9862ff707167947d3ca4ef5acfe4d0d8e4132f85","modified":1578184565173},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.js","hash":"58193c802f307ec9bc9e586c0e8a13ebef45d2f8","modified":1578184565183},{"_id":"themes/hexo-theme-bootstrap-blog/source/css/styles.styl","hash":"4dddade672b5af92db28b534b0f78a2f4fa35e00","modified":1553996316586},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/archive.ejs","hash":"eeb449bc1c375a1858be53d8871a3f0add1e54a3","modified":1578184565154},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/archive-post.ejs","hash":"d8864a561579381a2d7a547c1b5575073ade55da","modified":1578184565153},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.pack.js","hash":"2da892a02778236b64076e5e8802ef0566e1d9e8","modified":1578184565183},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/jquery.fancybox.css","hash":"2e54d51d21e68ebc4bb870f6e57d3bfb660d4f9c","modified":1578184565182},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/footer.ejs","hash":"4a7d623f7f1916e9a34c1755e25444bf60d3d382","modified":1578184565155},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/header.ejs","hash":"ced2953bdc3a6b0cd33c2fc44f613ce667b9d4b3","modified":1578184565157},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1578184565155},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/head.ejs","hash":"f9f556d0fb9102c2cfe1e631655749f0e4d319ff","modified":1553996316555},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/masthead.ejs","hash":"641fab80381f9137bb4652e97cc70b000e3da97b","modified":1578184565158},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/article.ejs","hash":"2cf9a0adda74dcdc4eabbe83f6ff700c042b5b98","modified":1578184565154},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/about.ejs","hash":"dc72485cc12bf32ab39249e28957b4c62c47b0e6","modified":1578184565163},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/sidebar.ejs","hash":"9b2f9e76d75afa2dd64201cbec71e057435996ce","modified":1578184565162},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/category.ejs","hash":"847cdde48d5ec7007d2c2d64703776d80f0f55d8","modified":1578184565163},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/tagcloud.ejs","hash":"0d9cf74861960939875b97e1a86cd8d3710591bf","modified":1578184565164},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/archive.ejs","hash":"a05b3924600af24ece0b3c0e44a789933d8434ee","modified":1578184565163},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image1.png","hash":"7301f7d4ca05d4503128340deafa00d17c5f3858","modified":1578184564971},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image5.png","hash":"ed1440c21d916ed4931d9da2751fc31f6edb5770","modified":1578184564980},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image3.png","hash":"d5fc84917e1949d814ccfa825f45f1aa6348c070","modified":1578184564977},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image1.png","hash":"a97e4e02d32e742fa4e95653f6506287d59bbbd7","modified":1578184564985},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/inline-scripts.ejs","hash":"1f3938a352588958d02e28a984899e8409084f87","modified":1578184565157},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image3.png","hash":"268f2e0b8a8a3a899a94104b604ac3fb4664f913","modified":1578184564993},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image2.png","hash":"b5cefa4d9f1e202f0f3e9cc78a52d568c44c2b24","modified":1578184564992},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image6.png","hash":"9dc3cb45cd2d9aeaee5e09f76389e69ee23a1756","modified":1578184564996},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image8.png","hash":"03fa73dc4db808eba84c5074d4e6ff1566d2a31b","modified":1578184564999},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/IISWebAppDeployTask.png","hash":"7e173046f8f6cd4334b1a551c158746bcdcdf117","modified":1578184565003},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/variables.png","hash":"bbc67a27e7cb755f41e41dee079477987e103fd1","modified":1578184565006},{"_id":"source/_posts/Let-the-Test-Plan-Tell-the-Story/TestPlanResults.png","hash":"d0d655716a17e0052286f066cde7b477f7c032e0","modified":1578184565016},{"_id":"source/_posts/Security-Configuration-for-Teams/1.png","hash":"a3a6243a0fa02ee42c3176b3d4bf4699a0b1dea1","modified":1578184565040},{"_id":"source/_posts/Security-Configuration-for-Teams/5.png","hash":"8cbdb4d157dfc417743c5ca18f83dbaf64ac42c3","modified":1578184565057},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/BackToTeams.jpg","hash":"1ddc8e55aca07c6f2ae86e692a8fda28dc7d8a35","modified":1578184565074},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/recent_posts.ejs","hash":"2e48283bbdaf0edf230a538b602ff4e7a88ad409","modified":1578184565164},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewParentTeam.jpg","hash":"657344241ddf649cc973626ca06b8bac08b5f782","modified":1578184565080},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-2.jpg","hash":"8825eb2aef2cce64236c8f0fda7a48fb9d64cbd4","modified":1578184565083},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-1.jpg","hash":"87da9537512cb19ea75e1d315db9d98de1c9f1db","modified":1578184565081},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/SelectWebAreaPath.jpg","hash":"c8b35d07b528863a0bfbec8b66e6780ba2ae3883","modified":1578184565097},{"_id":"source/images/Forks_Over_Knives_movie_poster.png","hash":"8f385ed49e6e893a76b350a39be3eda1cd88b392","modified":1578184565118},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_widget/tag.ejs","hash":"d84a67bd058aa3666458491692e0c6f4ce136a62","modified":1578184565164},{"_id":"source/images/Donald_2019.jpg","hash":"3527071e39de74e73af369b6aef78fe18e13c0f9","modified":1574633040201},{"_id":"source/_posts/The-Power-of-Time-Tracking/image.jpg","hash":"6586cc3959b3e1570715c9d402363ba3f6cd5ae5","modified":1578184565105},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/web.config.png","hash":"1afa3abe1306b6a54e41e583c7386b03c41d82cd","modified":1578184565008},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1578184565178},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"4c9c395d705d22af7da06870d18f434e2a2eeaf9","modified":1578184565180},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-media.js","hash":"e14c32cc6823b81b2f758512f13ed8eb9ef2b454","modified":1578184565180},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"83cdfea43632b613771691a11f56f99d85fb6dbd","modified":1578184565181},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1578184565179},{"_id":"themes/hexo-theme-bootstrap-blog/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1578184565181},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/date.ejs","hash":"5268ce89860066b436cb93873773475fb0ddf41f","modified":1578184565159},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/category.ejs","hash":"79092bf6e9ef978904e6da4e86deacbb6c5435a0","modified":1578184565159},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/nav.ejs","hash":"a049d079f802473c468c364cfad0f68c4c560929","modified":1578184565160},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/gallery.ejs","hash":"3878fb0716709bab5fe584733164cb3bb8482feb","modified":1578184565160},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/title.ejs","hash":"142f1640163d346f26e675b21c7f70c1aadce267","modified":1578184565161},{"_id":"source/images/V__FD25.jpg","hash":"9e2cd52c182b6e25e985b48f394dd1dce43e7a7e","modified":1578184565135},{"_id":"themes/hexo-theme-bootstrap-blog/layout/_partial/post/tag.ejs","hash":"b52aa363c2ca79316fca9f20267cb43a2ab53386","modified":1578184565161},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image11.png","hash":"af5287d33e904b10f1d1e1646a0ad59803fe5ba0","modified":1578184564975},{"_id":"source/_posts/A-New-Start-on-an-Old-Blog/V__FD25.jpg","hash":"9e2cd52c182b6e25e985b48f394dd1dce43e7a7e","modified":1578184564966},{"_id":"source/images/Lumina-950XL.png","hash":"029886f27b694f0ca869c144b178dbad6992a5ba","modified":1578184565122},{"_id":"themes/hexo-theme-bootstrap-blog/.gitignore","hash":"ea2b285a29690f1eabbad0f3a158e34e9ccd1d86","modified":1553996316541},{"_id":"themes/hexo-theme-bootstrap-blog/package.json","hash":"a185de57d2a79eeea743541f33f9f3724425553f","modified":1553996316576}],"Category":[],"Data":[],"Page":[{"title":"About Me","date":"2015-12-26T18:19:28.000Z","_content":"Ah yes, you have arrived at the very strange world of Donald on Software.  I'm Donald Schulz your host.  Almost everything you see here is written by me, and it reflects my (shall-we-say) uniquie sensibilities about software development and what ever I find interesting.\n\nI was a software developer in Los Angeles with more than 20 years of experience.  I am originally from Canada and relocated in 1998 to work for One, Inc until they folded after the colapse of the dot com boom in January 2002.  I have my own company, [The Web We Weave, Inc.](http://www.thewebweweave.net) which makes [AGP Maker](http://www.agpmaker.com), the Software used to simplify the Awards and Graduation programs in High Schools.  \n\nI currently work as an [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) and [DevOps](https://en.wikipedia.org/wiki/DevOps) Consultant for [Imaginet](http://www.Imaginet.com) and travel all over the [United States and Canada](http://binged.it/1NRvP6B) providing guidance and recommendations to our clients on improving their process, culture and tooling to develop better software.  You can learn more about me personnaly by reading what I write.\n\n\n## How Often Do New Things Appear Here?\n\"Whether or not I put things up will depend largely on whether, on a particular day, I have anything to put up,\" to paraphrase McSweeneys. \n\n## So, Am I Supposed to Check Back Every Day?\nDon't bother.  Subscribe to my RSS feed.  You will find the white RSS icon link at the top of each page is a direct link to my atom formated RSS feed or you can follow me on [Twitter](http://twitter.com/donaldlschulz)  Any time I put up a new posting I will announce it on twitter.\n\n## But You're Completely Wrong. How Can I Let You Know?\nAh, yes, for this purpose I have comments turned on for each and every article that I write.  Please, no flames, no all caps, no crap.  No one replying just to be the first to reply.  In an attempt to reduce or eliminate spam from the comment section, all anonymous comments will go through an approval process in an attempt to reduce or eliminate spam or at least not plugging the comment section with it.  However, you can register on [Disqus](https://disqus.com/) and as a registered user you will be able to enter comments immediately without going through this approval step.","source":"About-Me/index.md","raw":"title: About Me\ndate: 2015-12-26 10:19:28\n---\nAh yes, you have arrived at the very strange world of Donald on Software.  I'm Donald Schulz your host.  Almost everything you see here is written by me, and it reflects my (shall-we-say) uniquie sensibilities about software development and what ever I find interesting.\n\nI was a software developer in Los Angeles with more than 20 years of experience.  I am originally from Canada and relocated in 1998 to work for One, Inc until they folded after the colapse of the dot com boom in January 2002.  I have my own company, [The Web We Weave, Inc.](http://www.thewebweweave.net) which makes [AGP Maker](http://www.agpmaker.com), the Software used to simplify the Awards and Graduation programs in High Schools.  \n\nI currently work as an [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) and [DevOps](https://en.wikipedia.org/wiki/DevOps) Consultant for [Imaginet](http://www.Imaginet.com) and travel all over the [United States and Canada](http://binged.it/1NRvP6B) providing guidance and recommendations to our clients on improving their process, culture and tooling to develop better software.  You can learn more about me personnaly by reading what I write.\n\n\n## How Often Do New Things Appear Here?\n\"Whether or not I put things up will depend largely on whether, on a particular day, I have anything to put up,\" to paraphrase McSweeneys. \n\n## So, Am I Supposed to Check Back Every Day?\nDon't bother.  Subscribe to my RSS feed.  You will find the white RSS icon link at the top of each page is a direct link to my atom formated RSS feed or you can follow me on [Twitter](http://twitter.com/donaldlschulz)  Any time I put up a new posting I will announce it on twitter.\n\n## But You're Completely Wrong. How Can I Let You Know?\nAh, yes, for this purpose I have comments turned on for each and every article that I write.  Please, no flames, no all caps, no crap.  No one replying just to be the first to reply.  In an attempt to reduce or eliminate spam from the comment section, all anonymous comments will go through an approval process in an attempt to reduce or eliminate spam or at least not plugging the comment section with it.  However, you can register on [Disqus](https://disqus.com/) and as a registered user you will be able to enter comments immediately without going through this approval step.","updated":"2020-01-05T00:36:04.963Z","path":"About-Me/index.html","comments":1,"layout":"page","_id":"ck50aqgf40000s4ufzl6fj3mn","content":"<p>Ah yes, you have arrived at the very strange world of Donald on Software.  I’m Donald Schulz your host.  Almost everything you see here is written by me, and it reflects my (shall-we-say) uniquie sensibilities about software development and what ever I find interesting.</p>\n<p>I was a software developer in Los Angeles with more than 20 years of experience.  I am originally from Canada and relocated in 1998 to work for One, Inc until they folded after the colapse of the dot com boom in January 2002.  I have my own company, <a href=\"http://www.thewebweweave.net\" target=\"_blank\" rel=\"noopener\">The Web We Weave, Inc.</a> which makes <a href=\"http://www.agpmaker.com\" target=\"_blank\" rel=\"noopener\">AGP Maker</a>, the Software used to simplify the Awards and Graduation programs in High Schools.  </p>\n<p>I currently work as an <a href=\"https://en.wikipedia.org/wiki/Application_lifecycle_management\" target=\"_blank\" rel=\"noopener\">ALM</a> and <a href=\"https://en.wikipedia.org/wiki/DevOps\" target=\"_blank\" rel=\"noopener\">DevOps</a> Consultant for <a href=\"http://www.Imaginet.com\" target=\"_blank\" rel=\"noopener\">Imaginet</a> and travel all over the <a href=\"http://binged.it/1NRvP6B\" target=\"_blank\" rel=\"noopener\">United States and Canada</a> providing guidance and recommendations to our clients on improving their process, culture and tooling to develop better software.  You can learn more about me personnaly by reading what I write.</p>\n<h2 id=\"How-Often-Do-New-Things-Appear-Here\"><a href=\"#How-Often-Do-New-Things-Appear-Here\" class=\"headerlink\" title=\"How Often Do New Things Appear Here?\"></a>How Often Do New Things Appear Here?</h2><p>“Whether or not I put things up will depend largely on whether, on a particular day, I have anything to put up,” to paraphrase McSweeneys. </p>\n<h2 id=\"So-Am-I-Supposed-to-Check-Back-Every-Day\"><a href=\"#So-Am-I-Supposed-to-Check-Back-Every-Day\" class=\"headerlink\" title=\"So, Am I Supposed to Check Back Every Day?\"></a>So, Am I Supposed to Check Back Every Day?</h2><p>Don’t bother.  Subscribe to my RSS feed.  You will find the white RSS icon link at the top of each page is a direct link to my atom formated RSS feed or you can follow me on <a href=\"http://twitter.com/donaldlschulz\" target=\"_blank\" rel=\"noopener\">Twitter</a>  Any time I put up a new posting I will announce it on twitter.</p>\n<h2 id=\"But-You’re-Completely-Wrong-How-Can-I-Let-You-Know\"><a href=\"#But-You’re-Completely-Wrong-How-Can-I-Let-You-Know\" class=\"headerlink\" title=\"But You’re Completely Wrong. How Can I Let You Know?\"></a>But You’re Completely Wrong. How Can I Let You Know?</h2><p>Ah, yes, for this purpose I have comments turned on for each and every article that I write.  Please, no flames, no all caps, no crap.  No one replying just to be the first to reply.  In an attempt to reduce or eliminate spam from the comment section, all anonymous comments will go through an approval process in an attempt to reduce or eliminate spam or at least not plugging the comment section with it.  However, you can register on <a href=\"https://disqus.com/\" target=\"_blank\" rel=\"noopener\">Disqus</a> and as a registered user you will be able to enter comments immediately without going through this approval step.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Ah yes, you have arrived at the very strange world of Donald on Software.  I’m Donald Schulz your host.  Almost everything you see here is written by me, and it reflects my (shall-we-say) uniquie sensibilities about software development and what ever I find interesting.</p>\n<p>I was a software developer in Los Angeles with more than 20 years of experience.  I am originally from Canada and relocated in 1998 to work for One, Inc until they folded after the colapse of the dot com boom in January 2002.  I have my own company, <a href=\"http://www.thewebweweave.net\" target=\"_blank\" rel=\"noopener\">The Web We Weave, Inc.</a> which makes <a href=\"http://www.agpmaker.com\" target=\"_blank\" rel=\"noopener\">AGP Maker</a>, the Software used to simplify the Awards and Graduation programs in High Schools.  </p>\n<p>I currently work as an <a href=\"https://en.wikipedia.org/wiki/Application_lifecycle_management\" target=\"_blank\" rel=\"noopener\">ALM</a> and <a href=\"https://en.wikipedia.org/wiki/DevOps\" target=\"_blank\" rel=\"noopener\">DevOps</a> Consultant for <a href=\"http://www.Imaginet.com\" target=\"_blank\" rel=\"noopener\">Imaginet</a> and travel all over the <a href=\"http://binged.it/1NRvP6B\" target=\"_blank\" rel=\"noopener\">United States and Canada</a> providing guidance and recommendations to our clients on improving their process, culture and tooling to develop better software.  You can learn more about me personnaly by reading what I write.</p>\n<h2 id=\"How-Often-Do-New-Things-Appear-Here\"><a href=\"#How-Often-Do-New-Things-Appear-Here\" class=\"headerlink\" title=\"How Often Do New Things Appear Here?\"></a>How Often Do New Things Appear Here?</h2><p>“Whether or not I put things up will depend largely on whether, on a particular day, I have anything to put up,” to paraphrase McSweeneys. </p>\n<h2 id=\"So-Am-I-Supposed-to-Check-Back-Every-Day\"><a href=\"#So-Am-I-Supposed-to-Check-Back-Every-Day\" class=\"headerlink\" title=\"So, Am I Supposed to Check Back Every Day?\"></a>So, Am I Supposed to Check Back Every Day?</h2><p>Don’t bother.  Subscribe to my RSS feed.  You will find the white RSS icon link at the top of each page is a direct link to my atom formated RSS feed or you can follow me on <a href=\"http://twitter.com/donaldlschulz\" target=\"_blank\" rel=\"noopener\">Twitter</a>  Any time I put up a new posting I will announce it on twitter.</p>\n<h2 id=\"But-You’re-Completely-Wrong-How-Can-I-Let-You-Know\"><a href=\"#But-You’re-Completely-Wrong-How-Can-I-Let-You-Know\" class=\"headerlink\" title=\"But You’re Completely Wrong. How Can I Let You Know?\"></a>But You’re Completely Wrong. How Can I Let You Know?</h2><p>Ah, yes, for this purpose I have comments turned on for each and every article that I write.  Please, no flames, no all caps, no crap.  No one replying just to be the first to reply.  In an attempt to reduce or eliminate spam from the comment section, all anonymous comments will go through an approval process in an attempt to reduce or eliminate spam or at least not plugging the comment section with it.  However, you can register on <a href=\"https://disqus.com/\" target=\"_blank\" rel=\"noopener\">Disqus</a> and as a registered user you will be able to enter comments immediately without going through this approval step.</p>\n"}],"Post":[{"title":"A New Start on an Old Blog","date":"2016-01-15T04:21:45.000Z","_content":"\n{% img right /images/V__FD25.jpg 450 450 \"Fall colors in California\" %}\nIt has been quite a while since I have posted my last blog so today I thought I would bring you up to speed on what I have been doing with this site.  The last time I did a post like this was back in June of 2008.  Back then I talked about the transition that I made going from City Desk to Microsoft Content Management System which evenually was merged into SharePoint and from there we changed the blog into DotNetNuke.\n\nSince that time we have not created any new content but have moved that material to [BlogEngine.Net](http://dotnetblogengine.net/) and this really is a great tool but not the way I wanted to work.  I really do not want a Content Management system for my blog, I don't want pages that are rendered dynamically and the content pulled from a database.  What I really wanted were static pages and the content for those pages be stored and built the same way that I build all my software, stored in Version Control.\n\nJust before I move on and tell you more about my new blog workflow I thought I would share a picture from my backyard and that tree on the other side of the fence is usually green it does not change colors every fall but this year the weather has been cooler than usual, so yes we sometimes do get fall colors in California and here is the proof.\n\n## Hexo\n\n{% img right /images/hexo.jpg 100 100 \"Hexo Logo\" %}\n[Hexo](https://hexo.io/) is a static page generator program that takes [simple markup](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and turns it into static html pages.  This means I can deploy this anywhere from a build that I can generate it just like a regular [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) build because all the pieces are in source control.  It fully embrasses git and is a github open source project.  I thought that moving my blog to Hexo would help me in too ways, besides giving me the output that I am really looking for but also to use as a teaching tool on how the new Build system that is part of TFS 2015 fully embraces other technologies outside of dotNet and the Visual Studio family.  From here I check-in my new blogs into source control and that triggers a build which puts the source into a drop folder which is then deployed to my web site which is hosted on Azure.\n\nAs of this post I am using FTP in a PowerShell script which is used to deploy the web site which is not ideal.  I am working on creating an MSDeploy package that can then be deployed directly onto the Azure website that is hosting this blog.\n\n## The Work Flow\n\nThe process begins when I want to start a new blog.  Because my git repositories are available to me from almost any computer that I am working with I go to the local workspace of my Blog git repository checkout the dev branch and at the command line enter the following command\n```\n  hexo new Post \"A New Start on an Old Blog\"\n```\nThis will place a new md file in the _post folder with the same name as the title but the spaces replaced by hyphens (\"-\").  After that I like to open the folder at the root of my blog workspace using [Visual Studio Code](https://code.visualstudio.com/).  The thing that I like about using Visual Studio Code as my editor is that it understands simple markdown and will give me a pretty good preview as I am working on it and if my screen is wide enough I can even have one half of the screen to type in the raw simple markdown and the other half to see what it looks like.\n\nThe other thing that I like about this editor is that it understands and talks git.  Which means I can edit my files and save them and Visual Studio Code is going to inform me that I have uncommitted changes so I can add them to staging and commit them to my local repository as well as push them to my remote git repository.  Above you may have noticed that before I began this process I checked out the dev branch which means that I do not write my new posts in the master branch and the reason for that is that I have a continious integration trigger on the build server that is looking for anything that is checked into the master on the remote git repository.  Because I might start a blog on one machine and finish it on another I need some way to keep all these in sync and that is what I use the dev branch for.  Once I am happy with the post I will then merge the changes from dev into master and this will begin the build process.\n\n## Publishing the Post\n\nOnce I am happy with my post all I need to do is to merge the dev branch into Master and this starts the build process.  Which is really just another Hexo command that is called against my source which then generates all the static pages, javascript, images and so on and puts it into a public folder.\n```\n  hexo generate\n```\nIt is the content of this folder that then becomes my drop artifacts.  Because the Release Manager also has a CI trigger after the build has been sucessful it will begin a Release pipeline to get this drop into my web site.  My goal is to get this wrapped up into an MSDeploy package that can then be deployed directly onto my Azure web site.  I am still working on that and will provide a more detailed post on what I needed to do to get that to happen.  In the meantime, I need to make sure that my Test virtual machine is up and running in Azure as one of the first things that this Release Manager pipeline will do is to copy the contents of the drop onto this machine.  Then it calls a CodedUI test which really is not testing it will run my PowerShell script that will FTP the pages to my Azure web site.  It needs to do this as a user and the easiest way without me having to do this manually is to run the CUI to do it and complete it.\n\n## Summary\n\nSo there you have it, I have my blog in source control so I have no dependancy of a database and all the code to generate the web site and my content pages are in source control which makes it really easy if I ever need to make a move to a different site or location or anything like rebuild from a really bad crash.  As an ALM guy I really like this approach and what would be even better was having a new pre-production staging site to go over the site and give it a last and final approval before it goes live to the public site.","source":"_posts/A-New-Start-on-an-Old-Blog.md","raw":"title: A New Start on an Old Blog\ntags:\n  - Blogs\n  - ALM\ndate: 2016-01-14 20:21:45\n---\n\n{% img right /images/V__FD25.jpg 450 450 \"Fall colors in California\" %}\nIt has been quite a while since I have posted my last blog so today I thought I would bring you up to speed on what I have been doing with this site.  The last time I did a post like this was back in June of 2008.  Back then I talked about the transition that I made going from City Desk to Microsoft Content Management System which evenually was merged into SharePoint and from there we changed the blog into DotNetNuke.\n\nSince that time we have not created any new content but have moved that material to [BlogEngine.Net](http://dotnetblogengine.net/) and this really is a great tool but not the way I wanted to work.  I really do not want a Content Management system for my blog, I don't want pages that are rendered dynamically and the content pulled from a database.  What I really wanted were static pages and the content for those pages be stored and built the same way that I build all my software, stored in Version Control.\n\nJust before I move on and tell you more about my new blog workflow I thought I would share a picture from my backyard and that tree on the other side of the fence is usually green it does not change colors every fall but this year the weather has been cooler than usual, so yes we sometimes do get fall colors in California and here is the proof.\n\n## Hexo\n\n{% img right /images/hexo.jpg 100 100 \"Hexo Logo\" %}\n[Hexo](https://hexo.io/) is a static page generator program that takes [simple markup](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and turns it into static html pages.  This means I can deploy this anywhere from a build that I can generate it just like a regular [ALM](https://en.wikipedia.org/wiki/Application_lifecycle_management) build because all the pieces are in source control.  It fully embrasses git and is a github open source project.  I thought that moving my blog to Hexo would help me in too ways, besides giving me the output that I am really looking for but also to use as a teaching tool on how the new Build system that is part of TFS 2015 fully embraces other technologies outside of dotNet and the Visual Studio family.  From here I check-in my new blogs into source control and that triggers a build which puts the source into a drop folder which is then deployed to my web site which is hosted on Azure.\n\nAs of this post I am using FTP in a PowerShell script which is used to deploy the web site which is not ideal.  I am working on creating an MSDeploy package that can then be deployed directly onto the Azure website that is hosting this blog.\n\n## The Work Flow\n\nThe process begins when I want to start a new blog.  Because my git repositories are available to me from almost any computer that I am working with I go to the local workspace of my Blog git repository checkout the dev branch and at the command line enter the following command\n```\n  hexo new Post \"A New Start on an Old Blog\"\n```\nThis will place a new md file in the _post folder with the same name as the title but the spaces replaced by hyphens (\"-\").  After that I like to open the folder at the root of my blog workspace using [Visual Studio Code](https://code.visualstudio.com/).  The thing that I like about using Visual Studio Code as my editor is that it understands simple markdown and will give me a pretty good preview as I am working on it and if my screen is wide enough I can even have one half of the screen to type in the raw simple markdown and the other half to see what it looks like.\n\nThe other thing that I like about this editor is that it understands and talks git.  Which means I can edit my files and save them and Visual Studio Code is going to inform me that I have uncommitted changes so I can add them to staging and commit them to my local repository as well as push them to my remote git repository.  Above you may have noticed that before I began this process I checked out the dev branch which means that I do not write my new posts in the master branch and the reason for that is that I have a continious integration trigger on the build server that is looking for anything that is checked into the master on the remote git repository.  Because I might start a blog on one machine and finish it on another I need some way to keep all these in sync and that is what I use the dev branch for.  Once I am happy with the post I will then merge the changes from dev into master and this will begin the build process.\n\n## Publishing the Post\n\nOnce I am happy with my post all I need to do is to merge the dev branch into Master and this starts the build process.  Which is really just another Hexo command that is called against my source which then generates all the static pages, javascript, images and so on and puts it into a public folder.\n```\n  hexo generate\n```\nIt is the content of this folder that then becomes my drop artifacts.  Because the Release Manager also has a CI trigger after the build has been sucessful it will begin a Release pipeline to get this drop into my web site.  My goal is to get this wrapped up into an MSDeploy package that can then be deployed directly onto my Azure web site.  I am still working on that and will provide a more detailed post on what I needed to do to get that to happen.  In the meantime, I need to make sure that my Test virtual machine is up and running in Azure as one of the first things that this Release Manager pipeline will do is to copy the contents of the drop onto this machine.  Then it calls a CodedUI test which really is not testing it will run my PowerShell script that will FTP the pages to my Azure web site.  It needs to do this as a user and the easiest way without me having to do this manually is to run the CUI to do it and complete it.\n\n## Summary\n\nSo there you have it, I have my blog in source control so I have no dependancy of a database and all the code to generate the web site and my content pages are in source control which makes it really easy if I ever need to make a move to a different site or location or anything like rebuild from a really bad crash.  As an ALM guy I really like this approach and what would be even better was having a new pre-production staging site to go over the site and give it a last and final approval before it goes live to the public site.","slug":"A-New-Start-on-an-Old-Blog","published":1,"updated":"2020-01-05T00:36:04.964Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgf60001s4ufseyh4c3s","content":"<img src=\"/images/V__FD25.jpg\" class=\"right\" width=\"450\" height=\"450\" title=\"Fall colors in California\">\n<p>It has been quite a while since I have posted my last blog so today I thought I would bring you up to speed on what I have been doing with this site.  The last time I did a post like this was back in June of 2008.  Back then I talked about the transition that I made going from City Desk to Microsoft Content Management System which evenually was merged into SharePoint and from there we changed the blog into DotNetNuke.</p>\n<p>Since that time we have not created any new content but have moved that material to <a href=\"http://dotnetblogengine.net/\" target=\"_blank\" rel=\"noopener\">BlogEngine.Net</a> and this really is a great tool but not the way I wanted to work.  I really do not want a Content Management system for my blog, I don’t want pages that are rendered dynamically and the content pulled from a database.  What I really wanted were static pages and the content for those pages be stored and built the same way that I build all my software, stored in Version Control.</p>\n<p>Just before I move on and tell you more about my new blog workflow I thought I would share a picture from my backyard and that tree on the other side of the fence is usually green it does not change colors every fall but this year the weather has been cooler than usual, so yes we sometimes do get fall colors in California and here is the proof.</p>\n<h2 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h2><img src=\"/images/hexo.jpg\" class=\"right\" width=\"100\" height=\"100\" title=\"Hexo Logo\">\n<p><a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a> is a static page generator program that takes <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\" target=\"_blank\" rel=\"noopener\">simple markup</a> and turns it into static html pages.  This means I can deploy this anywhere from a build that I can generate it just like a regular <a href=\"https://en.wikipedia.org/wiki/Application_lifecycle_management\" target=\"_blank\" rel=\"noopener\">ALM</a> build because all the pieces are in source control.  It fully embrasses git and is a github open source project.  I thought that moving my blog to Hexo would help me in too ways, besides giving me the output that I am really looking for but also to use as a teaching tool on how the new Build system that is part of TFS 2015 fully embraces other technologies outside of dotNet and the Visual Studio family.  From here I check-in my new blogs into source control and that triggers a build which puts the source into a drop folder which is then deployed to my web site which is hosted on Azure.</p>\n<p>As of this post I am using FTP in a PowerShell script which is used to deploy the web site which is not ideal.  I am working on creating an MSDeploy package that can then be deployed directly onto the Azure website that is hosting this blog.</p>\n<h2 id=\"The-Work-Flow\"><a href=\"#The-Work-Flow\" class=\"headerlink\" title=\"The Work Flow\"></a>The Work Flow</h2><p>The process begins when I want to start a new blog.  Because my git repositories are available to me from almost any computer that I am working with I go to the local workspace of my Blog git repository checkout the dev branch and at the command line enter the following command<br><figure class=\"highlight haxe\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo <span class=\"keyword\">new</span> <span class=\"type\">Post</span> <span class=\"string\">\"A New Start on an Old Blog\"</span></span><br></pre></td></tr></table></figure></p>\n<p>This will place a new md file in the _post folder with the same name as the title but the spaces replaced by hyphens (“-“).  After that I like to open the folder at the root of my blog workspace using <a href=\"https://code.visualstudio.com/\" target=\"_blank\" rel=\"noopener\">Visual Studio Code</a>.  The thing that I like about using Visual Studio Code as my editor is that it understands simple markdown and will give me a pretty good preview as I am working on it and if my screen is wide enough I can even have one half of the screen to type in the raw simple markdown and the other half to see what it looks like.</p>\n<p>The other thing that I like about this editor is that it understands and talks git.  Which means I can edit my files and save them and Visual Studio Code is going to inform me that I have uncommitted changes so I can add them to staging and commit them to my local repository as well as push them to my remote git repository.  Above you may have noticed that before I began this process I checked out the dev branch which means that I do not write my new posts in the master branch and the reason for that is that I have a continious integration trigger on the build server that is looking for anything that is checked into the master on the remote git repository.  Because I might start a blog on one machine and finish it on another I need some way to keep all these in sync and that is what I use the dev branch for.  Once I am happy with the post I will then merge the changes from dev into master and this will begin the build process.</p>\n<h2 id=\"Publishing-the-Post\"><a href=\"#Publishing-the-Post\" class=\"headerlink\" title=\"Publishing the Post\"></a>Publishing the Post</h2><p>Once I am happy with my post all I need to do is to merge the dev branch into Master and this starts the build process.  Which is really just another Hexo command that is called against my source which then generates all the static pages, javascript, images and so on and puts it into a public folder.<br><figure class=\"highlight verilog\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo <span class=\"keyword\">generate</span></span><br></pre></td></tr></table></figure></p>\n<p>It is the content of this folder that then becomes my drop artifacts.  Because the Release Manager also has a CI trigger after the build has been sucessful it will begin a Release pipeline to get this drop into my web site.  My goal is to get this wrapped up into an MSDeploy package that can then be deployed directly onto my Azure web site.  I am still working on that and will provide a more detailed post on what I needed to do to get that to happen.  In the meantime, I need to make sure that my Test virtual machine is up and running in Azure as one of the first things that this Release Manager pipeline will do is to copy the contents of the drop onto this machine.  Then it calls a CodedUI test which really is not testing it will run my PowerShell script that will FTP the pages to my Azure web site.  It needs to do this as a user and the easiest way without me having to do this manually is to run the CUI to do it and complete it.</p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>So there you have it, I have my blog in source control so I have no dependancy of a database and all the code to generate the web site and my content pages are in source control which makes it really easy if I ever need to make a move to a different site or location or anything like rebuild from a really bad crash.  As an ALM guy I really like this approach and what would be even better was having a new pre-production staging site to go over the site and give it a last and final approval before it goes live to the public site.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/V__FD25.jpg\" class=\"right\" width=\"450\" height=\"450\" title=\"Fall colors in California\">\n<p>It has been quite a while since I have posted my last blog so today I thought I would bring you up to speed on what I have been doing with this site.  The last time I did a post like this was back in June of 2008.  Back then I talked about the transition that I made going from City Desk to Microsoft Content Management System which evenually was merged into SharePoint and from there we changed the blog into DotNetNuke.</p>\n<p>Since that time we have not created any new content but have moved that material to <a href=\"http://dotnetblogengine.net/\" target=\"_blank\" rel=\"noopener\">BlogEngine.Net</a> and this really is a great tool but not the way I wanted to work.  I really do not want a Content Management system for my blog, I don’t want pages that are rendered dynamically and the content pulled from a database.  What I really wanted were static pages and the content for those pages be stored and built the same way that I build all my software, stored in Version Control.</p>\n<p>Just before I move on and tell you more about my new blog workflow I thought I would share a picture from my backyard and that tree on the other side of the fence is usually green it does not change colors every fall but this year the weather has been cooler than usual, so yes we sometimes do get fall colors in California and here is the proof.</p>\n<h2 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h2><img src=\"/images/hexo.jpg\" class=\"right\" width=\"100\" height=\"100\" title=\"Hexo Logo\">\n<p><a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a> is a static page generator program that takes <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\" target=\"_blank\" rel=\"noopener\">simple markup</a> and turns it into static html pages.  This means I can deploy this anywhere from a build that I can generate it just like a regular <a href=\"https://en.wikipedia.org/wiki/Application_lifecycle_management\" target=\"_blank\" rel=\"noopener\">ALM</a> build because all the pieces are in source control.  It fully embrasses git and is a github open source project.  I thought that moving my blog to Hexo would help me in too ways, besides giving me the output that I am really looking for but also to use as a teaching tool on how the new Build system that is part of TFS 2015 fully embraces other technologies outside of dotNet and the Visual Studio family.  From here I check-in my new blogs into source control and that triggers a build which puts the source into a drop folder which is then deployed to my web site which is hosted on Azure.</p>\n<p>As of this post I am using FTP in a PowerShell script which is used to deploy the web site which is not ideal.  I am working on creating an MSDeploy package that can then be deployed directly onto the Azure website that is hosting this blog.</p>\n<h2 id=\"The-Work-Flow\"><a href=\"#The-Work-Flow\" class=\"headerlink\" title=\"The Work Flow\"></a>The Work Flow</h2><p>The process begins when I want to start a new blog.  Because my git repositories are available to me from almost any computer that I am working with I go to the local workspace of my Blog git repository checkout the dev branch and at the command line enter the following command<br><figure class=\"highlight haxe\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo <span class=\"keyword\">new</span> <span class=\"type\">Post</span> <span class=\"string\">\"A New Start on an Old Blog\"</span></span><br></pre></td></tr></table></figure></p>\n<p>This will place a new md file in the _post folder with the same name as the title but the spaces replaced by hyphens (“-“).  After that I like to open the folder at the root of my blog workspace using <a href=\"https://code.visualstudio.com/\" target=\"_blank\" rel=\"noopener\">Visual Studio Code</a>.  The thing that I like about using Visual Studio Code as my editor is that it understands simple markdown and will give me a pretty good preview as I am working on it and if my screen is wide enough I can even have one half of the screen to type in the raw simple markdown and the other half to see what it looks like.</p>\n<p>The other thing that I like about this editor is that it understands and talks git.  Which means I can edit my files and save them and Visual Studio Code is going to inform me that I have uncommitted changes so I can add them to staging and commit them to my local repository as well as push them to my remote git repository.  Above you may have noticed that before I began this process I checked out the dev branch which means that I do not write my new posts in the master branch and the reason for that is that I have a continious integration trigger on the build server that is looking for anything that is checked into the master on the remote git repository.  Because I might start a blog on one machine and finish it on another I need some way to keep all these in sync and that is what I use the dev branch for.  Once I am happy with the post I will then merge the changes from dev into master and this will begin the build process.</p>\n<h2 id=\"Publishing-the-Post\"><a href=\"#Publishing-the-Post\" class=\"headerlink\" title=\"Publishing the Post\"></a>Publishing the Post</h2><p>Once I am happy with my post all I need to do is to merge the dev branch into Master and this starts the build process.  Which is really just another Hexo command that is called against my source which then generates all the static pages, javascript, images and so on and puts it into a public folder.<br><figure class=\"highlight verilog\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo <span class=\"keyword\">generate</span></span><br></pre></td></tr></table></figure></p>\n<p>It is the content of this folder that then becomes my drop artifacts.  Because the Release Manager also has a CI trigger after the build has been sucessful it will begin a Release pipeline to get this drop into my web site.  My goal is to get this wrapped up into an MSDeploy package that can then be deployed directly onto my Azure web site.  I am still working on that and will provide a more detailed post on what I needed to do to get that to happen.  In the meantime, I need to make sure that my Test virtual machine is up and running in Azure as one of the first things that this Release Manager pipeline will do is to copy the contents of the drop onto this machine.  Then it calls a CodedUI test which really is not testing it will run my PowerShell script that will FTP the pages to my Azure web site.  It needs to do this as a user and the easiest way without me having to do this manually is to run the CUI to do it and complete it.</p>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>So there you have it, I have my blog in source control so I have no dependancy of a database and all the code to generate the web site and my content pages are in source control which makes it really easy if I ever need to make a move to a different site or location or anything like rebuild from a really bad crash.  As an ALM guy I really like this approach and what would be even better was having a new pre-production staging site to go over the site and give it a last and final approval before it goes live to the public site.</p>\n"},{"title":"Database Schema Compare where Visual Studio goes that extra mile","date":"2011-01-27T08:00:00.000Z","_content":"There are a number of good database tools out there for doing database schema comparisons.  I have used different ones over the years at first initially to help me write SQL differencing scripts that I could use when deploying database changes.  If your background is anything like mine where you were namely a Visual Basic or a C# developer and could get by with working on SQL if you could write directly to the database.  There were challenges with being able to script everything out using just SQL.  Today that is not nearly an issue for me and I can do quite a bit with scripting and could build those scripts by hand, but why? \n## WHAT… Visual Studio for database development?\nOver the years I have tried to promote SQL development to be done in Visual Studio.  I made a great case, SQL is code just as much as my VB, C#, F# or what ever your favorite language of choice happens to be and should be protected in source control.  Makes sense but it is a really hard sell.  Productivity goes down hill, errors begin to happen because this is not how the SQL teams are used to working on databases.  It was an easier sell for me because I loved working in Visual Studio and found the SQL tools not to be as intuitive to me.  I have never been able to figure out how I could walk through a stored procedure in Query Analyzer or Management Studio but have always been able to do this with stored procedures that I wrote from within Visual Studio and that was long before the data editions of Visual Studio. \n\nEver since the release of the Data Dude or its official name back then, Visual Studio Team Edition for Database Professionals, this was what I did and I tried to convince others that this is what we should be doing.  It was never an easy sell, yea the schema comparison was nice but our SQL professionals already had all kinds of comparison tools for SQL and it would be too hard for them to work this way.  They wanted to be able to make changes in a database and see the results of those changes, not have to deploy it somewhere first. \n\nSo as a quick summary of what we figured out so far.  Schema comparison from one database to another, nothing new, your SQL department probably has a number of these and use them to generate their change scripts.  How is Visual Studio schema comparison better than what I already have how is it going to go the extra mile?  That my friend starts with the database project which does a reverse engineering of sorts of what you have in the database and scripts the whole thing out into source files that you can check into source control and compare the changes just like you do with any other source code. \n\nNow once you have a database project you are able to not just do a schema comparison with two databases but you can also compare from a database and this project.  The extra mile is that I can even go so far as to deploy the differences to your test and production databases.  It gets even better but before I tell you the best part lets go through the actual steps that you would take to create this initial database project. \n## Create the Database Project\nI am going to walk you through the very simple steps that it takes to build a database project for the AdventureWorks database.  For this you will need Visual Studio 2010 Premium edition or higher. \n\nWe start by creating a new project and select “SQL Server 2008 Database Project” template from under the Database - SQL Server project types.  Give it a name and set the location.  I called mine AdventureWorks because I am going to work with the sample AdventureWorks database.  Click OK.. \n{% asset_img image1.png \"Create a Project\" %}\nVisual Studio will build a default database project for you, but it is not connected to anything so there is no actual database scripted out here.  We are going to do that now.  Right click on the database project and a context sensitive menu will popup with Import Database Objects and Settings… click on that now. \n{% asset_img image2.png \"Import Objects\" %}\nThis opens the Import Database Wizard dialog box.  If you have already connected to this database from Visual Studio then you will find an entry in the dropdown control Source database connection.  If not then you will create a new connection by clicking on the New Connection… button.  \n{% asset_img image3.png \"Import Wizard\" %}\nSo if you have a ready made connection in the dropdown, choose it and skip the next screen and step as I am going to build my new connection. \n{% asset_img image4.png \"New Connection\" %}\nBecause my adventure works database in on my local machine I went with that but this database could be a database that is anywhere on your network, this will all just work provided you do have the necessary permissions to connect to it in this way.  Clicking on OK takes us back to the previous screen with the Source database connection filled in. \n\nEveryone, click Start which will bring up the following screen and start to import and script out the database.  When it is all done click the Finish button.  Congratulations you have built a Database Project. \n{% asset_img image5.png \"Import Wizard Finishing\" %}\nYou can expand the solution under Schema Objects, Schemas, and I am showing the dbo schema and it has 3 table scripts.  All the objects of this database are scripted out here.  You can look at these files right here is Visual Studio. \n{% asset_img image6.png \"Solution Explorer\" %}\nHowever you might want to use the Schema View tool for looking at the objects which gives you a more Management Studio type of view. \n{% asset_img image7.png \"Toolbar\" %}\nJust click on the icon in the Solution Explorer that has the popup caption that says Database Schema Viewer. \n{% asset_img image8.png \"Schema View\" %}\n## Updating the Visual Studio Project from the database\nIn the past these were the steps that I would show and demonstrate on how to get a database project scripted out and now that it is code is really easy to get into version control because of the really tight integration from Visual Studio.  My thoughts after that is this is the tool that you should be working in to evolve the database.  Work in Visual Studio and deploy the changes to the database. \n## Light Bulb Moment\nJust recently I discovered how the SQL developer does not really need to leave their favorite tool for working on the database, Management Studio.  That’s right, the new workflow is to continue to make your changes in your local or isolated databases so that you can see first hand how the database changes are going to work.  When you are ready to get those changes into version control you use Visual Studio and the Database Schema comparison. \n{% asset_img image9.png \"Switch Control\" %}\nSo here we see what I always thought was the normal workflow, with the Project on the left and the database that we are going to deploy to on the right.  If instead we are working on the database and we want to push those change to the Project, then switch the source and target around. \n{% asset_img image10.png \"Options\" %}\nNow when you click the OK button you will get a schema comparison just like you always did but when deployed it will check out the project and update the source files.  This will then give you complete history and the files will move through the system from branch to branch with a perfect snapshot of what the database looked like for a specific build. \n{% asset_img image11.png \"Options\" %}\n1.  Click this button to get the party started. \n2.  This comment will disappear in the project source file. \n3.  The source will be checked out during the update. \n## The Recap of what we have just seen.\nThis totally changes my opinion on how to go forward with this great tool.  The fact that we can update the project source from the database was probably always there but if I missed the fact that this was possible then I am sure many others might have missed it as well.  It makes SQL development smooth and safe (all schema scripts under version control) and the ready for the next step to smooth and automated deployment. ","source":"_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile.md","raw":"title: Database Schema Compare where Visual Studio goes that extra mile\ndate: 2011-01-27 \ntags:\n- Database\n- SQL\n- Compare\n- ALM\n---\nThere are a number of good database tools out there for doing database schema comparisons.  I have used different ones over the years at first initially to help me write SQL differencing scripts that I could use when deploying database changes.  If your background is anything like mine where you were namely a Visual Basic or a C# developer and could get by with working on SQL if you could write directly to the database.  There were challenges with being able to script everything out using just SQL.  Today that is not nearly an issue for me and I can do quite a bit with scripting and could build those scripts by hand, but why? \n## WHAT… Visual Studio for database development?\nOver the years I have tried to promote SQL development to be done in Visual Studio.  I made a great case, SQL is code just as much as my VB, C#, F# or what ever your favorite language of choice happens to be and should be protected in source control.  Makes sense but it is a really hard sell.  Productivity goes down hill, errors begin to happen because this is not how the SQL teams are used to working on databases.  It was an easier sell for me because I loved working in Visual Studio and found the SQL tools not to be as intuitive to me.  I have never been able to figure out how I could walk through a stored procedure in Query Analyzer or Management Studio but have always been able to do this with stored procedures that I wrote from within Visual Studio and that was long before the data editions of Visual Studio. \n\nEver since the release of the Data Dude or its official name back then, Visual Studio Team Edition for Database Professionals, this was what I did and I tried to convince others that this is what we should be doing.  It was never an easy sell, yea the schema comparison was nice but our SQL professionals already had all kinds of comparison tools for SQL and it would be too hard for them to work this way.  They wanted to be able to make changes in a database and see the results of those changes, not have to deploy it somewhere first. \n\nSo as a quick summary of what we figured out so far.  Schema comparison from one database to another, nothing new, your SQL department probably has a number of these and use them to generate their change scripts.  How is Visual Studio schema comparison better than what I already have how is it going to go the extra mile?  That my friend starts with the database project which does a reverse engineering of sorts of what you have in the database and scripts the whole thing out into source files that you can check into source control and compare the changes just like you do with any other source code. \n\nNow once you have a database project you are able to not just do a schema comparison with two databases but you can also compare from a database and this project.  The extra mile is that I can even go so far as to deploy the differences to your test and production databases.  It gets even better but before I tell you the best part lets go through the actual steps that you would take to create this initial database project. \n## Create the Database Project\nI am going to walk you through the very simple steps that it takes to build a database project for the AdventureWorks database.  For this you will need Visual Studio 2010 Premium edition or higher. \n\nWe start by creating a new project and select “SQL Server 2008 Database Project” template from under the Database - SQL Server project types.  Give it a name and set the location.  I called mine AdventureWorks because I am going to work with the sample AdventureWorks database.  Click OK.. \n{% asset_img image1.png \"Create a Project\" %}\nVisual Studio will build a default database project for you, but it is not connected to anything so there is no actual database scripted out here.  We are going to do that now.  Right click on the database project and a context sensitive menu will popup with Import Database Objects and Settings… click on that now. \n{% asset_img image2.png \"Import Objects\" %}\nThis opens the Import Database Wizard dialog box.  If you have already connected to this database from Visual Studio then you will find an entry in the dropdown control Source database connection.  If not then you will create a new connection by clicking on the New Connection… button.  \n{% asset_img image3.png \"Import Wizard\" %}\nSo if you have a ready made connection in the dropdown, choose it and skip the next screen and step as I am going to build my new connection. \n{% asset_img image4.png \"New Connection\" %}\nBecause my adventure works database in on my local machine I went with that but this database could be a database that is anywhere on your network, this will all just work provided you do have the necessary permissions to connect to it in this way.  Clicking on OK takes us back to the previous screen with the Source database connection filled in. \n\nEveryone, click Start which will bring up the following screen and start to import and script out the database.  When it is all done click the Finish button.  Congratulations you have built a Database Project. \n{% asset_img image5.png \"Import Wizard Finishing\" %}\nYou can expand the solution under Schema Objects, Schemas, and I am showing the dbo schema and it has 3 table scripts.  All the objects of this database are scripted out here.  You can look at these files right here is Visual Studio. \n{% asset_img image6.png \"Solution Explorer\" %}\nHowever you might want to use the Schema View tool for looking at the objects which gives you a more Management Studio type of view. \n{% asset_img image7.png \"Toolbar\" %}\nJust click on the icon in the Solution Explorer that has the popup caption that says Database Schema Viewer. \n{% asset_img image8.png \"Schema View\" %}\n## Updating the Visual Studio Project from the database\nIn the past these were the steps that I would show and demonstrate on how to get a database project scripted out and now that it is code is really easy to get into version control because of the really tight integration from Visual Studio.  My thoughts after that is this is the tool that you should be working in to evolve the database.  Work in Visual Studio and deploy the changes to the database. \n## Light Bulb Moment\nJust recently I discovered how the SQL developer does not really need to leave their favorite tool for working on the database, Management Studio.  That’s right, the new workflow is to continue to make your changes in your local or isolated databases so that you can see first hand how the database changes are going to work.  When you are ready to get those changes into version control you use Visual Studio and the Database Schema comparison. \n{% asset_img image9.png \"Switch Control\" %}\nSo here we see what I always thought was the normal workflow, with the Project on the left and the database that we are going to deploy to on the right.  If instead we are working on the database and we want to push those change to the Project, then switch the source and target around. \n{% asset_img image10.png \"Options\" %}\nNow when you click the OK button you will get a schema comparison just like you always did but when deployed it will check out the project and update the source files.  This will then give you complete history and the files will move through the system from branch to branch with a perfect snapshot of what the database looked like for a specific build. \n{% asset_img image11.png \"Options\" %}\n1.  Click this button to get the party started. \n2.  This comment will disappear in the project source file. \n3.  The source will be checked out during the update. \n## The Recap of what we have just seen.\nThis totally changes my opinion on how to go forward with this great tool.  The fact that we can update the project source from the database was probably always there but if I missed the fact that this was possible then I am sure many others might have missed it as well.  It makes SQL development smooth and safe (all schema scripts under version control) and the ready for the next step to smooth and automated deployment. ","slug":"Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile","published":1,"updated":"2020-01-05T00:36:04.969Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfa0002s4uf4kff0ath","content":"<p>There are a number of good database tools out there for doing database schema comparisons.  I have used different ones over the years at first initially to help me write SQL differencing scripts that I could use when deploying database changes.  If your background is anything like mine where you were namely a Visual Basic or a C# developer and could get by with working on SQL if you could write directly to the database.  There were challenges with being able to script everything out using just SQL.  Today that is not nearly an issue for me and I can do quite a bit with scripting and could build those scripts by hand, but why? </p>\n<h2 id=\"WHAT…-Visual-Studio-for-database-development\"><a href=\"#WHAT…-Visual-Studio-for-database-development\" class=\"headerlink\" title=\"WHAT… Visual Studio for database development?\"></a>WHAT… Visual Studio for database development?</h2><p>Over the years I have tried to promote SQL development to be done in Visual Studio.  I made a great case, SQL is code just as much as my VB, C#, F# or what ever your favorite language of choice happens to be and should be protected in source control.  Makes sense but it is a really hard sell.  Productivity goes down hill, errors begin to happen because this is not how the SQL teams are used to working on databases.  It was an easier sell for me because I loved working in Visual Studio and found the SQL tools not to be as intuitive to me.  I have never been able to figure out how I could walk through a stored procedure in Query Analyzer or Management Studio but have always been able to do this with stored procedures that I wrote from within Visual Studio and that was long before the data editions of Visual Studio. </p>\n<p>Ever since the release of the Data Dude or its official name back then, Visual Studio Team Edition for Database Professionals, this was what I did and I tried to convince others that this is what we should be doing.  It was never an easy sell, yea the schema comparison was nice but our SQL professionals already had all kinds of comparison tools for SQL and it would be too hard for them to work this way.  They wanted to be able to make changes in a database and see the results of those changes, not have to deploy it somewhere first. </p>\n<p>So as a quick summary of what we figured out so far.  Schema comparison from one database to another, nothing new, your SQL department probably has a number of these and use them to generate their change scripts.  How is Visual Studio schema comparison better than what I already have how is it going to go the extra mile?  That my friend starts with the database project which does a reverse engineering of sorts of what you have in the database and scripts the whole thing out into source files that you can check into source control and compare the changes just like you do with any other source code. </p>\n<p>Now once you have a database project you are able to not just do a schema comparison with two databases but you can also compare from a database and this project.  The extra mile is that I can even go so far as to deploy the differences to your test and production databases.  It gets even better but before I tell you the best part lets go through the actual steps that you would take to create this initial database project. </p>\n<h2 id=\"Create-the-Database-Project\"><a href=\"#Create-the-Database-Project\" class=\"headerlink\" title=\"Create the Database Project\"></a>Create the Database Project</h2><p>I am going to walk you through the very simple steps that it takes to build a database project for the AdventureWorks database.  For this you will need Visual Studio 2010 Premium edition or higher. </p>\n<p>We start by creating a new project and select “SQL Server 2008 Database Project” template from under the Database - SQL Server project types.  Give it a name and set the location.  I called mine AdventureWorks because I am going to work with the sample AdventureWorks database.  Click OK..<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image1.png\" title=\"Create a Project\"><br>Visual Studio will build a default database project for you, but it is not connected to anything so there is no actual database scripted out here.  We are going to do that now.  Right click on the database project and a context sensitive menu will popup with Import Database Objects and Settings… click on that now.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image2.png\" title=\"Import Objects\"><br>This opens the Import Database Wizard dialog box.  If you have already connected to this database from Visual Studio then you will find an entry in the dropdown control Source database connection.  If not then you will create a new connection by clicking on the New Connection… button.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image3.png\" title=\"Import Wizard\"><br>So if you have a ready made connection in the dropdown, choose it and skip the next screen and step as I am going to build my new connection.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image4.png\" title=\"New Connection\"><br>Because my adventure works database in on my local machine I went with that but this database could be a database that is anywhere on your network, this will all just work provided you do have the necessary permissions to connect to it in this way.  Clicking on OK takes us back to the previous screen with the Source database connection filled in. </p>\n<p>Everyone, click Start which will bring up the following screen and start to import and script out the database.  When it is all done click the Finish button.  Congratulations you have built a Database Project.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image5.png\" title=\"Import Wizard Finishing\"><br>You can expand the solution under Schema Objects, Schemas, and I am showing the dbo schema and it has 3 table scripts.  All the objects of this database are scripted out here.  You can look at these files right here is Visual Studio.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image6.png\" title=\"Solution Explorer\"><br>However you might want to use the Schema View tool for looking at the objects which gives you a more Management Studio type of view.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image7.png\" title=\"Toolbar\"><br>Just click on the icon in the Solution Explorer that has the popup caption that says Database Schema Viewer.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image8.png\" title=\"Schema View\"></p>\n<h2 id=\"Updating-the-Visual-Studio-Project-from-the-database\"><a href=\"#Updating-the-Visual-Studio-Project-from-the-database\" class=\"headerlink\" title=\"Updating the Visual Studio Project from the database\"></a>Updating the Visual Studio Project from the database</h2><p>In the past these were the steps that I would show and demonstrate on how to get a database project scripted out and now that it is code is really easy to get into version control because of the really tight integration from Visual Studio.  My thoughts after that is this is the tool that you should be working in to evolve the database.  Work in Visual Studio and deploy the changes to the database. </p>\n<h2 id=\"Light-Bulb-Moment\"><a href=\"#Light-Bulb-Moment\" class=\"headerlink\" title=\"Light Bulb Moment\"></a>Light Bulb Moment</h2><p>Just recently I discovered how the SQL developer does not really need to leave their favorite tool for working on the database, Management Studio.  That’s right, the new workflow is to continue to make your changes in your local or isolated databases so that you can see first hand how the database changes are going to work.  When you are ready to get those changes into version control you use Visual Studio and the Database Schema comparison.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image9.png\" title=\"Switch Control\"><br>So here we see what I always thought was the normal workflow, with the Project on the left and the database that we are going to deploy to on the right.  If instead we are working on the database and we want to push those change to the Project, then switch the source and target around.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image10.png\" title=\"Options\"><br>Now when you click the OK button you will get a schema comparison just like you always did but when deployed it will check out the project and update the source files.  This will then give you complete history and the files will move through the system from branch to branch with a perfect snapshot of what the database looked like for a specific build.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image11.png\" title=\"Options\"></p>\n<ol>\n<li>Click this button to get the party started. </li>\n<li>This comment will disappear in the project source file. </li>\n<li>The source will be checked out during the update. <h2 id=\"The-Recap-of-what-we-have-just-seen\"><a href=\"#The-Recap-of-what-we-have-just-seen\" class=\"headerlink\" title=\"The Recap of what we have just seen.\"></a>The Recap of what we have just seen.</h2>This totally changes my opinion on how to go forward with this great tool.  The fact that we can update the project source from the database was probably always there but if I missed the fact that this was possible then I am sure many others might have missed it as well.  It makes SQL development smooth and safe (all schema scripts under version control) and the ready for the next step to smooth and automated deployment. </li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>There are a number of good database tools out there for doing database schema comparisons.  I have used different ones over the years at first initially to help me write SQL differencing scripts that I could use when deploying database changes.  If your background is anything like mine where you were namely a Visual Basic or a C# developer and could get by with working on SQL if you could write directly to the database.  There were challenges with being able to script everything out using just SQL.  Today that is not nearly an issue for me and I can do quite a bit with scripting and could build those scripts by hand, but why? </p>\n<h2 id=\"WHAT…-Visual-Studio-for-database-development\"><a href=\"#WHAT…-Visual-Studio-for-database-development\" class=\"headerlink\" title=\"WHAT… Visual Studio for database development?\"></a>WHAT… Visual Studio for database development?</h2><p>Over the years I have tried to promote SQL development to be done in Visual Studio.  I made a great case, SQL is code just as much as my VB, C#, F# or what ever your favorite language of choice happens to be and should be protected in source control.  Makes sense but it is a really hard sell.  Productivity goes down hill, errors begin to happen because this is not how the SQL teams are used to working on databases.  It was an easier sell for me because I loved working in Visual Studio and found the SQL tools not to be as intuitive to me.  I have never been able to figure out how I could walk through a stored procedure in Query Analyzer or Management Studio but have always been able to do this with stored procedures that I wrote from within Visual Studio and that was long before the data editions of Visual Studio. </p>\n<p>Ever since the release of the Data Dude or its official name back then, Visual Studio Team Edition for Database Professionals, this was what I did and I tried to convince others that this is what we should be doing.  It was never an easy sell, yea the schema comparison was nice but our SQL professionals already had all kinds of comparison tools for SQL and it would be too hard for them to work this way.  They wanted to be able to make changes in a database and see the results of those changes, not have to deploy it somewhere first. </p>\n<p>So as a quick summary of what we figured out so far.  Schema comparison from one database to another, nothing new, your SQL department probably has a number of these and use them to generate their change scripts.  How is Visual Studio schema comparison better than what I already have how is it going to go the extra mile?  That my friend starts with the database project which does a reverse engineering of sorts of what you have in the database and scripts the whole thing out into source files that you can check into source control and compare the changes just like you do with any other source code. </p>\n<p>Now once you have a database project you are able to not just do a schema comparison with two databases but you can also compare from a database and this project.  The extra mile is that I can even go so far as to deploy the differences to your test and production databases.  It gets even better but before I tell you the best part lets go through the actual steps that you would take to create this initial database project. </p>\n<h2 id=\"Create-the-Database-Project\"><a href=\"#Create-the-Database-Project\" class=\"headerlink\" title=\"Create the Database Project\"></a>Create the Database Project</h2><p>I am going to walk you through the very simple steps that it takes to build a database project for the AdventureWorks database.  For this you will need Visual Studio 2010 Premium edition or higher. </p>\n<p>We start by creating a new project and select “SQL Server 2008 Database Project” template from under the Database - SQL Server project types.  Give it a name and set the location.  I called mine AdventureWorks because I am going to work with the sample AdventureWorks database.  Click OK..<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image1.png\" title=\"Create a Project\"><br>Visual Studio will build a default database project for you, but it is not connected to anything so there is no actual database scripted out here.  We are going to do that now.  Right click on the database project and a context sensitive menu will popup with Import Database Objects and Settings… click on that now.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image2.png\" title=\"Import Objects\"><br>This opens the Import Database Wizard dialog box.  If you have already connected to this database from Visual Studio then you will find an entry in the dropdown control Source database connection.  If not then you will create a new connection by clicking on the New Connection… button.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image3.png\" title=\"Import Wizard\"><br>So if you have a ready made connection in the dropdown, choose it and skip the next screen and step as I am going to build my new connection.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image4.png\" title=\"New Connection\"><br>Because my adventure works database in on my local machine I went with that but this database could be a database that is anywhere on your network, this will all just work provided you do have the necessary permissions to connect to it in this way.  Clicking on OK takes us back to the previous screen with the Source database connection filled in. </p>\n<p>Everyone, click Start which will bring up the following screen and start to import and script out the database.  When it is all done click the Finish button.  Congratulations you have built a Database Project.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image5.png\" title=\"Import Wizard Finishing\"><br>You can expand the solution under Schema Objects, Schemas, and I am showing the dbo schema and it has 3 table scripts.  All the objects of this database are scripted out here.  You can look at these files right here is Visual Studio.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image6.png\" title=\"Solution Explorer\"><br>However you might want to use the Schema View tool for looking at the objects which gives you a more Management Studio type of view.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image7.png\" title=\"Toolbar\"><br>Just click on the icon in the Solution Explorer that has the popup caption that says Database Schema Viewer.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image8.png\" title=\"Schema View\"></p>\n<h2 id=\"Updating-the-Visual-Studio-Project-from-the-database\"><a href=\"#Updating-the-Visual-Studio-Project-from-the-database\" class=\"headerlink\" title=\"Updating the Visual Studio Project from the database\"></a>Updating the Visual Studio Project from the database</h2><p>In the past these were the steps that I would show and demonstrate on how to get a database project scripted out and now that it is code is really easy to get into version control because of the really tight integration from Visual Studio.  My thoughts after that is this is the tool that you should be working in to evolve the database.  Work in Visual Studio and deploy the changes to the database. </p>\n<h2 id=\"Light-Bulb-Moment\"><a href=\"#Light-Bulb-Moment\" class=\"headerlink\" title=\"Light Bulb Moment\"></a>Light Bulb Moment</h2><p>Just recently I discovered how the SQL developer does not really need to leave their favorite tool for working on the database, Management Studio.  That’s right, the new workflow is to continue to make your changes in your local or isolated databases so that you can see first hand how the database changes are going to work.  When you are ready to get those changes into version control you use Visual Studio and the Database Schema comparison.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image9.png\" title=\"Switch Control\"><br>So here we see what I always thought was the normal workflow, with the Project on the left and the database that we are going to deploy to on the right.  If instead we are working on the database and we want to push those change to the Project, then switch the source and target around.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image10.png\" title=\"Options\"><br>Now when you click the OK button you will get a schema comparison just like you always did but when deployed it will check out the project and update the source files.  This will then give you complete history and the files will move through the system from branch to branch with a perfect snapshot of what the database looked like for a specific build.<br><img src=\"/2011/01/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image11.png\" title=\"Options\"></p>\n<ol>\n<li>Click this button to get the party started. </li>\n<li>This comment will disappear in the project source file. </li>\n<li>The source will be checked out during the update. <h2 id=\"The-Recap-of-what-we-have-just-seen\"><a href=\"#The-Recap-of-what-we-have-just-seen\" class=\"headerlink\" title=\"The Recap of what we have just seen.\"></a>The Recap of what we have just seen.</h2>This totally changes my opinion on how to go forward with this great tool.  The fact that we can update the project source from the database was probably always there but if I missed the fact that this was possible then I am sure many others might have missed it as well.  It makes SQL development smooth and safe (all schema scripts under version control) and the ready for the next step to smooth and automated deployment. </li>\n</ol>\n"},{"title":"An Argument against the Date Based Version Number","date":"2017-02-23T05:37:07.000Z","_content":"{% img right /images/version.jpg 250 250 \"Version Sample\" %}\nIn the past I have followed two types of version numbers for the products that I build and support on the side.  Products that were customer facing all followed the Semantic concept of version control.  If there was a big change but not breaking then the minor number incremented.  If the change could have potential breaking changes then the Major number was incremented.  This concept works well in that everytime that code was changed the third digit, the build number was incremented.  We ignored the fourth number which was the revision as that was just a number to keep the build ID which was a makeup of the major, minor, build and revision, unique.  If I have 1 through 18 in revision numbers all for the same build, it means that nothing in the code has changed since revision 1. We are working on changes to the actual build definition and these are just builds of the same code.\n\n## Projects that are not Customer facing\n{% img left /images/internal.png 250 250 \"Internal Projects\" %}\nFor other types of products, things that I used internally were given a different format because at the time I didn't think it mattered and my only goal was to be able to look at the properties of an assembly and know which build it came from.  For that I used a format that would change automatically for each build and I would never have to change any of the version numbers ever.  This format followed a pattern like YY.MM.DD.RR, the RR representing the revision number that I allow TFS to create automatically to keep the build number unique.  So for a build that was run on say February 23, 2017 that version number could look like: \n```\n17.02.23.01\n```\nI would use a PowerShell script to write this to the assembly just prior to compile time and this would work as my version number.\n\nI have used this format for years and there have been many blog posts on how to do this automatically in TFS ever since the 2010 release.  Back then we were building activites to be used in the xaml builds and since then the ALM rangers have converted this into a PowerShell script as part of the Build Extensions, as well as many others that are available in the TFS Market Place as a build task.  The basic idea is to have this format as part of the build definition and most of these tools will extract the version like number out of the build name and that becomes the version number.\n```\nBuild number format: MyBuildName_v$(Year:yy).$(Month).$(DayOfMonth)$(Rev:.rr)\n```\n## But I Have Changed My Mind\n{% img right /images/changed.jpg 250 250 \"Version Sample\" %}\nSince moving into a more DevOps mindset if you please, I was beginning to see that I was loosing some valuable information about my internal builds.  I had no way of knowing when an actual code change occured because if I built the product on Feb 23 and then built it again on Feb 24 because I wanted to try something on the build machine there was no way to tell from the build number or the version of the assembly if anything had changed.  This is important stuff, but I also did not want to have to manually tweak the build number every time I did push something new into production and looking back at my old post [My New 3 Rules for Releases](../../../2016/09/My-New-3-Rules-for-Releases/index.html) the tools and solution to accomplish this were right at my finger tips.\n\n## But this is done on the Releases\nYes, they are and guess what? I did not have a formal release pipeline for some of these internal products.  Hey some of them were just packaged up as Nuget packages with a wrapper of Chocolate.  You will want to check out my post on [How I Use Chocolatey in my Releases](../../../2016/06/How-I-Use-Chocolatey-in-my-Releases/index.html) to really understand what I am talking about here.  \n\nAfter thinking about this for a while and having similar discussions with clients I came up with the idea of having at a minimum a Dev and Prod environment.  The Dev environment would do what I pretty much have always done, it would deploy the application and maybe even run some tests to verify that the build has been sucessful.  Sometimes I find issues here and I return back to the source, fix it up and send out another build.  \n\nWhen I am happy with the results I promote it to Production.  The promotion does not do anything to any environment or machine but does lock the build, increment the build number and my newest thing create a Release work item.\n\n## Why Create a Release Work item\nI will talk about this feature in more detail with some code samples in a future post. Briefly, the whole reason for the creation of a Release work item when I deploy to Production is to keep track of how many releases I have done in the last quarter.  I love good metrics and this is one that lets me know I am pushing code out into production and not just tweaking it to death.  Remember you can't get good feedback if you don't get it out there.\n\n## In Conclusion\nSo there you have it, all my products internal or customer facing I have much more clarity as to when a build has new code in it.  I could have gone though source control and found out from the code history and found the lastest changeset number and see the first time that this was used in a build but so much work for something that I can see at a glance and not having to look anywhere else for it. ","source":"_posts/An-Argument-against-the-Date-Based-Version-Number.md","raw":"---\ntitle: An Argument against the Date Based Version Number\ndate: 2017-02-22 21:37:07\ntags:\n- ALM\n- Products\n---\n{% img right /images/version.jpg 250 250 \"Version Sample\" %}\nIn the past I have followed two types of version numbers for the products that I build and support on the side.  Products that were customer facing all followed the Semantic concept of version control.  If there was a big change but not breaking then the minor number incremented.  If the change could have potential breaking changes then the Major number was incremented.  This concept works well in that everytime that code was changed the third digit, the build number was incremented.  We ignored the fourth number which was the revision as that was just a number to keep the build ID which was a makeup of the major, minor, build and revision, unique.  If I have 1 through 18 in revision numbers all for the same build, it means that nothing in the code has changed since revision 1. We are working on changes to the actual build definition and these are just builds of the same code.\n\n## Projects that are not Customer facing\n{% img left /images/internal.png 250 250 \"Internal Projects\" %}\nFor other types of products, things that I used internally were given a different format because at the time I didn't think it mattered and my only goal was to be able to look at the properties of an assembly and know which build it came from.  For that I used a format that would change automatically for each build and I would never have to change any of the version numbers ever.  This format followed a pattern like YY.MM.DD.RR, the RR representing the revision number that I allow TFS to create automatically to keep the build number unique.  So for a build that was run on say February 23, 2017 that version number could look like: \n```\n17.02.23.01\n```\nI would use a PowerShell script to write this to the assembly just prior to compile time and this would work as my version number.\n\nI have used this format for years and there have been many blog posts on how to do this automatically in TFS ever since the 2010 release.  Back then we were building activites to be used in the xaml builds and since then the ALM rangers have converted this into a PowerShell script as part of the Build Extensions, as well as many others that are available in the TFS Market Place as a build task.  The basic idea is to have this format as part of the build definition and most of these tools will extract the version like number out of the build name and that becomes the version number.\n```\nBuild number format: MyBuildName_v$(Year:yy).$(Month).$(DayOfMonth)$(Rev:.rr)\n```\n## But I Have Changed My Mind\n{% img right /images/changed.jpg 250 250 \"Version Sample\" %}\nSince moving into a more DevOps mindset if you please, I was beginning to see that I was loosing some valuable information about my internal builds.  I had no way of knowing when an actual code change occured because if I built the product on Feb 23 and then built it again on Feb 24 because I wanted to try something on the build machine there was no way to tell from the build number or the version of the assembly if anything had changed.  This is important stuff, but I also did not want to have to manually tweak the build number every time I did push something new into production and looking back at my old post [My New 3 Rules for Releases](../../../2016/09/My-New-3-Rules-for-Releases/index.html) the tools and solution to accomplish this were right at my finger tips.\n\n## But this is done on the Releases\nYes, they are and guess what? I did not have a formal release pipeline for some of these internal products.  Hey some of them were just packaged up as Nuget packages with a wrapper of Chocolate.  You will want to check out my post on [How I Use Chocolatey in my Releases](../../../2016/06/How-I-Use-Chocolatey-in-my-Releases/index.html) to really understand what I am talking about here.  \n\nAfter thinking about this for a while and having similar discussions with clients I came up with the idea of having at a minimum a Dev and Prod environment.  The Dev environment would do what I pretty much have always done, it would deploy the application and maybe even run some tests to verify that the build has been sucessful.  Sometimes I find issues here and I return back to the source, fix it up and send out another build.  \n\nWhen I am happy with the results I promote it to Production.  The promotion does not do anything to any environment or machine but does lock the build, increment the build number and my newest thing create a Release work item.\n\n## Why Create a Release Work item\nI will talk about this feature in more detail with some code samples in a future post. Briefly, the whole reason for the creation of a Release work item when I deploy to Production is to keep track of how many releases I have done in the last quarter.  I love good metrics and this is one that lets me know I am pushing code out into production and not just tweaking it to death.  Remember you can't get good feedback if you don't get it out there.\n\n## In Conclusion\nSo there you have it, all my products internal or customer facing I have much more clarity as to when a build has new code in it.  I could have gone though source control and found out from the code history and found the lastest changeset number and see the first time that this was used in a build but so much work for something that I can see at a glance and not having to look anywhere else for it. ","slug":"An-Argument-against-the-Date-Based-Version-Number","published":1,"updated":"2020-01-05T00:36:04.968Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfb0003s4uf88twwpd0","content":"<img src=\"/images/version.jpg\" class=\"right\" width=\"250\" height=\"250\" title=\"Version Sample\">\n<p>In the past I have followed two types of version numbers for the products that I build and support on the side.  Products that were customer facing all followed the Semantic concept of version control.  If there was a big change but not breaking then the minor number incremented.  If the change could have potential breaking changes then the Major number was incremented.  This concept works well in that everytime that code was changed the third digit, the build number was incremented.  We ignored the fourth number which was the revision as that was just a number to keep the build ID which was a makeup of the major, minor, build and revision, unique.  If I have 1 through 18 in revision numbers all for the same build, it means that nothing in the code has changed since revision 1. We are working on changes to the actual build definition and these are just builds of the same code.</p>\n<h2 id=\"Projects-that-are-not-Customer-facing\"><a href=\"#Projects-that-are-not-Customer-facing\" class=\"headerlink\" title=\"Projects that are not Customer facing\"></a>Projects that are not Customer facing</h2><img src=\"/images/internal.png\" class=\"left\" width=\"250\" height=\"250\" title=\"Internal Projects\">\n<p>For other types of products, things that I used internally were given a different format because at the time I didn’t think it mattered and my only goal was to be able to look at the properties of an assembly and know which build it came from.  For that I used a format that would change automatically for each build and I would never have to change any of the version numbers ever.  This format followed a pattern like YY.MM.DD.RR, the RR representing the revision number that I allow TFS to create automatically to keep the build number unique.  So for a build that was run on say February 23, 2017 that version number could look like:<br><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">17<span class=\"selector-class\">.02</span><span class=\"selector-class\">.23</span><span class=\"selector-class\">.01</span></span><br></pre></td></tr></table></figure></p>\n<p>I would use a PowerShell script to write this to the assembly just prior to compile time and this would work as my version number.</p>\n<p>I have used this format for years and there have been many blog posts on how to do this automatically in TFS ever since the 2010 release.  Back then we were building activites to be used in the xaml builds and since then the ALM rangers have converted this into a PowerShell script as part of the Build Extensions, as well as many others that are available in the TFS Market Place as a build task.  The basic idea is to have this format as part of the build definition and most of these tools will extract the version like number out of the build name and that becomes the version number.<br><figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Build number <span class=\"keyword\">format</span>: MyBuildName_v$(Year:yy).$(Month).$(DayOfMonth)$(Rev:.rr)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"But-I-Have-Changed-My-Mind\"><a href=\"#But-I-Have-Changed-My-Mind\" class=\"headerlink\" title=\"But I Have Changed My Mind\"></a>But I Have Changed My Mind</h2><img src=\"/images/changed.jpg\" class=\"right\" width=\"250\" height=\"250\" title=\"Version Sample\">\n<p>Since moving into a more DevOps mindset if you please, I was beginning to see that I was loosing some valuable information about my internal builds.  I had no way of knowing when an actual code change occured because if I built the product on Feb 23 and then built it again on Feb 24 because I wanted to try something on the build machine there was no way to tell from the build number or the version of the assembly if anything had changed.  This is important stuff, but I also did not want to have to manually tweak the build number every time I did push something new into production and looking back at my old post <a href=\"../../../2016/09/My-New-3-Rules-for-Releases/index.html\">My New 3 Rules for Releases</a> the tools and solution to accomplish this were right at my finger tips.</p>\n<h2 id=\"But-this-is-done-on-the-Releases\"><a href=\"#But-this-is-done-on-the-Releases\" class=\"headerlink\" title=\"But this is done on the Releases\"></a>But this is done on the Releases</h2><p>Yes, they are and guess what? I did not have a formal release pipeline for some of these internal products.  Hey some of them were just packaged up as Nuget packages with a wrapper of Chocolate.  You will want to check out my post on <a href=\"../../../2016/06/How-I-Use-Chocolatey-in-my-Releases/index.html\">How I Use Chocolatey in my Releases</a> to really understand what I am talking about here.  </p>\n<p>After thinking about this for a while and having similar discussions with clients I came up with the idea of having at a minimum a Dev and Prod environment.  The Dev environment would do what I pretty much have always done, it would deploy the application and maybe even run some tests to verify that the build has been sucessful.  Sometimes I find issues here and I return back to the source, fix it up and send out another build.  </p>\n<p>When I am happy with the results I promote it to Production.  The promotion does not do anything to any environment or machine but does lock the build, increment the build number and my newest thing create a Release work item.</p>\n<h2 id=\"Why-Create-a-Release-Work-item\"><a href=\"#Why-Create-a-Release-Work-item\" class=\"headerlink\" title=\"Why Create a Release Work item\"></a>Why Create a Release Work item</h2><p>I will talk about this feature in more detail with some code samples in a future post. Briefly, the whole reason for the creation of a Release work item when I deploy to Production is to keep track of how many releases I have done in the last quarter.  I love good metrics and this is one that lets me know I am pushing code out into production and not just tweaking it to death.  Remember you can’t get good feedback if you don’t get it out there.</p>\n<h2 id=\"In-Conclusion\"><a href=\"#In-Conclusion\" class=\"headerlink\" title=\"In Conclusion\"></a>In Conclusion</h2><p>So there you have it, all my products internal or customer facing I have much more clarity as to when a build has new code in it.  I could have gone though source control and found out from the code history and found the lastest changeset number and see the first time that this was used in a build but so much work for something that I can see at a glance and not having to look anywhere else for it. </p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/version.jpg\" class=\"right\" width=\"250\" height=\"250\" title=\"Version Sample\">\n<p>In the past I have followed two types of version numbers for the products that I build and support on the side.  Products that were customer facing all followed the Semantic concept of version control.  If there was a big change but not breaking then the minor number incremented.  If the change could have potential breaking changes then the Major number was incremented.  This concept works well in that everytime that code was changed the third digit, the build number was incremented.  We ignored the fourth number which was the revision as that was just a number to keep the build ID which was a makeup of the major, minor, build and revision, unique.  If I have 1 through 18 in revision numbers all for the same build, it means that nothing in the code has changed since revision 1. We are working on changes to the actual build definition and these are just builds of the same code.</p>\n<h2 id=\"Projects-that-are-not-Customer-facing\"><a href=\"#Projects-that-are-not-Customer-facing\" class=\"headerlink\" title=\"Projects that are not Customer facing\"></a>Projects that are not Customer facing</h2><img src=\"/images/internal.png\" class=\"left\" width=\"250\" height=\"250\" title=\"Internal Projects\">\n<p>For other types of products, things that I used internally were given a different format because at the time I didn’t think it mattered and my only goal was to be able to look at the properties of an assembly and know which build it came from.  For that I used a format that would change automatically for each build and I would never have to change any of the version numbers ever.  This format followed a pattern like YY.MM.DD.RR, the RR representing the revision number that I allow TFS to create automatically to keep the build number unique.  So for a build that was run on say February 23, 2017 that version number could look like:<br><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">17<span class=\"selector-class\">.02</span><span class=\"selector-class\">.23</span><span class=\"selector-class\">.01</span></span><br></pre></td></tr></table></figure></p>\n<p>I would use a PowerShell script to write this to the assembly just prior to compile time and this would work as my version number.</p>\n<p>I have used this format for years and there have been many blog posts on how to do this automatically in TFS ever since the 2010 release.  Back then we were building activites to be used in the xaml builds and since then the ALM rangers have converted this into a PowerShell script as part of the Build Extensions, as well as many others that are available in the TFS Market Place as a build task.  The basic idea is to have this format as part of the build definition and most of these tools will extract the version like number out of the build name and that becomes the version number.<br><figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Build number <span class=\"keyword\">format</span>: MyBuildName_v$(Year:yy).$(Month).$(DayOfMonth)$(Rev:.rr)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"But-I-Have-Changed-My-Mind\"><a href=\"#But-I-Have-Changed-My-Mind\" class=\"headerlink\" title=\"But I Have Changed My Mind\"></a>But I Have Changed My Mind</h2><img src=\"/images/changed.jpg\" class=\"right\" width=\"250\" height=\"250\" title=\"Version Sample\">\n<p>Since moving into a more DevOps mindset if you please, I was beginning to see that I was loosing some valuable information about my internal builds.  I had no way of knowing when an actual code change occured because if I built the product on Feb 23 and then built it again on Feb 24 because I wanted to try something on the build machine there was no way to tell from the build number or the version of the assembly if anything had changed.  This is important stuff, but I also did not want to have to manually tweak the build number every time I did push something new into production and looking back at my old post <a href=\"../../../2016/09/My-New-3-Rules-for-Releases/index.html\">My New 3 Rules for Releases</a> the tools and solution to accomplish this were right at my finger tips.</p>\n<h2 id=\"But-this-is-done-on-the-Releases\"><a href=\"#But-this-is-done-on-the-Releases\" class=\"headerlink\" title=\"But this is done on the Releases\"></a>But this is done on the Releases</h2><p>Yes, they are and guess what? I did not have a formal release pipeline for some of these internal products.  Hey some of them were just packaged up as Nuget packages with a wrapper of Chocolate.  You will want to check out my post on <a href=\"../../../2016/06/How-I-Use-Chocolatey-in-my-Releases/index.html\">How I Use Chocolatey in my Releases</a> to really understand what I am talking about here.  </p>\n<p>After thinking about this for a while and having similar discussions with clients I came up with the idea of having at a minimum a Dev and Prod environment.  The Dev environment would do what I pretty much have always done, it would deploy the application and maybe even run some tests to verify that the build has been sucessful.  Sometimes I find issues here and I return back to the source, fix it up and send out another build.  </p>\n<p>When I am happy with the results I promote it to Production.  The promotion does not do anything to any environment or machine but does lock the build, increment the build number and my newest thing create a Release work item.</p>\n<h2 id=\"Why-Create-a-Release-Work-item\"><a href=\"#Why-Create-a-Release-Work-item\" class=\"headerlink\" title=\"Why Create a Release Work item\"></a>Why Create a Release Work item</h2><p>I will talk about this feature in more detail with some code samples in a future post. Briefly, the whole reason for the creation of a Release work item when I deploy to Production is to keep track of how many releases I have done in the last quarter.  I love good metrics and this is one that lets me know I am pushing code out into production and not just tweaking it to death.  Remember you can’t get good feedback if you don’t get it out there.</p>\n<h2 id=\"In-Conclusion\"><a href=\"#In-Conclusion\" class=\"headerlink\" title=\"In Conclusion\"></a>In Conclusion</h2><p>So there you have it, all my products internal or customer facing I have much more clarity as to when a build has new code in it.  I could have gone though source control and found out from the code history and found the lastest changeset number and see the first time that this was used in a build but so much work for something that I can see at a glance and not having to look anywhere else for it. </p>\n"},{"title":"C#.net or VB.net","date":"2008-06-21T07:00:00.000Z","_content":"## Starting from the Beginning\nI have been a Visual Basic developer for over ten years now.  It was not my first language that got me excited about programming.  No, that would have been Clipper.  I accidentally fell into Clipper much the same way that Visual Basic started as an experiment for me.  \n\nIt was sometime around 1987 or 1988 when I was working as the accountant and network administrator for my family’s car dealership.  I was running into limitations in the current accounting software we were using and I knew that a change would be needed.  However, I had great difficulty in finding software that had the functionality and flexibility that was required.  I started looking into some source code solutions and found one that was originally written in dBase III and was Clipper ready.  This was my start to serious programming as this source code did not work and it took me two months to work through the code learning to work with Clipper as I went along. \n\nClipper 4 worked well for me, which was a very process level language.  There were no surprises.  The user could only process data in the exact steps designed in the software.  When Clipper 5 was released I upgraded, which exposed me to some new and unfamiliar aspects of programming.  Clipper introduced three “built in objects” and soon several “third party vendors” started coming out with add-ons for Clipper that allowed the creation of your own objects and classes.  You should realize that by this time I was becoming quite the Clipper programmer.  I was designing new features to our accounting software and building complementary add-ons.  I was experimenting with Windows but was never able to implement it into the dealership until Windows 3.1 was released.  However, we were running our Accounting software in a DOS box through Windows.  Nothing special here, but it worked.  Nantucket, the company that owned Clipper, made a lot of promises that there would be a Windows version of Clipper coming out soon. \n\nIn the mean time, I read an article by a fellow Clipper guru that suggested looking into Visual Basic to get a better handle on working with objects.  So I got a copy of Visual Basic 1.0 for exactly that purpose; to get a better understanding about how objects worked and be able to actually write a Window application.  I was still thinking that Clipper was going to be my main programming language with Visual Basic as a tool to learn about objects.  This was similar to the original purpose of Pascal, which was to learn structured programming.  Anyway, I was having a great time with Visual Basic, reading a book or two on it and building some really simple programs. \n\nOn a flight to Comdex one year, I was sitting next to two guys from Sequiter Software.  They noticed that I was reading one of my Visual Basic books and asked me how I liked Visual Basic?  I told them I was enjoying it very much but I was really a Clipper programmer and it would be great if Visual Basic could access a database.  Well what followed was some very interesting information that they shared with me: Sequiter makes a product called CodeBase which is a very fast engine that reads and writes to dBase, Fox, and Clipper data.  I could use it in Visual Basic by declaring the procedures to the API from within Visual Basic.  Well that was it: from then on Visual Basic became a very important tool in what would later become my programming career.  Remember I was an accountant that couldn’t find good software that worked the way I needed it to work for me. \n\nSo I guess you could say that I have been building database applications in Visual Basic since version 1.0.  Just so that you are clear on who I really am, I did not take any short cuts to change careers.  I did take a number of correspondence courses where I studied Assembler, Basic, Pascal, C, and COBOL and then went to College where I graduated with distinction.  I am constantly learning and doing the best I can to keep up on the latest technology and am always interested in creating better software products. \n## My Take on C#.net vs. VB.net\nC# (sharp) is the language that was designed for Dot Net and was used to build much of the Framework.  It shares the same first name as its big brother C++, but it doesn’t really feel like any C++ that I have ever taken out for a spin.  C# really is a cross between the best of Visual Basic, the best of C++, and some elements of Java which make it the perfect language for Dot Net.  \n\nMicrosoft made an announcement that the new version of Visual Basic would finally be corrected to match the standards of other languages.  There have always been inconsistencies between VB and other languages when it came to the number of elements that were created in an array and the value of true.  Hey, somewhere along the way, this now very powerful language got itself all screwed up, but then VB.net is the first version where the forms are not being handled by the Ruby Engine.  The operating system will actually be in charge of what we see on the screen.  Anyway, at the time of this announcement I was all excited about the future of VB and what the heck was C# anyway, it did not even seem too important to me. \n\nThen sometime in the early part of 2002, Microsoft made the announcement that I think surprised almost every serious VB programmer.  They were going to re-tool Visual Basic.net to make it more compatible to Visual Basic 6.  Well this was enough for me to consider looking into C# more seriously. \n\nBefore we go on I thought I would take a moment to talk about the bad rap that VB (as a language) has been given.  VB has been attacked over the years by the programming community for not being a very serious language.  The language started off as a very simple tool to build desktop applications for Windows.  Over the years VB has become a very powerful programming language and probably its curse has been the ease in which you can build Windows applications.  I say a curse, because anyone who has worked with this language has been able to build a working program quickly and easily.  On the other hand you would not typically take a language like C++ straight out of the box and start writing a program without at least taking a number of courses and reading a few books on the subject.  You might give up and leave that sort of programming to the professionals.  But none of that is involved when programming in VB since it is such a forgiving language.  However, even with VB’s simplicity there is a lot more to writing a good solid VB program then just a piece of code that works.  There is choosing good programming practices and constantly refining your skill so that you write the most efficient and easily maintainable code that you can possibly write.  The bad rap really belongs to the VB programmers that have picked up some very bad habits along the way and have failed to refine their skill to build elegant and well managed code.  There are bad C++ programmers out there too just not as many. \n\nI have spent a fair amount of time in a variety of C and C++ environments and have found that it was just too much work to build a Windows desktop application.  Visual Basic makes a lot of sense since that was how it was designed.  C on the other hand has it roots in building drivers and operating systems and I do not typically engage in those kinds of projects. \n## Making the Change\nI am leaning towards programming in C# instead of VB but not just because I am upset with the decisions that were made on the future of VB: I need something much more powerful then that to justify my reasons. \n\nOne of the things I really like in C# is the new inline XML style comments.  This is not available in VB.net.  With this I am able to produce some very clear comments in my code, where they happen and with them produce a technical document.  Many times in the past I have had to duplicate some of this effort, and then update my documents when the code went through some changes.  Not anymore, it is all in one place and as I make changes my documents are also updated. \n\nSecondly, Dot Net is built around this new concept of Namespaces, which is the way that Microsoft is getting around the DLL hell issues that have plagued us for years.  I have some interesting stories to tell on that subject but will need to wait for another time.  In C#, the Namespace is exposed right out there directly in your face.  You can adjust the Namespace in VB.net but you need to do this through the IDE and is just not in your face.  I have done some work with multiple projects that support other projects and I just think it is a lot cleaner when I have total control over the Namespace. \n\nThirdly, there is the learning curve.  VB.net is not just an upgrade from Visual Basic.  It is a new life style and you really need to get into that life style if you really want to take advantage of the Frame Work and go beyond what we have coded and designed in the past.  This Frame Work is wonderful and I am almost tempted to say that the only limitation is lack of our imagination.  Since I started getting into C#, I have had to take each step with a fresh new approach.  When I was playing with the early beta’s I found that I was constantly doing a comparison with the previous version of VB.  I think my return on investment with C# is a whole lot better then if I had gone the VB.net route.  Something to keep in mind, Dot Net programming is for new programs, not to port over a working application.  Microsoft has made sure that the Dot Net Framework supports our old code, so why touch something that works fine.  Instead it is to create new applications and rebuilds of programs that lacked functionality that was difficult or impossible to implement in the past. \n\nIt is true that all languages compile to a common Immediate Language (IL) file that is used by the Common Language Runtime (CLR) but there are some advantages to using C# over VB. \n## Conclusions\nIn a survey of 633 development managers conducted by BZ Research during June of 2002 the results show that VB.net is being used in 21.8% of the current projects being developed while C#.net is being used in 18.8%.  Over the next 12 months these same development managers are planning future development, where VB.net will be used in 43.4% of the projects and C#.net will rise to 36.7%.  Pretty close. \n\nThese numbers would support what I have heard through the grape-vine that many of the VB shops are making plans to go the VB.net route: I think in part it is because of the upgrade path that has been followed in the past.  They are not taking into consideration that VB.net is not the same VB that they have worked with over the past decade or so.  I am sure that many of these shops will eventually start to move towards C#, since this is the language of the Framework and clearly the best way to start over.  I think the training could be more cost effective and less expensive then attempts to retrain them with VB.net. \n\nMy prediction is that the growth of C#.net will be even greater then what is being portrayed in this survey which shows them pretty close to a draw.  As for me and my house, we are going to skip dabbling in VB.net and go straight to the future of good programming, C#. \n \n","source":"_posts/C-net-or-VB-net.md","raw":"title: 'C#.net or VB.net'\ndate: 2008-06-21\ntags: \n- dotNet\n- c#\n- vb\n---\n## Starting from the Beginning\nI have been a Visual Basic developer for over ten years now.  It was not my first language that got me excited about programming.  No, that would have been Clipper.  I accidentally fell into Clipper much the same way that Visual Basic started as an experiment for me.  \n\nIt was sometime around 1987 or 1988 when I was working as the accountant and network administrator for my family’s car dealership.  I was running into limitations in the current accounting software we were using and I knew that a change would be needed.  However, I had great difficulty in finding software that had the functionality and flexibility that was required.  I started looking into some source code solutions and found one that was originally written in dBase III and was Clipper ready.  This was my start to serious programming as this source code did not work and it took me two months to work through the code learning to work with Clipper as I went along. \n\nClipper 4 worked well for me, which was a very process level language.  There were no surprises.  The user could only process data in the exact steps designed in the software.  When Clipper 5 was released I upgraded, which exposed me to some new and unfamiliar aspects of programming.  Clipper introduced three “built in objects” and soon several “third party vendors” started coming out with add-ons for Clipper that allowed the creation of your own objects and classes.  You should realize that by this time I was becoming quite the Clipper programmer.  I was designing new features to our accounting software and building complementary add-ons.  I was experimenting with Windows but was never able to implement it into the dealership until Windows 3.1 was released.  However, we were running our Accounting software in a DOS box through Windows.  Nothing special here, but it worked.  Nantucket, the company that owned Clipper, made a lot of promises that there would be a Windows version of Clipper coming out soon. \n\nIn the mean time, I read an article by a fellow Clipper guru that suggested looking into Visual Basic to get a better handle on working with objects.  So I got a copy of Visual Basic 1.0 for exactly that purpose; to get a better understanding about how objects worked and be able to actually write a Window application.  I was still thinking that Clipper was going to be my main programming language with Visual Basic as a tool to learn about objects.  This was similar to the original purpose of Pascal, which was to learn structured programming.  Anyway, I was having a great time with Visual Basic, reading a book or two on it and building some really simple programs. \n\nOn a flight to Comdex one year, I was sitting next to two guys from Sequiter Software.  They noticed that I was reading one of my Visual Basic books and asked me how I liked Visual Basic?  I told them I was enjoying it very much but I was really a Clipper programmer and it would be great if Visual Basic could access a database.  Well what followed was some very interesting information that they shared with me: Sequiter makes a product called CodeBase which is a very fast engine that reads and writes to dBase, Fox, and Clipper data.  I could use it in Visual Basic by declaring the procedures to the API from within Visual Basic.  Well that was it: from then on Visual Basic became a very important tool in what would later become my programming career.  Remember I was an accountant that couldn’t find good software that worked the way I needed it to work for me. \n\nSo I guess you could say that I have been building database applications in Visual Basic since version 1.0.  Just so that you are clear on who I really am, I did not take any short cuts to change careers.  I did take a number of correspondence courses where I studied Assembler, Basic, Pascal, C, and COBOL and then went to College where I graduated with distinction.  I am constantly learning and doing the best I can to keep up on the latest technology and am always interested in creating better software products. \n## My Take on C#.net vs. VB.net\nC# (sharp) is the language that was designed for Dot Net and was used to build much of the Framework.  It shares the same first name as its big brother C++, but it doesn’t really feel like any C++ that I have ever taken out for a spin.  C# really is a cross between the best of Visual Basic, the best of C++, and some elements of Java which make it the perfect language for Dot Net.  \n\nMicrosoft made an announcement that the new version of Visual Basic would finally be corrected to match the standards of other languages.  There have always been inconsistencies between VB and other languages when it came to the number of elements that were created in an array and the value of true.  Hey, somewhere along the way, this now very powerful language got itself all screwed up, but then VB.net is the first version where the forms are not being handled by the Ruby Engine.  The operating system will actually be in charge of what we see on the screen.  Anyway, at the time of this announcement I was all excited about the future of VB and what the heck was C# anyway, it did not even seem too important to me. \n\nThen sometime in the early part of 2002, Microsoft made the announcement that I think surprised almost every serious VB programmer.  They were going to re-tool Visual Basic.net to make it more compatible to Visual Basic 6.  Well this was enough for me to consider looking into C# more seriously. \n\nBefore we go on I thought I would take a moment to talk about the bad rap that VB (as a language) has been given.  VB has been attacked over the years by the programming community for not being a very serious language.  The language started off as a very simple tool to build desktop applications for Windows.  Over the years VB has become a very powerful programming language and probably its curse has been the ease in which you can build Windows applications.  I say a curse, because anyone who has worked with this language has been able to build a working program quickly and easily.  On the other hand you would not typically take a language like C++ straight out of the box and start writing a program without at least taking a number of courses and reading a few books on the subject.  You might give up and leave that sort of programming to the professionals.  But none of that is involved when programming in VB since it is such a forgiving language.  However, even with VB’s simplicity there is a lot more to writing a good solid VB program then just a piece of code that works.  There is choosing good programming practices and constantly refining your skill so that you write the most efficient and easily maintainable code that you can possibly write.  The bad rap really belongs to the VB programmers that have picked up some very bad habits along the way and have failed to refine their skill to build elegant and well managed code.  There are bad C++ programmers out there too just not as many. \n\nI have spent a fair amount of time in a variety of C and C++ environments and have found that it was just too much work to build a Windows desktop application.  Visual Basic makes a lot of sense since that was how it was designed.  C on the other hand has it roots in building drivers and operating systems and I do not typically engage in those kinds of projects. \n## Making the Change\nI am leaning towards programming in C# instead of VB but not just because I am upset with the decisions that were made on the future of VB: I need something much more powerful then that to justify my reasons. \n\nOne of the things I really like in C# is the new inline XML style comments.  This is not available in VB.net.  With this I am able to produce some very clear comments in my code, where they happen and with them produce a technical document.  Many times in the past I have had to duplicate some of this effort, and then update my documents when the code went through some changes.  Not anymore, it is all in one place and as I make changes my documents are also updated. \n\nSecondly, Dot Net is built around this new concept of Namespaces, which is the way that Microsoft is getting around the DLL hell issues that have plagued us for years.  I have some interesting stories to tell on that subject but will need to wait for another time.  In C#, the Namespace is exposed right out there directly in your face.  You can adjust the Namespace in VB.net but you need to do this through the IDE and is just not in your face.  I have done some work with multiple projects that support other projects and I just think it is a lot cleaner when I have total control over the Namespace. \n\nThirdly, there is the learning curve.  VB.net is not just an upgrade from Visual Basic.  It is a new life style and you really need to get into that life style if you really want to take advantage of the Frame Work and go beyond what we have coded and designed in the past.  This Frame Work is wonderful and I am almost tempted to say that the only limitation is lack of our imagination.  Since I started getting into C#, I have had to take each step with a fresh new approach.  When I was playing with the early beta’s I found that I was constantly doing a comparison with the previous version of VB.  I think my return on investment with C# is a whole lot better then if I had gone the VB.net route.  Something to keep in mind, Dot Net programming is for new programs, not to port over a working application.  Microsoft has made sure that the Dot Net Framework supports our old code, so why touch something that works fine.  Instead it is to create new applications and rebuilds of programs that lacked functionality that was difficult or impossible to implement in the past. \n\nIt is true that all languages compile to a common Immediate Language (IL) file that is used by the Common Language Runtime (CLR) but there are some advantages to using C# over VB. \n## Conclusions\nIn a survey of 633 development managers conducted by BZ Research during June of 2002 the results show that VB.net is being used in 21.8% of the current projects being developed while C#.net is being used in 18.8%.  Over the next 12 months these same development managers are planning future development, where VB.net will be used in 43.4% of the projects and C#.net will rise to 36.7%.  Pretty close. \n\nThese numbers would support what I have heard through the grape-vine that many of the VB shops are making plans to go the VB.net route: I think in part it is because of the upgrade path that has been followed in the past.  They are not taking into consideration that VB.net is not the same VB that they have worked with over the past decade or so.  I am sure that many of these shops will eventually start to move towards C#, since this is the language of the Framework and clearly the best way to start over.  I think the training could be more cost effective and less expensive then attempts to retrain them with VB.net. \n\nMy prediction is that the growth of C#.net will be even greater then what is being portrayed in this survey which shows them pretty close to a draw.  As for me and my house, we are going to skip dabbling in VB.net and go straight to the future of good programming, C#. \n \n","slug":"C-net-or-VB-net","published":1,"updated":"2020-01-05T00:36:04.968Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfc0004s4ufyly67iww","content":"<h2 id=\"Starting-from-the-Beginning\"><a href=\"#Starting-from-the-Beginning\" class=\"headerlink\" title=\"Starting from the Beginning\"></a>Starting from the Beginning</h2><p>I have been a Visual Basic developer for over ten years now.  It was not my first language that got me excited about programming.  No, that would have been Clipper.  I accidentally fell into Clipper much the same way that Visual Basic started as an experiment for me.  </p>\n<p>It was sometime around 1987 or 1988 when I was working as the accountant and network administrator for my family’s car dealership.  I was running into limitations in the current accounting software we were using and I knew that a change would be needed.  However, I had great difficulty in finding software that had the functionality and flexibility that was required.  I started looking into some source code solutions and found one that was originally written in dBase III and was Clipper ready.  This was my start to serious programming as this source code did not work and it took me two months to work through the code learning to work with Clipper as I went along. </p>\n<p>Clipper 4 worked well for me, which was a very process level language.  There were no surprises.  The user could only process data in the exact steps designed in the software.  When Clipper 5 was released I upgraded, which exposed me to some new and unfamiliar aspects of programming.  Clipper introduced three “built in objects” and soon several “third party vendors” started coming out with add-ons for Clipper that allowed the creation of your own objects and classes.  You should realize that by this time I was becoming quite the Clipper programmer.  I was designing new features to our accounting software and building complementary add-ons.  I was experimenting with Windows but was never able to implement it into the dealership until Windows 3.1 was released.  However, we were running our Accounting software in a DOS box through Windows.  Nothing special here, but it worked.  Nantucket, the company that owned Clipper, made a lot of promises that there would be a Windows version of Clipper coming out soon. </p>\n<p>In the mean time, I read an article by a fellow Clipper guru that suggested looking into Visual Basic to get a better handle on working with objects.  So I got a copy of Visual Basic 1.0 for exactly that purpose; to get a better understanding about how objects worked and be able to actually write a Window application.  I was still thinking that Clipper was going to be my main programming language with Visual Basic as a tool to learn about objects.  This was similar to the original purpose of Pascal, which was to learn structured programming.  Anyway, I was having a great time with Visual Basic, reading a book or two on it and building some really simple programs. </p>\n<p>On a flight to Comdex one year, I was sitting next to two guys from Sequiter Software.  They noticed that I was reading one of my Visual Basic books and asked me how I liked Visual Basic?  I told them I was enjoying it very much but I was really a Clipper programmer and it would be great if Visual Basic could access a database.  Well what followed was some very interesting information that they shared with me: Sequiter makes a product called CodeBase which is a very fast engine that reads and writes to dBase, Fox, and Clipper data.  I could use it in Visual Basic by declaring the procedures to the API from within Visual Basic.  Well that was it: from then on Visual Basic became a very important tool in what would later become my programming career.  Remember I was an accountant that couldn’t find good software that worked the way I needed it to work for me. </p>\n<p>So I guess you could say that I have been building database applications in Visual Basic since version 1.0.  Just so that you are clear on who I really am, I did not take any short cuts to change careers.  I did take a number of correspondence courses where I studied Assembler, Basic, Pascal, C, and COBOL and then went to College where I graduated with distinction.  I am constantly learning and doing the best I can to keep up on the latest technology and am always interested in creating better software products. </p>\n<h2 id=\"My-Take-on-C-net-vs-VB-net\"><a href=\"#My-Take-on-C-net-vs-VB-net\" class=\"headerlink\" title=\"My Take on C#.net vs. VB.net\"></a>My Take on C#.net vs. VB.net</h2><p>C# (sharp) is the language that was designed for Dot Net and was used to build much of the Framework.  It shares the same first name as its big brother C++, but it doesn’t really feel like any C++ that I have ever taken out for a spin.  C# really is a cross between the best of Visual Basic, the best of C++, and some elements of Java which make it the perfect language for Dot Net.  </p>\n<p>Microsoft made an announcement that the new version of Visual Basic would finally be corrected to match the standards of other languages.  There have always been inconsistencies between VB and other languages when it came to the number of elements that were created in an array and the value of true.  Hey, somewhere along the way, this now very powerful language got itself all screwed up, but then VB.net is the first version where the forms are not being handled by the Ruby Engine.  The operating system will actually be in charge of what we see on the screen.  Anyway, at the time of this announcement I was all excited about the future of VB and what the heck was C# anyway, it did not even seem too important to me. </p>\n<p>Then sometime in the early part of 2002, Microsoft made the announcement that I think surprised almost every serious VB programmer.  They were going to re-tool Visual Basic.net to make it more compatible to Visual Basic 6.  Well this was enough for me to consider looking into C# more seriously. </p>\n<p>Before we go on I thought I would take a moment to talk about the bad rap that VB (as a language) has been given.  VB has been attacked over the years by the programming community for not being a very serious language.  The language started off as a very simple tool to build desktop applications for Windows.  Over the years VB has become a very powerful programming language and probably its curse has been the ease in which you can build Windows applications.  I say a curse, because anyone who has worked with this language has been able to build a working program quickly and easily.  On the other hand you would not typically take a language like C++ straight out of the box and start writing a program without at least taking a number of courses and reading a few books on the subject.  You might give up and leave that sort of programming to the professionals.  But none of that is involved when programming in VB since it is such a forgiving language.  However, even with VB’s simplicity there is a lot more to writing a good solid VB program then just a piece of code that works.  There is choosing good programming practices and constantly refining your skill so that you write the most efficient and easily maintainable code that you can possibly write.  The bad rap really belongs to the VB programmers that have picked up some very bad habits along the way and have failed to refine their skill to build elegant and well managed code.  There are bad C++ programmers out there too just not as many. </p>\n<p>I have spent a fair amount of time in a variety of C and C++ environments and have found that it was just too much work to build a Windows desktop application.  Visual Basic makes a lot of sense since that was how it was designed.  C on the other hand has it roots in building drivers and operating systems and I do not typically engage in those kinds of projects. </p>\n<h2 id=\"Making-the-Change\"><a href=\"#Making-the-Change\" class=\"headerlink\" title=\"Making the Change\"></a>Making the Change</h2><p>I am leaning towards programming in C# instead of VB but not just because I am upset with the decisions that were made on the future of VB: I need something much more powerful then that to justify my reasons. </p>\n<p>One of the things I really like in C# is the new inline XML style comments.  This is not available in VB.net.  With this I am able to produce some very clear comments in my code, where they happen and with them produce a technical document.  Many times in the past I have had to duplicate some of this effort, and then update my documents when the code went through some changes.  Not anymore, it is all in one place and as I make changes my documents are also updated. </p>\n<p>Secondly, Dot Net is built around this new concept of Namespaces, which is the way that Microsoft is getting around the DLL hell issues that have plagued us for years.  I have some interesting stories to tell on that subject but will need to wait for another time.  In C#, the Namespace is exposed right out there directly in your face.  You can adjust the Namespace in VB.net but you need to do this through the IDE and is just not in your face.  I have done some work with multiple projects that support other projects and I just think it is a lot cleaner when I have total control over the Namespace. </p>\n<p>Thirdly, there is the learning curve.  VB.net is not just an upgrade from Visual Basic.  It is a new life style and you really need to get into that life style if you really want to take advantage of the Frame Work and go beyond what we have coded and designed in the past.  This Frame Work is wonderful and I am almost tempted to say that the only limitation is lack of our imagination.  Since I started getting into C#, I have had to take each step with a fresh new approach.  When I was playing with the early beta’s I found that I was constantly doing a comparison with the previous version of VB.  I think my return on investment with C# is a whole lot better then if I had gone the VB.net route.  Something to keep in mind, Dot Net programming is for new programs, not to port over a working application.  Microsoft has made sure that the Dot Net Framework supports our old code, so why touch something that works fine.  Instead it is to create new applications and rebuilds of programs that lacked functionality that was difficult or impossible to implement in the past. </p>\n<p>It is true that all languages compile to a common Immediate Language (IL) file that is used by the Common Language Runtime (CLR) but there are some advantages to using C# over VB. </p>\n<h2 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h2><p>In a survey of 633 development managers conducted by BZ Research during June of 2002 the results show that VB.net is being used in 21.8% of the current projects being developed while C#.net is being used in 18.8%.  Over the next 12 months these same development managers are planning future development, where VB.net will be used in 43.4% of the projects and C#.net will rise to 36.7%.  Pretty close. </p>\n<p>These numbers would support what I have heard through the grape-vine that many of the VB shops are making plans to go the VB.net route: I think in part it is because of the upgrade path that has been followed in the past.  They are not taking into consideration that VB.net is not the same VB that they have worked with over the past decade or so.  I am sure that many of these shops will eventually start to move towards C#, since this is the language of the Framework and clearly the best way to start over.  I think the training could be more cost effective and less expensive then attempts to retrain them with VB.net. </p>\n<p>My prediction is that the growth of C#.net will be even greater then what is being portrayed in this survey which shows them pretty close to a draw.  As for me and my house, we are going to skip dabbling in VB.net and go straight to the future of good programming, C#. </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Starting-from-the-Beginning\"><a href=\"#Starting-from-the-Beginning\" class=\"headerlink\" title=\"Starting from the Beginning\"></a>Starting from the Beginning</h2><p>I have been a Visual Basic developer for over ten years now.  It was not my first language that got me excited about programming.  No, that would have been Clipper.  I accidentally fell into Clipper much the same way that Visual Basic started as an experiment for me.  </p>\n<p>It was sometime around 1987 or 1988 when I was working as the accountant and network administrator for my family’s car dealership.  I was running into limitations in the current accounting software we were using and I knew that a change would be needed.  However, I had great difficulty in finding software that had the functionality and flexibility that was required.  I started looking into some source code solutions and found one that was originally written in dBase III and was Clipper ready.  This was my start to serious programming as this source code did not work and it took me two months to work through the code learning to work with Clipper as I went along. </p>\n<p>Clipper 4 worked well for me, which was a very process level language.  There were no surprises.  The user could only process data in the exact steps designed in the software.  When Clipper 5 was released I upgraded, which exposed me to some new and unfamiliar aspects of programming.  Clipper introduced three “built in objects” and soon several “third party vendors” started coming out with add-ons for Clipper that allowed the creation of your own objects and classes.  You should realize that by this time I was becoming quite the Clipper programmer.  I was designing new features to our accounting software and building complementary add-ons.  I was experimenting with Windows but was never able to implement it into the dealership until Windows 3.1 was released.  However, we were running our Accounting software in a DOS box through Windows.  Nothing special here, but it worked.  Nantucket, the company that owned Clipper, made a lot of promises that there would be a Windows version of Clipper coming out soon. </p>\n<p>In the mean time, I read an article by a fellow Clipper guru that suggested looking into Visual Basic to get a better handle on working with objects.  So I got a copy of Visual Basic 1.0 for exactly that purpose; to get a better understanding about how objects worked and be able to actually write a Window application.  I was still thinking that Clipper was going to be my main programming language with Visual Basic as a tool to learn about objects.  This was similar to the original purpose of Pascal, which was to learn structured programming.  Anyway, I was having a great time with Visual Basic, reading a book or two on it and building some really simple programs. </p>\n<p>On a flight to Comdex one year, I was sitting next to two guys from Sequiter Software.  They noticed that I was reading one of my Visual Basic books and asked me how I liked Visual Basic?  I told them I was enjoying it very much but I was really a Clipper programmer and it would be great if Visual Basic could access a database.  Well what followed was some very interesting information that they shared with me: Sequiter makes a product called CodeBase which is a very fast engine that reads and writes to dBase, Fox, and Clipper data.  I could use it in Visual Basic by declaring the procedures to the API from within Visual Basic.  Well that was it: from then on Visual Basic became a very important tool in what would later become my programming career.  Remember I was an accountant that couldn’t find good software that worked the way I needed it to work for me. </p>\n<p>So I guess you could say that I have been building database applications in Visual Basic since version 1.0.  Just so that you are clear on who I really am, I did not take any short cuts to change careers.  I did take a number of correspondence courses where I studied Assembler, Basic, Pascal, C, and COBOL and then went to College where I graduated with distinction.  I am constantly learning and doing the best I can to keep up on the latest technology and am always interested in creating better software products. </p>\n<h2 id=\"My-Take-on-C-net-vs-VB-net\"><a href=\"#My-Take-on-C-net-vs-VB-net\" class=\"headerlink\" title=\"My Take on C#.net vs. VB.net\"></a>My Take on C#.net vs. VB.net</h2><p>C# (sharp) is the language that was designed for Dot Net and was used to build much of the Framework.  It shares the same first name as its big brother C++, but it doesn’t really feel like any C++ that I have ever taken out for a spin.  C# really is a cross between the best of Visual Basic, the best of C++, and some elements of Java which make it the perfect language for Dot Net.  </p>\n<p>Microsoft made an announcement that the new version of Visual Basic would finally be corrected to match the standards of other languages.  There have always been inconsistencies between VB and other languages when it came to the number of elements that were created in an array and the value of true.  Hey, somewhere along the way, this now very powerful language got itself all screwed up, but then VB.net is the first version where the forms are not being handled by the Ruby Engine.  The operating system will actually be in charge of what we see on the screen.  Anyway, at the time of this announcement I was all excited about the future of VB and what the heck was C# anyway, it did not even seem too important to me. </p>\n<p>Then sometime in the early part of 2002, Microsoft made the announcement that I think surprised almost every serious VB programmer.  They were going to re-tool Visual Basic.net to make it more compatible to Visual Basic 6.  Well this was enough for me to consider looking into C# more seriously. </p>\n<p>Before we go on I thought I would take a moment to talk about the bad rap that VB (as a language) has been given.  VB has been attacked over the years by the programming community for not being a very serious language.  The language started off as a very simple tool to build desktop applications for Windows.  Over the years VB has become a very powerful programming language and probably its curse has been the ease in which you can build Windows applications.  I say a curse, because anyone who has worked with this language has been able to build a working program quickly and easily.  On the other hand you would not typically take a language like C++ straight out of the box and start writing a program without at least taking a number of courses and reading a few books on the subject.  You might give up and leave that sort of programming to the professionals.  But none of that is involved when programming in VB since it is such a forgiving language.  However, even with VB’s simplicity there is a lot more to writing a good solid VB program then just a piece of code that works.  There is choosing good programming practices and constantly refining your skill so that you write the most efficient and easily maintainable code that you can possibly write.  The bad rap really belongs to the VB programmers that have picked up some very bad habits along the way and have failed to refine their skill to build elegant and well managed code.  There are bad C++ programmers out there too just not as many. </p>\n<p>I have spent a fair amount of time in a variety of C and C++ environments and have found that it was just too much work to build a Windows desktop application.  Visual Basic makes a lot of sense since that was how it was designed.  C on the other hand has it roots in building drivers and operating systems and I do not typically engage in those kinds of projects. </p>\n<h2 id=\"Making-the-Change\"><a href=\"#Making-the-Change\" class=\"headerlink\" title=\"Making the Change\"></a>Making the Change</h2><p>I am leaning towards programming in C# instead of VB but not just because I am upset with the decisions that were made on the future of VB: I need something much more powerful then that to justify my reasons. </p>\n<p>One of the things I really like in C# is the new inline XML style comments.  This is not available in VB.net.  With this I am able to produce some very clear comments in my code, where they happen and with them produce a technical document.  Many times in the past I have had to duplicate some of this effort, and then update my documents when the code went through some changes.  Not anymore, it is all in one place and as I make changes my documents are also updated. </p>\n<p>Secondly, Dot Net is built around this new concept of Namespaces, which is the way that Microsoft is getting around the DLL hell issues that have plagued us for years.  I have some interesting stories to tell on that subject but will need to wait for another time.  In C#, the Namespace is exposed right out there directly in your face.  You can adjust the Namespace in VB.net but you need to do this through the IDE and is just not in your face.  I have done some work with multiple projects that support other projects and I just think it is a lot cleaner when I have total control over the Namespace. </p>\n<p>Thirdly, there is the learning curve.  VB.net is not just an upgrade from Visual Basic.  It is a new life style and you really need to get into that life style if you really want to take advantage of the Frame Work and go beyond what we have coded and designed in the past.  This Frame Work is wonderful and I am almost tempted to say that the only limitation is lack of our imagination.  Since I started getting into C#, I have had to take each step with a fresh new approach.  When I was playing with the early beta’s I found that I was constantly doing a comparison with the previous version of VB.  I think my return on investment with C# is a whole lot better then if I had gone the VB.net route.  Something to keep in mind, Dot Net programming is for new programs, not to port over a working application.  Microsoft has made sure that the Dot Net Framework supports our old code, so why touch something that works fine.  Instead it is to create new applications and rebuilds of programs that lacked functionality that was difficult or impossible to implement in the past. </p>\n<p>It is true that all languages compile to a common Immediate Language (IL) file that is used by the Common Language Runtime (CLR) but there are some advantages to using C# over VB. </p>\n<h2 id=\"Conclusions\"><a href=\"#Conclusions\" class=\"headerlink\" title=\"Conclusions\"></a>Conclusions</h2><p>In a survey of 633 development managers conducted by BZ Research during June of 2002 the results show that VB.net is being used in 21.8% of the current projects being developed while C#.net is being used in 18.8%.  Over the next 12 months these same development managers are planning future development, where VB.net will be used in 43.4% of the projects and C#.net will rise to 36.7%.  Pretty close. </p>\n<p>These numbers would support what I have heard through the grape-vine that many of the VB shops are making plans to go the VB.net route: I think in part it is because of the upgrade path that has been followed in the past.  They are not taking into consideration that VB.net is not the same VB that they have worked with over the past decade or so.  I am sure that many of these shops will eventually start to move towards C#, since this is the language of the Framework and clearly the best way to start over.  I think the training could be more cost effective and less expensive then attempts to retrain them with VB.net. </p>\n<p>My prediction is that the growth of C#.net will be even greater then what is being portrayed in this survey which shows them pretty close to a draw.  As for me and my house, we are going to skip dabbling in VB.net and go straight to the future of good programming, C#. </p>\n"},{"title":"Database Unit Testing from the Beginning","date":"2011-11-15T08:00:00.000Z","_content":"The concept of unit testing for a database and really this means a database project still seems like a wild idea.  Of course I am still surprise how many development shops still use their production database as their source of truth which it shouldn’t be but that’s because they do not have their database in source control.  In order to take you down the road to explore some of the benefits that are going to be available to you with being able to run unit tests on your database I need to get us all caught up with how to create a database project as this is where the magic happens. \n## Creating a Database Project\nYou need to have Visual Studio 2010 Premium or higher to create a database project.  One of the options that are available to us is to reverse engineer an existing database and that is what we are going to do in these first steps.  I have installed the sample database called AdventureWorks.  This is available as a free [download from the Codeplex site](http://msftdbprodsamples.codeplex.com/releases/view/55926 ). \n\n{% asset_img image1.png \"Create a Project\" %}\n\nFrom Visual Studio you will want to create a new Project and select the SQL Server 2008 Wizard which can be found under the SQL Server node found under the Database category.  Give it a name, I called my AdventureWorks and give it a location on your hard drive where you want the project to be located. \n\nA wizard will popup and take you through a number of pages, just accept the defaults until you get to the Import Database Schema page as this is something we do want to do is to import the AdventureWorks database. \n\n{% asset_img image2.png \"New Project Wizard\" %}\n\nMake sure you check the Import existing schema and then you will likely want to click on the New Connection button unless you have made a previous connection to the database, that connection string won’t be found in the dropdown. \n\n{% asset_img image3.png \"Connection Properties\" %}\n\nIf you have connected to databases in the past this dialog box should be very familiar to you.  Basically we need to say where the SQL server is.  In this case it is on my local machine and is the default instance.  Other common server names are also localhost\\SQLExpress as that is the name instance that SQL Express creates when it is installed.  After you get the server instance completed the dropdown of database names will be populated and from there you should be able to find the AdventureWorks database.  I also like to click on the Test Connection button just to confirm that there aren’t any connectivity issues.  Click OK and we are ready to move on. \n\nClick Next and continue through the wizard pages and accepting the defaults.  On the last page click Finish.  This is where this Visual Studio wizard really does it’s work as it creates the project and does a complete reverse engineering of the database.  The end result is a Visual Studio SQL Database project that represents the database in code which is suitable for checking into Source Control, capable of deploying changes that might be made to this project, being able to compare changes between versions and much much more. \n## Lets get to Unit Testing\nWhen you are on a database project as in I have physically clicked on it so that it has focus you will see that a number of toolbar buttons appear.  We want to click on the one called Schema View. \n\n{% asset_img image4.png \"Solution Explorer\" %}\n\nThis brings up another little window in the same area as the Solution and Team Explorer area of Visual Studio called the Schema View. \n\n{% asset_img image5.png \"Schema View\" %}\n\nFrom this view you will want to expand Schemas, then expand the HumanResources, expand Programmability, Stored Procedures and finally you want to right click onto the uspUpdateEmployeePersonalInfo and choose Create Unit Tests… \n\nIf you don’t already have a Test Project the next step will let you create a skeleton Unit test for this stored procedure and the option to create a new Test project in the language of your choice. \n\n{% asset_img image6.png \"Create Unit Tests\" %}\n\nYou will find that when this window opens you can choose more than just the one stored procedure that we choose in the previous step but yours is the only one that is checked.  If you did want to have more than one stored procedure in the same class file you could pick them as well.  Then set the Project name or select and existing Test Project and give it a decent class name.  I named mine HumanRecourceUnitTests.cs.  After you click OK it will build all the pieces the test project and default unittest.cs file that we don’t need and everything just starts to look like a typical Unit Test until the following dialog pops up. \n\n{% asset_img image7.png \"Project DatabaseUnitTests Configuration\" %}\n\nNow in order to run unit tests against the database you need a connection to the database.  In the first part of this you should be able to find your original stored procedure that you used to create the database project.  You will notice that this dialog has an optional additional what it calls a secondary data connection to validate unit tests.  In this sample we will not need this but in a real world application you may so let me explain that scenario.  When an application that is built with a database connection, typically that application and the connection string would just have enough rights to run the stored procedures and nothing else.  In those cases you will want to test those connection string when running the stored procedure that you are testing but that same connection string would not have the rights to check the database to see if those rights are valid especially in a scenario where you want to check if the right values got inserted or deleted, that is where this secondary data connection comes in, it would be a data  connection that had higher rights to look at those values directly from the tables. \n\nAfter you have clicked the OK button Visual Studio will display a skeleton of a unit test to test this stored procedure. \n\n{% asset_img image8.png \"Testing Stored Procedure\" %}\n\nIn theory we have a unit test that we could run, but the results would indicate that the results are inconclusive because although this stored procedure is being run, it is really just exercising the stored procedure and not really testing it as in giving it some values to insert and checking if those values come back. \n\nWe are going to replace the unit test calls here with the following code snippet.  I have it all in one piece here for you to easily grab this but following this I will break down this code so you can see what is going on.  It is very similar to what the skeleton provided with us but we give it some distinct values. \n\n```\n-- Prepare the test data (expected results)\nDECLARE @EmployeeId int\n\nSELECT TOP 1 @EmployeeId = EmployeeId\n\tFROM HumanResources.Employee\n\n-- Wrap it into a transaction to return us into a clean state after the test\nBEGIN TRANSACTION\n\n-- Call the target code\nEXEC HumanResources.uspUpdateEmployeePersonalInfo\n\t@EmployeeId, '987654321', '3/2/1987', 'S', 'F'\n\t\n-- Select the results to verify\nSELECT NationalIdNumber, Birthdate, MartialStatus, Gender\n\tFROM HumanResources.Employee\n\tWHERE EmployeeId = @EmployeeId\n\t\nROLLBACK TRANSACTION\n```\nThe first part of this code is to capture the EmployeeId that we want to update so that is what the first DECLARE statement does.  In the next call we just want to capture an existing EmployeeId from the Employee table and because we really don’t care which on it runs us but we only want want we use the TOP 1 clause in that statement.  At this point our declared variable @EmployeeId now has this value.\n \n**Note:** *I have found that there could be a breaking change here that depends on which version of the adventure works database that you have as some will have the employeeId and others will have this column named BusinessEntityID.  To find which one you have go back to the Schema View of the project and expand the Schemas, HumanResources and Tables.  Find the Employee table and expand the Columns, the column in question will be that first one right there.*\n\n{% asset_img image9.png \"Schema View\" %}\n\nBecause the stored procedure will make changes to the data in the table and we may not want to actually commit those changes we just want to test these changes we surround the next pieces around a transaction and after we have collected our validation values we can roll this back. \n\nAfter the transaction we call the update stored procedure and pass in some specific data.  Next we call a select statement to get those values from the table with the EmployeeId that we just passed into the previous steps.  Finally we roll the whole transaction back so that we do not actually make any changes to the database so we can run this test over and over. \n\nBefore we can actually run this test we need to make some changes to the Test Conditions portion of the unit test.  First you will want to remove the existing entry that is shown there by clicking on the Delete Test button. \n\n{% asset_img image10.png \"Test Conditions: Data Checksum\" %}\n\nAfter you have removing the existing Test Condition we can then add a new one or more to verify the results.  Select Scalar Value from the dropdown control and click on the “+” button. \n\n{% asset_img image11.png \"Test Conditions: Scalar Value\" %}\n\nOn the scalarValueCondition1 line that this action creates, right click on this line and choose Properties, which will display the properties window.  Update the following information: \n- Name: VerifyNationalId\n- Column number: 1\n- Enabled: True\n- Expected value: 987654321\n- Null expected: False\n- ResultSet: 1\n- Row number: 1\n\n{% asset_img image12.png \"Properties\" %}\n\nWhat is really happening here is that we are going to look at that first column and see if it matches the NationalId that we sent to the stored procedure.  NationalId is the first column that is returned in the select statement. \n\nWe are now ready to run the unit test and see that it is working and pass the test.  Typically in a unit test you could be anywhere in the method of the unit test do a right click and you will see one of the context choices being to run test.  However what we have been working on so far has been the design surface of the database unit tests which is why we were able to write SQL statement to write our tests.  To see or get to the actual code page you need to go back to the HumanResourceUnitTests.cs file and right click on it and choose view code. \n\n{% asset_img image13.png \"Solution Explorer / View Code\" %}\n\nAs an alternative you could select the file in the solution and press the F7 key, either way you will then be looking at the actual test and if you right click anywhere within that method you will see that one of your choice is to Run Tests.  Do that now and you will see the test results go from a Pending to hopefully a Pass.  If you do get a failure with an exception you will want to check the column names from this table.  Some of the names changed and even the way they are spelled.  It appears to be case sensitive as well.  Like I mentioned before, there seem to be more than one version of this sample database out there and they did make some changes. \n\n{% asset_img image14.png \"Test Results\" %}\n\nNow that we do have a working test, I always like to make a change to prove that it is working by making it fail.  So to make it fail, change the Expected value to 9876543210000.  I basically just added 4 zeros to the end of the expected result.  Re-run the test and it should fail and if we look at the Test Result Details we can see that the expected results did not match, which is exactly what we expected. \n\nTake out the padded zeros and run the test once more so that we get a passed test once more.  This is just a step to keep or tests correct. \n## Associate the Unit Test to a Test Case\nThe following section is going to need TFS 2010 in order complete this part of the walk through, and even better if you have Lab Management setup to complete the development, build, deploy, test cycle on these database unit tests. \n\nRight now, the unit test that we created can be run from Visual Studio just like we have done in this walk through.  You can also make these part of an automated build which if this test project was included in the solution for an automated build in Team Foundation Server (TFS) it would automatically run and be part of the build report.  However, this would not update the Test Plan / Test Suite / Test Case that the QA people are using to manage their tests, but it can. \n\nIn Visual Studio, Create a new Work Item of type: Test Case, and call it “uspUpdateEmployeePersonalInfo Stored Procedure Test”.  We won’t fill anything in the steps section as we are going to go straight to automation with this Test Case.  Click on the Associated Automation tab and click on the ellipse “…” button \n\n{% asset_img image15.png \"Choose Test\" %}\n\nThis will bring up the Choose Test dialog box and because we have just this one test open in Visual Studio we will see the exact test that we want associated with this test case.  Click on the OK button. \n\nWe now have a test case that can be used to test the stored procedure in automation.  When this test case is run in automation it will update the test results and will be reported to the Test Plan and Suite that this test case is a part of. \n","source":"_posts/Database-Unit-Testing-from-the-Beginning.md","raw":"title: Database Unit Testing from the Beginning\ndate: 2011-11-15\ntags:\n- SQL\n- Database\n- Testing\n- ALM\n---\nThe concept of unit testing for a database and really this means a database project still seems like a wild idea.  Of course I am still surprise how many development shops still use their production database as their source of truth which it shouldn’t be but that’s because they do not have their database in source control.  In order to take you down the road to explore some of the benefits that are going to be available to you with being able to run unit tests on your database I need to get us all caught up with how to create a database project as this is where the magic happens. \n## Creating a Database Project\nYou need to have Visual Studio 2010 Premium or higher to create a database project.  One of the options that are available to us is to reverse engineer an existing database and that is what we are going to do in these first steps.  I have installed the sample database called AdventureWorks.  This is available as a free [download from the Codeplex site](http://msftdbprodsamples.codeplex.com/releases/view/55926 ). \n\n{% asset_img image1.png \"Create a Project\" %}\n\nFrom Visual Studio you will want to create a new Project and select the SQL Server 2008 Wizard which can be found under the SQL Server node found under the Database category.  Give it a name, I called my AdventureWorks and give it a location on your hard drive where you want the project to be located. \n\nA wizard will popup and take you through a number of pages, just accept the defaults until you get to the Import Database Schema page as this is something we do want to do is to import the AdventureWorks database. \n\n{% asset_img image2.png \"New Project Wizard\" %}\n\nMake sure you check the Import existing schema and then you will likely want to click on the New Connection button unless you have made a previous connection to the database, that connection string won’t be found in the dropdown. \n\n{% asset_img image3.png \"Connection Properties\" %}\n\nIf you have connected to databases in the past this dialog box should be very familiar to you.  Basically we need to say where the SQL server is.  In this case it is on my local machine and is the default instance.  Other common server names are also localhost\\SQLExpress as that is the name instance that SQL Express creates when it is installed.  After you get the server instance completed the dropdown of database names will be populated and from there you should be able to find the AdventureWorks database.  I also like to click on the Test Connection button just to confirm that there aren’t any connectivity issues.  Click OK and we are ready to move on. \n\nClick Next and continue through the wizard pages and accepting the defaults.  On the last page click Finish.  This is where this Visual Studio wizard really does it’s work as it creates the project and does a complete reverse engineering of the database.  The end result is a Visual Studio SQL Database project that represents the database in code which is suitable for checking into Source Control, capable of deploying changes that might be made to this project, being able to compare changes between versions and much much more. \n## Lets get to Unit Testing\nWhen you are on a database project as in I have physically clicked on it so that it has focus you will see that a number of toolbar buttons appear.  We want to click on the one called Schema View. \n\n{% asset_img image4.png \"Solution Explorer\" %}\n\nThis brings up another little window in the same area as the Solution and Team Explorer area of Visual Studio called the Schema View. \n\n{% asset_img image5.png \"Schema View\" %}\n\nFrom this view you will want to expand Schemas, then expand the HumanResources, expand Programmability, Stored Procedures and finally you want to right click onto the uspUpdateEmployeePersonalInfo and choose Create Unit Tests… \n\nIf you don’t already have a Test Project the next step will let you create a skeleton Unit test for this stored procedure and the option to create a new Test project in the language of your choice. \n\n{% asset_img image6.png \"Create Unit Tests\" %}\n\nYou will find that when this window opens you can choose more than just the one stored procedure that we choose in the previous step but yours is the only one that is checked.  If you did want to have more than one stored procedure in the same class file you could pick them as well.  Then set the Project name or select and existing Test Project and give it a decent class name.  I named mine HumanRecourceUnitTests.cs.  After you click OK it will build all the pieces the test project and default unittest.cs file that we don’t need and everything just starts to look like a typical Unit Test until the following dialog pops up. \n\n{% asset_img image7.png \"Project DatabaseUnitTests Configuration\" %}\n\nNow in order to run unit tests against the database you need a connection to the database.  In the first part of this you should be able to find your original stored procedure that you used to create the database project.  You will notice that this dialog has an optional additional what it calls a secondary data connection to validate unit tests.  In this sample we will not need this but in a real world application you may so let me explain that scenario.  When an application that is built with a database connection, typically that application and the connection string would just have enough rights to run the stored procedures and nothing else.  In those cases you will want to test those connection string when running the stored procedure that you are testing but that same connection string would not have the rights to check the database to see if those rights are valid especially in a scenario where you want to check if the right values got inserted or deleted, that is where this secondary data connection comes in, it would be a data  connection that had higher rights to look at those values directly from the tables. \n\nAfter you have clicked the OK button Visual Studio will display a skeleton of a unit test to test this stored procedure. \n\n{% asset_img image8.png \"Testing Stored Procedure\" %}\n\nIn theory we have a unit test that we could run, but the results would indicate that the results are inconclusive because although this stored procedure is being run, it is really just exercising the stored procedure and not really testing it as in giving it some values to insert and checking if those values come back. \n\nWe are going to replace the unit test calls here with the following code snippet.  I have it all in one piece here for you to easily grab this but following this I will break down this code so you can see what is going on.  It is very similar to what the skeleton provided with us but we give it some distinct values. \n\n```\n-- Prepare the test data (expected results)\nDECLARE @EmployeeId int\n\nSELECT TOP 1 @EmployeeId = EmployeeId\n\tFROM HumanResources.Employee\n\n-- Wrap it into a transaction to return us into a clean state after the test\nBEGIN TRANSACTION\n\n-- Call the target code\nEXEC HumanResources.uspUpdateEmployeePersonalInfo\n\t@EmployeeId, '987654321', '3/2/1987', 'S', 'F'\n\t\n-- Select the results to verify\nSELECT NationalIdNumber, Birthdate, MartialStatus, Gender\n\tFROM HumanResources.Employee\n\tWHERE EmployeeId = @EmployeeId\n\t\nROLLBACK TRANSACTION\n```\nThe first part of this code is to capture the EmployeeId that we want to update so that is what the first DECLARE statement does.  In the next call we just want to capture an existing EmployeeId from the Employee table and because we really don’t care which on it runs us but we only want want we use the TOP 1 clause in that statement.  At this point our declared variable @EmployeeId now has this value.\n \n**Note:** *I have found that there could be a breaking change here that depends on which version of the adventure works database that you have as some will have the employeeId and others will have this column named BusinessEntityID.  To find which one you have go back to the Schema View of the project and expand the Schemas, HumanResources and Tables.  Find the Employee table and expand the Columns, the column in question will be that first one right there.*\n\n{% asset_img image9.png \"Schema View\" %}\n\nBecause the stored procedure will make changes to the data in the table and we may not want to actually commit those changes we just want to test these changes we surround the next pieces around a transaction and after we have collected our validation values we can roll this back. \n\nAfter the transaction we call the update stored procedure and pass in some specific data.  Next we call a select statement to get those values from the table with the EmployeeId that we just passed into the previous steps.  Finally we roll the whole transaction back so that we do not actually make any changes to the database so we can run this test over and over. \n\nBefore we can actually run this test we need to make some changes to the Test Conditions portion of the unit test.  First you will want to remove the existing entry that is shown there by clicking on the Delete Test button. \n\n{% asset_img image10.png \"Test Conditions: Data Checksum\" %}\n\nAfter you have removing the existing Test Condition we can then add a new one or more to verify the results.  Select Scalar Value from the dropdown control and click on the “+” button. \n\n{% asset_img image11.png \"Test Conditions: Scalar Value\" %}\n\nOn the scalarValueCondition1 line that this action creates, right click on this line and choose Properties, which will display the properties window.  Update the following information: \n- Name: VerifyNationalId\n- Column number: 1\n- Enabled: True\n- Expected value: 987654321\n- Null expected: False\n- ResultSet: 1\n- Row number: 1\n\n{% asset_img image12.png \"Properties\" %}\n\nWhat is really happening here is that we are going to look at that first column and see if it matches the NationalId that we sent to the stored procedure.  NationalId is the first column that is returned in the select statement. \n\nWe are now ready to run the unit test and see that it is working and pass the test.  Typically in a unit test you could be anywhere in the method of the unit test do a right click and you will see one of the context choices being to run test.  However what we have been working on so far has been the design surface of the database unit tests which is why we were able to write SQL statement to write our tests.  To see or get to the actual code page you need to go back to the HumanResourceUnitTests.cs file and right click on it and choose view code. \n\n{% asset_img image13.png \"Solution Explorer / View Code\" %}\n\nAs an alternative you could select the file in the solution and press the F7 key, either way you will then be looking at the actual test and if you right click anywhere within that method you will see that one of your choice is to Run Tests.  Do that now and you will see the test results go from a Pending to hopefully a Pass.  If you do get a failure with an exception you will want to check the column names from this table.  Some of the names changed and even the way they are spelled.  It appears to be case sensitive as well.  Like I mentioned before, there seem to be more than one version of this sample database out there and they did make some changes. \n\n{% asset_img image14.png \"Test Results\" %}\n\nNow that we do have a working test, I always like to make a change to prove that it is working by making it fail.  So to make it fail, change the Expected value to 9876543210000.  I basically just added 4 zeros to the end of the expected result.  Re-run the test and it should fail and if we look at the Test Result Details we can see that the expected results did not match, which is exactly what we expected. \n\nTake out the padded zeros and run the test once more so that we get a passed test once more.  This is just a step to keep or tests correct. \n## Associate the Unit Test to a Test Case\nThe following section is going to need TFS 2010 in order complete this part of the walk through, and even better if you have Lab Management setup to complete the development, build, deploy, test cycle on these database unit tests. \n\nRight now, the unit test that we created can be run from Visual Studio just like we have done in this walk through.  You can also make these part of an automated build which if this test project was included in the solution for an automated build in Team Foundation Server (TFS) it would automatically run and be part of the build report.  However, this would not update the Test Plan / Test Suite / Test Case that the QA people are using to manage their tests, but it can. \n\nIn Visual Studio, Create a new Work Item of type: Test Case, and call it “uspUpdateEmployeePersonalInfo Stored Procedure Test”.  We won’t fill anything in the steps section as we are going to go straight to automation with this Test Case.  Click on the Associated Automation tab and click on the ellipse “…” button \n\n{% asset_img image15.png \"Choose Test\" %}\n\nThis will bring up the Choose Test dialog box and because we have just this one test open in Visual Studio we will see the exact test that we want associated with this test case.  Click on the OK button. \n\nWe now have a test case that can be used to test the stored procedure in automation.  When this test case is run in automation it will update the test results and will be reported to the Test Plan and Suite that this test case is a part of. \n","slug":"Database-Unit-Testing-from-the-Beginning","published":1,"updated":"2020-01-05T00:36:04.983Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfd0005s4ufj97srk3k","content":"<p>The concept of unit testing for a database and really this means a database project still seems like a wild idea.  Of course I am still surprise how many development shops still use their production database as their source of truth which it shouldn’t be but that’s because they do not have their database in source control.  In order to take you down the road to explore some of the benefits that are going to be available to you with being able to run unit tests on your database I need to get us all caught up with how to create a database project as this is where the magic happens. </p>\n<h2 id=\"Creating-a-Database-Project\"><a href=\"#Creating-a-Database-Project\" class=\"headerlink\" title=\"Creating a Database Project\"></a>Creating a Database Project</h2><p>You need to have Visual Studio 2010 Premium or higher to create a database project.  One of the options that are available to us is to reverse engineer an existing database and that is what we are going to do in these first steps.  I have installed the sample database called AdventureWorks.  This is available as a free <a href=\"http://msftdbprodsamples.codeplex.com/releases/view/55926\" target=\"_blank\" rel=\"noopener\">download from the Codeplex site</a>. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image1.png\" title=\"Create a Project\">\n<p>From Visual Studio you will want to create a new Project and select the SQL Server 2008 Wizard which can be found under the SQL Server node found under the Database category.  Give it a name, I called my AdventureWorks and give it a location on your hard drive where you want the project to be located. </p>\n<p>A wizard will popup and take you through a number of pages, just accept the defaults until you get to the Import Database Schema page as this is something we do want to do is to import the AdventureWorks database. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image2.png\" title=\"New Project Wizard\">\n<p>Make sure you check the Import existing schema and then you will likely want to click on the New Connection button unless you have made a previous connection to the database, that connection string won’t be found in the dropdown. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image3.png\" title=\"Connection Properties\">\n<p>If you have connected to databases in the past this dialog box should be very familiar to you.  Basically we need to say where the SQL server is.  In this case it is on my local machine and is the default instance.  Other common server names are also localhost\\SQLExpress as that is the name instance that SQL Express creates when it is installed.  After you get the server instance completed the dropdown of database names will be populated and from there you should be able to find the AdventureWorks database.  I also like to click on the Test Connection button just to confirm that there aren’t any connectivity issues.  Click OK and we are ready to move on. </p>\n<p>Click Next and continue through the wizard pages and accepting the defaults.  On the last page click Finish.  This is where this Visual Studio wizard really does it’s work as it creates the project and does a complete reverse engineering of the database.  The end result is a Visual Studio SQL Database project that represents the database in code which is suitable for checking into Source Control, capable of deploying changes that might be made to this project, being able to compare changes between versions and much much more. </p>\n<h2 id=\"Lets-get-to-Unit-Testing\"><a href=\"#Lets-get-to-Unit-Testing\" class=\"headerlink\" title=\"Lets get to Unit Testing\"></a>Lets get to Unit Testing</h2><p>When you are on a database project as in I have physically clicked on it so that it has focus you will see that a number of toolbar buttons appear.  We want to click on the one called Schema View. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image4.png\" title=\"Solution Explorer\">\n<p>This brings up another little window in the same area as the Solution and Team Explorer area of Visual Studio called the Schema View. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image5.png\" title=\"Schema View\">\n<p>From this view you will want to expand Schemas, then expand the HumanResources, expand Programmability, Stored Procedures and finally you want to right click onto the uspUpdateEmployeePersonalInfo and choose Create Unit Tests… </p>\n<p>If you don’t already have a Test Project the next step will let you create a skeleton Unit test for this stored procedure and the option to create a new Test project in the language of your choice. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image6.png\" title=\"Create Unit Tests\">\n<p>You will find that when this window opens you can choose more than just the one stored procedure that we choose in the previous step but yours is the only one that is checked.  If you did want to have more than one stored procedure in the same class file you could pick them as well.  Then set the Project name or select and existing Test Project and give it a decent class name.  I named mine HumanRecourceUnitTests.cs.  After you click OK it will build all the pieces the test project and default unittest.cs file that we don’t need and everything just starts to look like a typical Unit Test until the following dialog pops up. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image7.png\" title=\"Project DatabaseUnitTests Configuration\">\n<p>Now in order to run unit tests against the database you need a connection to the database.  In the first part of this you should be able to find your original stored procedure that you used to create the database project.  You will notice that this dialog has an optional additional what it calls a secondary data connection to validate unit tests.  In this sample we will not need this but in a real world application you may so let me explain that scenario.  When an application that is built with a database connection, typically that application and the connection string would just have enough rights to run the stored procedures and nothing else.  In those cases you will want to test those connection string when running the stored procedure that you are testing but that same connection string would not have the rights to check the database to see if those rights are valid especially in a scenario where you want to check if the right values got inserted or deleted, that is where this secondary data connection comes in, it would be a data  connection that had higher rights to look at those values directly from the tables. </p>\n<p>After you have clicked the OK button Visual Studio will display a skeleton of a unit test to test this stored procedure. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image8.png\" title=\"Testing Stored Procedure\">\n<p>In theory we have a unit test that we could run, but the results would indicate that the results are inconclusive because although this stored procedure is being run, it is really just exercising the stored procedure and not really testing it as in giving it some values to insert and checking if those values come back. </p>\n<p>We are going to replace the unit test calls here with the following code snippet.  I have it all in one piece here for you to easily grab this but following this I will break down this code so you can see what is going on.  It is very similar to what the skeleton provided with us but we give it some distinct values. </p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Prepare the test data (expected results)</span></span><br><span class=\"line\"><span class=\"keyword\">DECLARE</span> @EmployeeId <span class=\"built_in\">int</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> TOP <span class=\"number\">1</span> @EmployeeId = EmployeeId</span><br><span class=\"line\">\t<span class=\"keyword\">FROM</span> HumanResources.Employee</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Wrap it into a transaction to return us into a clean state after the test</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span> <span class=\"keyword\">TRANSACTION</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Call the target code</span></span><br><span class=\"line\">EXEC HumanResources.uspUpdateEmployeePersonalInfo</span><br><span class=\"line\">\t@EmployeeId, <span class=\"string\">'987654321'</span>, <span class=\"string\">'3/2/1987'</span>, <span class=\"string\">'S'</span>, <span class=\"string\">'F'</span></span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\">-- Select the results to verify</span></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> NationalIdNumber, Birthdate, MartialStatus, Gender</span><br><span class=\"line\">\t<span class=\"keyword\">FROM</span> HumanResources.Employee</span><br><span class=\"line\">\t<span class=\"keyword\">WHERE</span> EmployeeId = @EmployeeId</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"keyword\">ROLLBACK</span> <span class=\"keyword\">TRANSACTION</span></span><br></pre></td></tr></table></figure>\n<p>The first part of this code is to capture the EmployeeId that we want to update so that is what the first DECLARE statement does.  In the next call we just want to capture an existing EmployeeId from the Employee table and because we really don’t care which on it runs us but we only want want we use the TOP 1 clause in that statement.  At this point our declared variable @EmployeeId now has this value.</p>\n<p><strong>Note:</strong> <em>I have found that there could be a breaking change here that depends on which version of the adventure works database that you have as some will have the employeeId and others will have this column named BusinessEntityID.  To find which one you have go back to the Schema View of the project and expand the Schemas, HumanResources and Tables.  Find the Employee table and expand the Columns, the column in question will be that first one right there.</em></p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image9.png\" title=\"Schema View\">\n<p>Because the stored procedure will make changes to the data in the table and we may not want to actually commit those changes we just want to test these changes we surround the next pieces around a transaction and after we have collected our validation values we can roll this back. </p>\n<p>After the transaction we call the update stored procedure and pass in some specific data.  Next we call a select statement to get those values from the table with the EmployeeId that we just passed into the previous steps.  Finally we roll the whole transaction back so that we do not actually make any changes to the database so we can run this test over and over. </p>\n<p>Before we can actually run this test we need to make some changes to the Test Conditions portion of the unit test.  First you will want to remove the existing entry that is shown there by clicking on the Delete Test button. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image10.png\" title=\"Test Conditions: Data Checksum\">\n<p>After you have removing the existing Test Condition we can then add a new one or more to verify the results.  Select Scalar Value from the dropdown control and click on the “+” button. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image11.png\" title=\"Test Conditions: Scalar Value\">\n<p>On the scalarValueCondition1 line that this action creates, right click on this line and choose Properties, which will display the properties window.  Update the following information: </p>\n<ul>\n<li>Name: VerifyNationalId</li>\n<li>Column number: 1</li>\n<li>Enabled: True</li>\n<li>Expected value: 987654321</li>\n<li>Null expected: False</li>\n<li>ResultSet: 1</li>\n<li>Row number: 1</li>\n</ul>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image12.png\" title=\"Properties\">\n<p>What is really happening here is that we are going to look at that first column and see if it matches the NationalId that we sent to the stored procedure.  NationalId is the first column that is returned in the select statement. </p>\n<p>We are now ready to run the unit test and see that it is working and pass the test.  Typically in a unit test you could be anywhere in the method of the unit test do a right click and you will see one of the context choices being to run test.  However what we have been working on so far has been the design surface of the database unit tests which is why we were able to write SQL statement to write our tests.  To see or get to the actual code page you need to go back to the HumanResourceUnitTests.cs file and right click on it and choose view code. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image13.png\" title=\"Solution Explorer / View Code\">\n<p>As an alternative you could select the file in the solution and press the F7 key, either way you will then be looking at the actual test and if you right click anywhere within that method you will see that one of your choice is to Run Tests.  Do that now and you will see the test results go from a Pending to hopefully a Pass.  If you do get a failure with an exception you will want to check the column names from this table.  Some of the names changed and even the way they are spelled.  It appears to be case sensitive as well.  Like I mentioned before, there seem to be more than one version of this sample database out there and they did make some changes. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image14.png\" title=\"Test Results\">\n<p>Now that we do have a working test, I always like to make a change to prove that it is working by making it fail.  So to make it fail, change the Expected value to 9876543210000.  I basically just added 4 zeros to the end of the expected result.  Re-run the test and it should fail and if we look at the Test Result Details we can see that the expected results did not match, which is exactly what we expected. </p>\n<p>Take out the padded zeros and run the test once more so that we get a passed test once more.  This is just a step to keep or tests correct. </p>\n<h2 id=\"Associate-the-Unit-Test-to-a-Test-Case\"><a href=\"#Associate-the-Unit-Test-to-a-Test-Case\" class=\"headerlink\" title=\"Associate the Unit Test to a Test Case\"></a>Associate the Unit Test to a Test Case</h2><p>The following section is going to need TFS 2010 in order complete this part of the walk through, and even better if you have Lab Management setup to complete the development, build, deploy, test cycle on these database unit tests. </p>\n<p>Right now, the unit test that we created can be run from Visual Studio just like we have done in this walk through.  You can also make these part of an automated build which if this test project was included in the solution for an automated build in Team Foundation Server (TFS) it would automatically run and be part of the build report.  However, this would not update the Test Plan / Test Suite / Test Case that the QA people are using to manage their tests, but it can. </p>\n<p>In Visual Studio, Create a new Work Item of type: Test Case, and call it “uspUpdateEmployeePersonalInfo Stored Procedure Test”.  We won’t fill anything in the steps section as we are going to go straight to automation with this Test Case.  Click on the Associated Automation tab and click on the ellipse “…” button </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image15.png\" title=\"Choose Test\">\n<p>This will bring up the Choose Test dialog box and because we have just this one test open in Visual Studio we will see the exact test that we want associated with this test case.  Click on the OK button. </p>\n<p>We now have a test case that can be used to test the stored procedure in automation.  When this test case is run in automation it will update the test results and will be reported to the Test Plan and Suite that this test case is a part of. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>The concept of unit testing for a database and really this means a database project still seems like a wild idea.  Of course I am still surprise how many development shops still use their production database as their source of truth which it shouldn’t be but that’s because they do not have their database in source control.  In order to take you down the road to explore some of the benefits that are going to be available to you with being able to run unit tests on your database I need to get us all caught up with how to create a database project as this is where the magic happens. </p>\n<h2 id=\"Creating-a-Database-Project\"><a href=\"#Creating-a-Database-Project\" class=\"headerlink\" title=\"Creating a Database Project\"></a>Creating a Database Project</h2><p>You need to have Visual Studio 2010 Premium or higher to create a database project.  One of the options that are available to us is to reverse engineer an existing database and that is what we are going to do in these first steps.  I have installed the sample database called AdventureWorks.  This is available as a free <a href=\"http://msftdbprodsamples.codeplex.com/releases/view/55926\" target=\"_blank\" rel=\"noopener\">download from the Codeplex site</a>. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image1.png\" title=\"Create a Project\">\n<p>From Visual Studio you will want to create a new Project and select the SQL Server 2008 Wizard which can be found under the SQL Server node found under the Database category.  Give it a name, I called my AdventureWorks and give it a location on your hard drive where you want the project to be located. </p>\n<p>A wizard will popup and take you through a number of pages, just accept the defaults until you get to the Import Database Schema page as this is something we do want to do is to import the AdventureWorks database. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image2.png\" title=\"New Project Wizard\">\n<p>Make sure you check the Import existing schema and then you will likely want to click on the New Connection button unless you have made a previous connection to the database, that connection string won’t be found in the dropdown. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image3.png\" title=\"Connection Properties\">\n<p>If you have connected to databases in the past this dialog box should be very familiar to you.  Basically we need to say where the SQL server is.  In this case it is on my local machine and is the default instance.  Other common server names are also localhost\\SQLExpress as that is the name instance that SQL Express creates when it is installed.  After you get the server instance completed the dropdown of database names will be populated and from there you should be able to find the AdventureWorks database.  I also like to click on the Test Connection button just to confirm that there aren’t any connectivity issues.  Click OK and we are ready to move on. </p>\n<p>Click Next and continue through the wizard pages and accepting the defaults.  On the last page click Finish.  This is where this Visual Studio wizard really does it’s work as it creates the project and does a complete reverse engineering of the database.  The end result is a Visual Studio SQL Database project that represents the database in code which is suitable for checking into Source Control, capable of deploying changes that might be made to this project, being able to compare changes between versions and much much more. </p>\n<h2 id=\"Lets-get-to-Unit-Testing\"><a href=\"#Lets-get-to-Unit-Testing\" class=\"headerlink\" title=\"Lets get to Unit Testing\"></a>Lets get to Unit Testing</h2><p>When you are on a database project as in I have physically clicked on it so that it has focus you will see that a number of toolbar buttons appear.  We want to click on the one called Schema View. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image4.png\" title=\"Solution Explorer\">\n<p>This brings up another little window in the same area as the Solution and Team Explorer area of Visual Studio called the Schema View. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image5.png\" title=\"Schema View\">\n<p>From this view you will want to expand Schemas, then expand the HumanResources, expand Programmability, Stored Procedures and finally you want to right click onto the uspUpdateEmployeePersonalInfo and choose Create Unit Tests… </p>\n<p>If you don’t already have a Test Project the next step will let you create a skeleton Unit test for this stored procedure and the option to create a new Test project in the language of your choice. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image6.png\" title=\"Create Unit Tests\">\n<p>You will find that when this window opens you can choose more than just the one stored procedure that we choose in the previous step but yours is the only one that is checked.  If you did want to have more than one stored procedure in the same class file you could pick them as well.  Then set the Project name or select and existing Test Project and give it a decent class name.  I named mine HumanRecourceUnitTests.cs.  After you click OK it will build all the pieces the test project and default unittest.cs file that we don’t need and everything just starts to look like a typical Unit Test until the following dialog pops up. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image7.png\" title=\"Project DatabaseUnitTests Configuration\">\n<p>Now in order to run unit tests against the database you need a connection to the database.  In the first part of this you should be able to find your original stored procedure that you used to create the database project.  You will notice that this dialog has an optional additional what it calls a secondary data connection to validate unit tests.  In this sample we will not need this but in a real world application you may so let me explain that scenario.  When an application that is built with a database connection, typically that application and the connection string would just have enough rights to run the stored procedures and nothing else.  In those cases you will want to test those connection string when running the stored procedure that you are testing but that same connection string would not have the rights to check the database to see if those rights are valid especially in a scenario where you want to check if the right values got inserted or deleted, that is where this secondary data connection comes in, it would be a data  connection that had higher rights to look at those values directly from the tables. </p>\n<p>After you have clicked the OK button Visual Studio will display a skeleton of a unit test to test this stored procedure. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image8.png\" title=\"Testing Stored Procedure\">\n<p>In theory we have a unit test that we could run, but the results would indicate that the results are inconclusive because although this stored procedure is being run, it is really just exercising the stored procedure and not really testing it as in giving it some values to insert and checking if those values come back. </p>\n<p>We are going to replace the unit test calls here with the following code snippet.  I have it all in one piece here for you to easily grab this but following this I will break down this code so you can see what is going on.  It is very similar to what the skeleton provided with us but we give it some distinct values. </p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Prepare the test data (expected results)</span></span><br><span class=\"line\"><span class=\"keyword\">DECLARE</span> @EmployeeId <span class=\"built_in\">int</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> TOP <span class=\"number\">1</span> @EmployeeId = EmployeeId</span><br><span class=\"line\">\t<span class=\"keyword\">FROM</span> HumanResources.Employee</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Wrap it into a transaction to return us into a clean state after the test</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span> <span class=\"keyword\">TRANSACTION</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Call the target code</span></span><br><span class=\"line\">EXEC HumanResources.uspUpdateEmployeePersonalInfo</span><br><span class=\"line\">\t@EmployeeId, <span class=\"string\">'987654321'</span>, <span class=\"string\">'3/2/1987'</span>, <span class=\"string\">'S'</span>, <span class=\"string\">'F'</span></span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"comment\">-- Select the results to verify</span></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> NationalIdNumber, Birthdate, MartialStatus, Gender</span><br><span class=\"line\">\t<span class=\"keyword\">FROM</span> HumanResources.Employee</span><br><span class=\"line\">\t<span class=\"keyword\">WHERE</span> EmployeeId = @EmployeeId</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"keyword\">ROLLBACK</span> <span class=\"keyword\">TRANSACTION</span></span><br></pre></td></tr></table></figure>\n<p>The first part of this code is to capture the EmployeeId that we want to update so that is what the first DECLARE statement does.  In the next call we just want to capture an existing EmployeeId from the Employee table and because we really don’t care which on it runs us but we only want want we use the TOP 1 clause in that statement.  At this point our declared variable @EmployeeId now has this value.</p>\n<p><strong>Note:</strong> <em>I have found that there could be a breaking change here that depends on which version of the adventure works database that you have as some will have the employeeId and others will have this column named BusinessEntityID.  To find which one you have go back to the Schema View of the project and expand the Schemas, HumanResources and Tables.  Find the Employee table and expand the Columns, the column in question will be that first one right there.</em></p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image9.png\" title=\"Schema View\">\n<p>Because the stored procedure will make changes to the data in the table and we may not want to actually commit those changes we just want to test these changes we surround the next pieces around a transaction and after we have collected our validation values we can roll this back. </p>\n<p>After the transaction we call the update stored procedure and pass in some specific data.  Next we call a select statement to get those values from the table with the EmployeeId that we just passed into the previous steps.  Finally we roll the whole transaction back so that we do not actually make any changes to the database so we can run this test over and over. </p>\n<p>Before we can actually run this test we need to make some changes to the Test Conditions portion of the unit test.  First you will want to remove the existing entry that is shown there by clicking on the Delete Test button. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image10.png\" title=\"Test Conditions: Data Checksum\">\n<p>After you have removing the existing Test Condition we can then add a new one or more to verify the results.  Select Scalar Value from the dropdown control and click on the “+” button. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image11.png\" title=\"Test Conditions: Scalar Value\">\n<p>On the scalarValueCondition1 line that this action creates, right click on this line and choose Properties, which will display the properties window.  Update the following information: </p>\n<ul>\n<li>Name: VerifyNationalId</li>\n<li>Column number: 1</li>\n<li>Enabled: True</li>\n<li>Expected value: 987654321</li>\n<li>Null expected: False</li>\n<li>ResultSet: 1</li>\n<li>Row number: 1</li>\n</ul>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image12.png\" title=\"Properties\">\n<p>What is really happening here is that we are going to look at that first column and see if it matches the NationalId that we sent to the stored procedure.  NationalId is the first column that is returned in the select statement. </p>\n<p>We are now ready to run the unit test and see that it is working and pass the test.  Typically in a unit test you could be anywhere in the method of the unit test do a right click and you will see one of the context choices being to run test.  However what we have been working on so far has been the design surface of the database unit tests which is why we were able to write SQL statement to write our tests.  To see or get to the actual code page you need to go back to the HumanResourceUnitTests.cs file and right click on it and choose view code. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image13.png\" title=\"Solution Explorer / View Code\">\n<p>As an alternative you could select the file in the solution and press the F7 key, either way you will then be looking at the actual test and if you right click anywhere within that method you will see that one of your choice is to Run Tests.  Do that now and you will see the test results go from a Pending to hopefully a Pass.  If you do get a failure with an exception you will want to check the column names from this table.  Some of the names changed and even the way they are spelled.  It appears to be case sensitive as well.  Like I mentioned before, there seem to be more than one version of this sample database out there and they did make some changes. </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image14.png\" title=\"Test Results\">\n<p>Now that we do have a working test, I always like to make a change to prove that it is working by making it fail.  So to make it fail, change the Expected value to 9876543210000.  I basically just added 4 zeros to the end of the expected result.  Re-run the test and it should fail and if we look at the Test Result Details we can see that the expected results did not match, which is exactly what we expected. </p>\n<p>Take out the padded zeros and run the test once more so that we get a passed test once more.  This is just a step to keep or tests correct. </p>\n<h2 id=\"Associate-the-Unit-Test-to-a-Test-Case\"><a href=\"#Associate-the-Unit-Test-to-a-Test-Case\" class=\"headerlink\" title=\"Associate the Unit Test to a Test Case\"></a>Associate the Unit Test to a Test Case</h2><p>The following section is going to need TFS 2010 in order complete this part of the walk through, and even better if you have Lab Management setup to complete the development, build, deploy, test cycle on these database unit tests. </p>\n<p>Right now, the unit test that we created can be run from Visual Studio just like we have done in this walk through.  You can also make these part of an automated build which if this test project was included in the solution for an automated build in Team Foundation Server (TFS) it would automatically run and be part of the build report.  However, this would not update the Test Plan / Test Suite / Test Case that the QA people are using to manage their tests, but it can. </p>\n<p>In Visual Studio, Create a new Work Item of type: Test Case, and call it “uspUpdateEmployeePersonalInfo Stored Procedure Test”.  We won’t fill anything in the steps section as we are going to go straight to automation with this Test Case.  Click on the Associated Automation tab and click on the ellipse “…” button </p>\n<img src=\"/2011/11/Database-Unit-Testing-from-the-Beginning/image15.png\" title=\"Choose Test\">\n<p>This will bring up the Choose Test dialog box and because we have just this one test open in Visual Studio we will see the exact test that we want associated with this test case.  Click on the OK button. </p>\n<p>We now have a test case that can be used to test the stored procedure in automation.  When this test case is run in automation it will update the test results and will be reported to the Test Plan and Suite that this test case is a part of. </p>\n"},{"title":"Easy Configuration Updates During Deployment","date":"2019-10-24T16:39:35.000Z","_content":"In a proper CI/CD setup where we are building once and deploying that build to various environments as it travels down the pipeline towards production there is almost always a set of configuration files that are different for each environment.  Over the years there have been a number of different techniques that have been used to manage this like never deploying the web.config file during a website deployment or storing the various configurations and copy them to their locations at the time of deployment.  The problem with either of these techniques is that if there is a change in the configuration you need to track down all these config files and update them and because of the mindless copy and paste that would occur with that exercise, it really opens you up for the possibility of errors and because one of these environments could be production, the risk factor is very high.\n## Configuration Transformation\nAlong the way better solutions did appear like the web.config that had a version for each environment.  What actually went in the files where just the things that were different for each environment done using Transform commands.  The biggest problem with this was that you needed something like a build to drive it and so in order to get a build with the proper QA web.config file I would have to create a separate QA build which kills the whole build once and deploying to many environments policy that you want to keep in place.\n\nThere was also a transformation that could take place if you used MSDeploy or WebDeploy to deploy your website.  This is the recommended way to build and deploy a website to IIS.  You simply pass in the arguments \"**/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true**\" and the outcome would be a zip file of the content, a batch file and a SetParameter file.  The problem with the SetParameter file is that although it was pretty good at getting your default connection setup, if you had other connections and things that you wanted it was a very complicated process in order for you to implement this in a complicated web site.\n## Token Replacement\nAnother approach was to tokenize your configuration files with values that you want replaced.  I have used this technique for many years on my own projects.  Since the purchase of Release Manager there was introduced to the Azure DevOps community the idea that a replaceable variable would begin by double underscore and end with a double underscore.  For example:\n```\n     __MyVariable__\n```\nThen a tokenize replacement tool would replace these tokens with real values from your variables table in the build or release pipelines.  There is an extension task called \"Replace Tokens\" and it looks for tokens like this matches them with the variable in the variables table in this case that would be MyVariable and replace it with the value for that scope (the scope is the environment that you are deploying to like Dev, QA, Staging, Prod).\n\nLike I said, this works quite well and I was happy with this except that the tokens were stored in my source control and I don't have a build process updating my configurations as I am running these programs locally while writing code or debugging an application.  I used to have little logic methods that looked to see if what was returned was a token then pass in my local connection string.  Now I am doing something different in my work area then what is happening to the application in other environments.\n## A Better Approach and So Easy\nThe rest of this post is going to explore an even better way and it is so easy, no complicated transform rules to learn or create.  I will setup a sample web application and walk you through the process.  There are a number of assumptions that I a going to make so you can determine if you fall into the same sort of scenario then this solution should work for you as well right out of the gate.  If your situation is a little bit different you might need to make a few adjustments.\n1. I am using Azure DevOps Service\n2. The web application is an ASP.Net MVC full framework (uses web.config)\n3. The web application could be a ASP.NET Core (uses appsettings.json)\n4. The target can be an IIS running in a virtual machine\n5. The target can be an Azure App Service.\n\nThis is what we are going to cover here.  Deploying to the targets is exactly the same, there is a little different approach that you take between the full framework and the dotnet Core applications and I will cover them both.\n## The web.config files\nFor the older web sites that are built using the full .net framework, the configuration is managed by the web.config file.  In my sample application I have a web.config file that has two different connection strings and they are different for each of my environments that I am deploying to.\n{% asset_img web.config.png \"web.config with two connection strings\" %}\nAs I am developing this web site I would have these connection strings pointing to either my local database such as it is showing or this could be a shared database that you are using with the rest of your team for development.  Bottom line here is that you setup the connection string as you need it to work in your workspace.\n## A Word About the build\nMake sure that when you create the build that you are creating an MSDeploy package, which really is just a zip file will all the pieces needed for MSDeploy to deploy your web site to your target.  If you are creating a new build definition, which the choice of templates comes up choose the ASP.NET (if you are using the older full framework web application) or the ASP.NET Core (if you are building the new dotnet core web application).  This will give you all the necessary tasks to get you up and going quickly.  When you do that it already populates the MSBuild arguments with all the appropriate arguments to create the package for you.\n\nIf you already have a build definition setup and it is not creating the MSDeploy package, then add the following arguments to the MSBuild task.\n- [ ] /p:DeployOnBuild=true\n- [ ] /p:WebPublishMethod=Package\n- [ ] /p:PackageAsSingleFile=true\n- [ ] /p:SkipInvalidConfigurations=true\n- [ ] /p:PackageLocation=\"$(build.artifactStagingDirectory)\n\nJust list this one after the other separated by a space and you are going to land up with a zipped up package in your artifactStagingDirectory.  In the new definition that was created from the template it also included a Publish Artifact task which then takes the output from artifactStagingDirectory and pushes it into the final build artifact.  We will need that for our deployment steps which is where the point of this post takes place.  If you do not have a Artifact Publish task then you will need to add one at the end of your build definition.\n\nIf you search for Publish build artifacts you should see the task.  You can pretty much leave the defaults as they are just make sure that the Path to publish is : _**$(build.artifactstagingdirectory)**_\n## The Release definition\nI am assuming that you probably already have a release definition setup to deploy this web application to a couple of environments that may or may not include Production.  Likely because you had issues with dealing with the different configurations especially between your testing environments like Dev and QA and the Production environment which is usually wildly different from the other two.  I would also assume that is why you are reading this post so that you can implement these changes and have the peace of mind that each environment will get the correct connection strings as they travel through the pipeline.\n\nIn this next section this is where things differ between the older ASP.Net full framework and dotnet Core.  I will cover each of these in their own section and you just need to follow the instructions to the type of web app you are deploying.\n\n### ASP.NET (older full framework)\nFor the legacy AGP.Net full framework the connection string is stored in the web.config file.  So lets look at that.  \n{% asset_img web.config.png \"web.config with two connection strings\" %}\nIn this sample you see that I have two valid connection strings in the connectionStrings section of this web.config file.  One is called **\"DefaultConnection\"** and the other is called **\"SecondConnection\"**  It is important to note the name of these two connection strings because we will need them in the next part as add them to the variables table in the release definition.\n##### Variable Table\nIn case you may not be aware of what I am talking about, in an Azure DevOps Release Definition there is a menu item at the top that says Variables.  When you click on it a page opens up similar to the following.\n{% asset_img variables.png \"variables table\" %}\nExcept that you won't have the entries that I already have here.  One of the differences between the variable table that is in the Build Definition compared to the Release Definition is the addition of the Scope.  The Scope represents the environment that this change will be applied to.  If you have a variable that is updated the same for all the environments then you can leave it to the default scope which is Release and that will apply to all the environments unless there is one for a particular environment.  Specific scope variables will override the default Release ones.  So that means I can repeat the variable names that I have for each of the connection strings for each environment that I am deploying to like Dev, QA, Staging, Production and  as it goes through the pipeline the correct connection string will be applied.\n##### IIS Web App Deployment Task\nNow, before you run off and try this out there is one more thing we need to do to this definition before it is ready to go.  The magic which makes all this happen is the task \"IIS Web App Deploy\"  When you are searching for the tasks I just enter IIS and that should be enough to see this task bubble to the top of the list.\n{% asset_img IISWebAppDeployTask.png \"IIS Web App Deploy\" %}\nIn the property settings of this task you want to expand the File Transforms & Variable Substitution Options.  Make sure the checkbox XML variable substitution has been checked as this is what will kick off the updating of the web.config with the correct values.\n\nThis just makes things so much simpler and easy to manage no more of these situations where we are managing connection strings in a bunch of places or having to write complicated transform rules, you don't even need to tokenize the web.config file it all just happens almost like magic.\n\n### ASP.NET (newer Core)\nFor the newer ASP.NET Core, things are a little different because the web.config if there even is one is much cleaner as ASP.NET Core is trying to follow a convention over configuration even more so then it has in the past.  You are going to find that the connection strings are stored in a json file called appsettings.json.  So the principle is similar the actual syntax is a bit different.  So lets start by looking at a possible appsettings.json file and go from there.\n{% asset_img Json.png \"appsettings.json file\" %}\nIt is pretty clear that I do have two connection strings setup here but how we manage this in the variables table is a bit different.\n##### Variable Table\nWhen we add these values to the variable table you will notice that I have included the parent node that holds these connection string as the prefix followed by a \".\" then the actual connection string name.  This is something that is different than dealing with the web.config file.\n{% asset_img variable-json.png \"Variables Table\" %}\nYou will notice in this example I have given you a bigger example of a release definition where we have a different connection string for each of the different scopes (deployable environments in our pipeline).\n##### IIS Web App Deployment Task\nOne more slight twist that we need to address is in the IIS Web App Deployment task.  This one is probably a bit more obvious as we are transforming a different type of file so there would be a change in the properties of this task for that.\n{% asset_img IISWebAppDeployJsonTask.png \"IIS Web App Deploy\" %}\nThe only thing you need here is to expand the File Transforms & Variable Substitution Options and add the name of the json file.  In this case it is the appsettings.json file and the task will take care of it for you.\n","source":"_posts/Easy-Configuration-updates-during-Deployment.md","raw":"---\ntitle: Easy Configuration Updates During Deployment\ndate: 2019-10-24 09:39:35\ntags:\n- DevOps\n- TFS\n- dotNet\n---\nIn a proper CI/CD setup where we are building once and deploying that build to various environments as it travels down the pipeline towards production there is almost always a set of configuration files that are different for each environment.  Over the years there have been a number of different techniques that have been used to manage this like never deploying the web.config file during a website deployment or storing the various configurations and copy them to their locations at the time of deployment.  The problem with either of these techniques is that if there is a change in the configuration you need to track down all these config files and update them and because of the mindless copy and paste that would occur with that exercise, it really opens you up for the possibility of errors and because one of these environments could be production, the risk factor is very high.\n## Configuration Transformation\nAlong the way better solutions did appear like the web.config that had a version for each environment.  What actually went in the files where just the things that were different for each environment done using Transform commands.  The biggest problem with this was that you needed something like a build to drive it and so in order to get a build with the proper QA web.config file I would have to create a separate QA build which kills the whole build once and deploying to many environments policy that you want to keep in place.\n\nThere was also a transformation that could take place if you used MSDeploy or WebDeploy to deploy your website.  This is the recommended way to build and deploy a website to IIS.  You simply pass in the arguments \"**/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true**\" and the outcome would be a zip file of the content, a batch file and a SetParameter file.  The problem with the SetParameter file is that although it was pretty good at getting your default connection setup, if you had other connections and things that you wanted it was a very complicated process in order for you to implement this in a complicated web site.\n## Token Replacement\nAnother approach was to tokenize your configuration files with values that you want replaced.  I have used this technique for many years on my own projects.  Since the purchase of Release Manager there was introduced to the Azure DevOps community the idea that a replaceable variable would begin by double underscore and end with a double underscore.  For example:\n```\n     __MyVariable__\n```\nThen a tokenize replacement tool would replace these tokens with real values from your variables table in the build or release pipelines.  There is an extension task called \"Replace Tokens\" and it looks for tokens like this matches them with the variable in the variables table in this case that would be MyVariable and replace it with the value for that scope (the scope is the environment that you are deploying to like Dev, QA, Staging, Prod).\n\nLike I said, this works quite well and I was happy with this except that the tokens were stored in my source control and I don't have a build process updating my configurations as I am running these programs locally while writing code or debugging an application.  I used to have little logic methods that looked to see if what was returned was a token then pass in my local connection string.  Now I am doing something different in my work area then what is happening to the application in other environments.\n## A Better Approach and So Easy\nThe rest of this post is going to explore an even better way and it is so easy, no complicated transform rules to learn or create.  I will setup a sample web application and walk you through the process.  There are a number of assumptions that I a going to make so you can determine if you fall into the same sort of scenario then this solution should work for you as well right out of the gate.  If your situation is a little bit different you might need to make a few adjustments.\n1. I am using Azure DevOps Service\n2. The web application is an ASP.Net MVC full framework (uses web.config)\n3. The web application could be a ASP.NET Core (uses appsettings.json)\n4. The target can be an IIS running in a virtual machine\n5. The target can be an Azure App Service.\n\nThis is what we are going to cover here.  Deploying to the targets is exactly the same, there is a little different approach that you take between the full framework and the dotnet Core applications and I will cover them both.\n## The web.config files\nFor the older web sites that are built using the full .net framework, the configuration is managed by the web.config file.  In my sample application I have a web.config file that has two different connection strings and they are different for each of my environments that I am deploying to.\n{% asset_img web.config.png \"web.config with two connection strings\" %}\nAs I am developing this web site I would have these connection strings pointing to either my local database such as it is showing or this could be a shared database that you are using with the rest of your team for development.  Bottom line here is that you setup the connection string as you need it to work in your workspace.\n## A Word About the build\nMake sure that when you create the build that you are creating an MSDeploy package, which really is just a zip file will all the pieces needed for MSDeploy to deploy your web site to your target.  If you are creating a new build definition, which the choice of templates comes up choose the ASP.NET (if you are using the older full framework web application) or the ASP.NET Core (if you are building the new dotnet core web application).  This will give you all the necessary tasks to get you up and going quickly.  When you do that it already populates the MSBuild arguments with all the appropriate arguments to create the package for you.\n\nIf you already have a build definition setup and it is not creating the MSDeploy package, then add the following arguments to the MSBuild task.\n- [ ] /p:DeployOnBuild=true\n- [ ] /p:WebPublishMethod=Package\n- [ ] /p:PackageAsSingleFile=true\n- [ ] /p:SkipInvalidConfigurations=true\n- [ ] /p:PackageLocation=\"$(build.artifactStagingDirectory)\n\nJust list this one after the other separated by a space and you are going to land up with a zipped up package in your artifactStagingDirectory.  In the new definition that was created from the template it also included a Publish Artifact task which then takes the output from artifactStagingDirectory and pushes it into the final build artifact.  We will need that for our deployment steps which is where the point of this post takes place.  If you do not have a Artifact Publish task then you will need to add one at the end of your build definition.\n\nIf you search for Publish build artifacts you should see the task.  You can pretty much leave the defaults as they are just make sure that the Path to publish is : _**$(build.artifactstagingdirectory)**_\n## The Release definition\nI am assuming that you probably already have a release definition setup to deploy this web application to a couple of environments that may or may not include Production.  Likely because you had issues with dealing with the different configurations especially between your testing environments like Dev and QA and the Production environment which is usually wildly different from the other two.  I would also assume that is why you are reading this post so that you can implement these changes and have the peace of mind that each environment will get the correct connection strings as they travel through the pipeline.\n\nIn this next section this is where things differ between the older ASP.Net full framework and dotnet Core.  I will cover each of these in their own section and you just need to follow the instructions to the type of web app you are deploying.\n\n### ASP.NET (older full framework)\nFor the legacy AGP.Net full framework the connection string is stored in the web.config file.  So lets look at that.  \n{% asset_img web.config.png \"web.config with two connection strings\" %}\nIn this sample you see that I have two valid connection strings in the connectionStrings section of this web.config file.  One is called **\"DefaultConnection\"** and the other is called **\"SecondConnection\"**  It is important to note the name of these two connection strings because we will need them in the next part as add them to the variables table in the release definition.\n##### Variable Table\nIn case you may not be aware of what I am talking about, in an Azure DevOps Release Definition there is a menu item at the top that says Variables.  When you click on it a page opens up similar to the following.\n{% asset_img variables.png \"variables table\" %}\nExcept that you won't have the entries that I already have here.  One of the differences between the variable table that is in the Build Definition compared to the Release Definition is the addition of the Scope.  The Scope represents the environment that this change will be applied to.  If you have a variable that is updated the same for all the environments then you can leave it to the default scope which is Release and that will apply to all the environments unless there is one for a particular environment.  Specific scope variables will override the default Release ones.  So that means I can repeat the variable names that I have for each of the connection strings for each environment that I am deploying to like Dev, QA, Staging, Production and  as it goes through the pipeline the correct connection string will be applied.\n##### IIS Web App Deployment Task\nNow, before you run off and try this out there is one more thing we need to do to this definition before it is ready to go.  The magic which makes all this happen is the task \"IIS Web App Deploy\"  When you are searching for the tasks I just enter IIS and that should be enough to see this task bubble to the top of the list.\n{% asset_img IISWebAppDeployTask.png \"IIS Web App Deploy\" %}\nIn the property settings of this task you want to expand the File Transforms & Variable Substitution Options.  Make sure the checkbox XML variable substitution has been checked as this is what will kick off the updating of the web.config with the correct values.\n\nThis just makes things so much simpler and easy to manage no more of these situations where we are managing connection strings in a bunch of places or having to write complicated transform rules, you don't even need to tokenize the web.config file it all just happens almost like magic.\n\n### ASP.NET (newer Core)\nFor the newer ASP.NET Core, things are a little different because the web.config if there even is one is much cleaner as ASP.NET Core is trying to follow a convention over configuration even more so then it has in the past.  You are going to find that the connection strings are stored in a json file called appsettings.json.  So the principle is similar the actual syntax is a bit different.  So lets start by looking at a possible appsettings.json file and go from there.\n{% asset_img Json.png \"appsettings.json file\" %}\nIt is pretty clear that I do have two connection strings setup here but how we manage this in the variables table is a bit different.\n##### Variable Table\nWhen we add these values to the variable table you will notice that I have included the parent node that holds these connection string as the prefix followed by a \".\" then the actual connection string name.  This is something that is different than dealing with the web.config file.\n{% asset_img variable-json.png \"Variables Table\" %}\nYou will notice in this example I have given you a bigger example of a release definition where we have a different connection string for each of the different scopes (deployable environments in our pipeline).\n##### IIS Web App Deployment Task\nOne more slight twist that we need to address is in the IIS Web App Deployment task.  This one is probably a bit more obvious as we are transforming a different type of file so there would be a change in the properties of this task for that.\n{% asset_img IISWebAppDeployJsonTask.png \"IIS Web App Deploy\" %}\nThe only thing you need here is to expand the File Transforms & Variable Substitution Options and add the name of the json file.  In this case it is the appsettings.json file and the task will take care of it for you.\n","slug":"Easy-Configuration-updates-during-Deployment","published":1,"updated":"2020-01-05T00:36:05.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfe0006s4uf2jrf7rm8","content":"<p>In a proper CI/CD setup where we are building once and deploying that build to various environments as it travels down the pipeline towards production there is almost always a set of configuration files that are different for each environment.  Over the years there have been a number of different techniques that have been used to manage this like never deploying the web.config file during a website deployment or storing the various configurations and copy them to their locations at the time of deployment.  The problem with either of these techniques is that if there is a change in the configuration you need to track down all these config files and update them and because of the mindless copy and paste that would occur with that exercise, it really opens you up for the possibility of errors and because one of these environments could be production, the risk factor is very high.</p>\n<h2 id=\"Configuration-Transformation\"><a href=\"#Configuration-Transformation\" class=\"headerlink\" title=\"Configuration Transformation\"></a>Configuration Transformation</h2><p>Along the way better solutions did appear like the web.config that had a version for each environment.  What actually went in the files where just the things that were different for each environment done using Transform commands.  The biggest problem with this was that you needed something like a build to drive it and so in order to get a build with the proper QA web.config file I would have to create a separate QA build which kills the whole build once and deploying to many environments policy that you want to keep in place.</p>\n<p>There was also a transformation that could take place if you used MSDeploy or WebDeploy to deploy your website.  This is the recommended way to build and deploy a website to IIS.  You simply pass in the arguments “<strong>/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true</strong>“ and the outcome would be a zip file of the content, a batch file and a SetParameter file.  The problem with the SetParameter file is that although it was pretty good at getting your default connection setup, if you had other connections and things that you wanted it was a very complicated process in order for you to implement this in a complicated web site.</p>\n<h2 id=\"Token-Replacement\"><a href=\"#Token-Replacement\" class=\"headerlink\" title=\"Token Replacement\"></a>Token Replacement</h2><p>Another approach was to tokenize your configuration files with values that you want replaced.  I have used this technique for many years on my own projects.  Since the purchase of Release Manager there was introduced to the Azure DevOps community the idea that a replaceable variable would begin by double underscore and end with a double underscore.  For example:<br><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">__MyVariable__</span></span><br></pre></td></tr></table></figure></p>\n<p>Then a tokenize replacement tool would replace these tokens with real values from your variables table in the build or release pipelines.  There is an extension task called “Replace Tokens” and it looks for tokens like this matches them with the variable in the variables table in this case that would be MyVariable and replace it with the value for that scope (the scope is the environment that you are deploying to like Dev, QA, Staging, Prod).</p>\n<p>Like I said, this works quite well and I was happy with this except that the tokens were stored in my source control and I don’t have a build process updating my configurations as I am running these programs locally while writing code or debugging an application.  I used to have little logic methods that looked to see if what was returned was a token then pass in my local connection string.  Now I am doing something different in my work area then what is happening to the application in other environments.</p>\n<h2 id=\"A-Better-Approach-and-So-Easy\"><a href=\"#A-Better-Approach-and-So-Easy\" class=\"headerlink\" title=\"A Better Approach and So Easy\"></a>A Better Approach and So Easy</h2><p>The rest of this post is going to explore an even better way and it is so easy, no complicated transform rules to learn or create.  I will setup a sample web application and walk you through the process.  There are a number of assumptions that I a going to make so you can determine if you fall into the same sort of scenario then this solution should work for you as well right out of the gate.  If your situation is a little bit different you might need to make a few adjustments.</p>\n<ol>\n<li>I am using Azure DevOps Service</li>\n<li>The web application is an ASP.Net MVC full framework (uses web.config)</li>\n<li>The web application could be a ASP.NET Core (uses appsettings.json)</li>\n<li>The target can be an IIS running in a virtual machine</li>\n<li>The target can be an Azure App Service.</li>\n</ol>\n<p>This is what we are going to cover here.  Deploying to the targets is exactly the same, there is a little different approach that you take between the full framework and the dotnet Core applications and I will cover them both.</p>\n<h2 id=\"The-web-config-files\"><a href=\"#The-web-config-files\" class=\"headerlink\" title=\"The web.config files\"></a>The web.config files</h2><p>For the older web sites that are built using the full .net framework, the configuration is managed by the web.config file.  In my sample application I have a web.config file that has two different connection strings and they are different for each of my environments that I am deploying to.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/web.config.png\" title=\"web.config with two connection strings\"><br>As I am developing this web site I would have these connection strings pointing to either my local database such as it is showing or this could be a shared database that you are using with the rest of your team for development.  Bottom line here is that you setup the connection string as you need it to work in your workspace.</p>\n<h2 id=\"A-Word-About-the-build\"><a href=\"#A-Word-About-the-build\" class=\"headerlink\" title=\"A Word About the build\"></a>A Word About the build</h2><p>Make sure that when you create the build that you are creating an MSDeploy package, which really is just a zip file will all the pieces needed for MSDeploy to deploy your web site to your target.  If you are creating a new build definition, which the choice of templates comes up choose the ASP.NET (if you are using the older full framework web application) or the ASP.NET Core (if you are building the new dotnet core web application).  This will give you all the necessary tasks to get you up and going quickly.  When you do that it already populates the MSBuild arguments with all the appropriate arguments to create the package for you.</p>\n<p>If you already have a build definition setup and it is not creating the MSDeploy package, then add the following arguments to the MSBuild task.</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:DeployOnBuild=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:WebPublishMethod=Package</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:PackageAsSingleFile=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:SkipInvalidConfigurations=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:PackageLocation=”$(build.artifactStagingDirectory)</li>\n</ul>\n<p>Just list this one after the other separated by a space and you are going to land up with a zipped up package in your artifactStagingDirectory.  In the new definition that was created from the template it also included a Publish Artifact task which then takes the output from artifactStagingDirectory and pushes it into the final build artifact.  We will need that for our deployment steps which is where the point of this post takes place.  If you do not have a Artifact Publish task then you will need to add one at the end of your build definition.</p>\n<p>If you search for Publish build artifacts you should see the task.  You can pretty much leave the defaults as they are just make sure that the Path to publish is : <em><strong>$(build.artifactstagingdirectory)</strong></em></p>\n<h2 id=\"The-Release-definition\"><a href=\"#The-Release-definition\" class=\"headerlink\" title=\"The Release definition\"></a>The Release definition</h2><p>I am assuming that you probably already have a release definition setup to deploy this web application to a couple of environments that may or may not include Production.  Likely because you had issues with dealing with the different configurations especially between your testing environments like Dev and QA and the Production environment which is usually wildly different from the other two.  I would also assume that is why you are reading this post so that you can implement these changes and have the peace of mind that each environment will get the correct connection strings as they travel through the pipeline.</p>\n<p>In this next section this is where things differ between the older ASP.Net full framework and dotnet Core.  I will cover each of these in their own section and you just need to follow the instructions to the type of web app you are deploying.</p>\n<h3 id=\"ASP-NET-older-full-framework\"><a href=\"#ASP-NET-older-full-framework\" class=\"headerlink\" title=\"ASP.NET (older full framework)\"></a>ASP.NET (older full framework)</h3><p>For the legacy AGP.Net full framework the connection string is stored in the web.config file.  So lets look at that.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/web.config.png\" title=\"web.config with two connection strings\"><br>In this sample you see that I have two valid connection strings in the connectionStrings section of this web.config file.  One is called <strong>“DefaultConnection”</strong> and the other is called <strong>“SecondConnection”</strong>  It is important to note the name of these two connection strings because we will need them in the next part as add them to the variables table in the release definition.</p>\n<h5 id=\"Variable-Table\"><a href=\"#Variable-Table\" class=\"headerlink\" title=\"Variable Table\"></a>Variable Table</h5><p>In case you may not be aware of what I am talking about, in an Azure DevOps Release Definition there is a menu item at the top that says Variables.  When you click on it a page opens up similar to the following.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/variables.png\" title=\"variables table\"><br>Except that you won’t have the entries that I already have here.  One of the differences between the variable table that is in the Build Definition compared to the Release Definition is the addition of the Scope.  The Scope represents the environment that this change will be applied to.  If you have a variable that is updated the same for all the environments then you can leave it to the default scope which is Release and that will apply to all the environments unless there is one for a particular environment.  Specific scope variables will override the default Release ones.  So that means I can repeat the variable names that I have for each of the connection strings for each environment that I am deploying to like Dev, QA, Staging, Production and  as it goes through the pipeline the correct connection string will be applied.</p>\n<h5 id=\"IIS-Web-App-Deployment-Task\"><a href=\"#IIS-Web-App-Deployment-Task\" class=\"headerlink\" title=\"IIS Web App Deployment Task\"></a>IIS Web App Deployment Task</h5><p>Now, before you run off and try this out there is one more thing we need to do to this definition before it is ready to go.  The magic which makes all this happen is the task “IIS Web App Deploy”  When you are searching for the tasks I just enter IIS and that should be enough to see this task bubble to the top of the list.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/IISWebAppDeployTask.png\" title=\"IIS Web App Deploy\"><br>In the property settings of this task you want to expand the File Transforms &amp; Variable Substitution Options.  Make sure the checkbox XML variable substitution has been checked as this is what will kick off the updating of the web.config with the correct values.</p>\n<p>This just makes things so much simpler and easy to manage no more of these situations where we are managing connection strings in a bunch of places or having to write complicated transform rules, you don’t even need to tokenize the web.config file it all just happens almost like magic.</p>\n<h3 id=\"ASP-NET-newer-Core\"><a href=\"#ASP-NET-newer-Core\" class=\"headerlink\" title=\"ASP.NET (newer Core)\"></a>ASP.NET (newer Core)</h3><p>For the newer ASP.NET Core, things are a little different because the web.config if there even is one is much cleaner as ASP.NET Core is trying to follow a convention over configuration even more so then it has in the past.  You are going to find that the connection strings are stored in a json file called appsettings.json.  So the principle is similar the actual syntax is a bit different.  So lets start by looking at a possible appsettings.json file and go from there.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/Json.png\" title=\"appsettings.json file\"><br>It is pretty clear that I do have two connection strings setup here but how we manage this in the variables table is a bit different.</p>\n<h5 id=\"Variable-Table-1\"><a href=\"#Variable-Table-1\" class=\"headerlink\" title=\"Variable Table\"></a>Variable Table</h5><p>When we add these values to the variable table you will notice that I have included the parent node that holds these connection string as the prefix followed by a “.” then the actual connection string name.  This is something that is different than dealing with the web.config file.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/variable-json.png\" title=\"Variables Table\"><br>You will notice in this example I have given you a bigger example of a release definition where we have a different connection string for each of the different scopes (deployable environments in our pipeline).</p>\n<h5 id=\"IIS-Web-App-Deployment-Task-1\"><a href=\"#IIS-Web-App-Deployment-Task-1\" class=\"headerlink\" title=\"IIS Web App Deployment Task\"></a>IIS Web App Deployment Task</h5><p>One more slight twist that we need to address is in the IIS Web App Deployment task.  This one is probably a bit more obvious as we are transforming a different type of file so there would be a change in the properties of this task for that.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/IISWebAppDeployJsonTask.png\" title=\"IIS Web App Deploy\"><br>The only thing you need here is to expand the File Transforms &amp; Variable Substitution Options and add the name of the json file.  In this case it is the appsettings.json file and the task will take care of it for you.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>In a proper CI/CD setup where we are building once and deploying that build to various environments as it travels down the pipeline towards production there is almost always a set of configuration files that are different for each environment.  Over the years there have been a number of different techniques that have been used to manage this like never deploying the web.config file during a website deployment or storing the various configurations and copy them to their locations at the time of deployment.  The problem with either of these techniques is that if there is a change in the configuration you need to track down all these config files and update them and because of the mindless copy and paste that would occur with that exercise, it really opens you up for the possibility of errors and because one of these environments could be production, the risk factor is very high.</p>\n<h2 id=\"Configuration-Transformation\"><a href=\"#Configuration-Transformation\" class=\"headerlink\" title=\"Configuration Transformation\"></a>Configuration Transformation</h2><p>Along the way better solutions did appear like the web.config that had a version for each environment.  What actually went in the files where just the things that were different for each environment done using Transform commands.  The biggest problem with this was that you needed something like a build to drive it and so in order to get a build with the proper QA web.config file I would have to create a separate QA build which kills the whole build once and deploying to many environments policy that you want to keep in place.</p>\n<p>There was also a transformation that could take place if you used MSDeploy or WebDeploy to deploy your website.  This is the recommended way to build and deploy a website to IIS.  You simply pass in the arguments “<strong>/p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true</strong>“ and the outcome would be a zip file of the content, a batch file and a SetParameter file.  The problem with the SetParameter file is that although it was pretty good at getting your default connection setup, if you had other connections and things that you wanted it was a very complicated process in order for you to implement this in a complicated web site.</p>\n<h2 id=\"Token-Replacement\"><a href=\"#Token-Replacement\" class=\"headerlink\" title=\"Token Replacement\"></a>Token Replacement</h2><p>Another approach was to tokenize your configuration files with values that you want replaced.  I have used this technique for many years on my own projects.  Since the purchase of Release Manager there was introduced to the Azure DevOps community the idea that a replaceable variable would begin by double underscore and end with a double underscore.  For example:<br><figure class=\"highlight markdown\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"strong\">__MyVariable__</span></span><br></pre></td></tr></table></figure></p>\n<p>Then a tokenize replacement tool would replace these tokens with real values from your variables table in the build or release pipelines.  There is an extension task called “Replace Tokens” and it looks for tokens like this matches them with the variable in the variables table in this case that would be MyVariable and replace it with the value for that scope (the scope is the environment that you are deploying to like Dev, QA, Staging, Prod).</p>\n<p>Like I said, this works quite well and I was happy with this except that the tokens were stored in my source control and I don’t have a build process updating my configurations as I am running these programs locally while writing code or debugging an application.  I used to have little logic methods that looked to see if what was returned was a token then pass in my local connection string.  Now I am doing something different in my work area then what is happening to the application in other environments.</p>\n<h2 id=\"A-Better-Approach-and-So-Easy\"><a href=\"#A-Better-Approach-and-So-Easy\" class=\"headerlink\" title=\"A Better Approach and So Easy\"></a>A Better Approach and So Easy</h2><p>The rest of this post is going to explore an even better way and it is so easy, no complicated transform rules to learn or create.  I will setup a sample web application and walk you through the process.  There are a number of assumptions that I a going to make so you can determine if you fall into the same sort of scenario then this solution should work for you as well right out of the gate.  If your situation is a little bit different you might need to make a few adjustments.</p>\n<ol>\n<li>I am using Azure DevOps Service</li>\n<li>The web application is an ASP.Net MVC full framework (uses web.config)</li>\n<li>The web application could be a ASP.NET Core (uses appsettings.json)</li>\n<li>The target can be an IIS running in a virtual machine</li>\n<li>The target can be an Azure App Service.</li>\n</ol>\n<p>This is what we are going to cover here.  Deploying to the targets is exactly the same, there is a little different approach that you take between the full framework and the dotnet Core applications and I will cover them both.</p>\n<h2 id=\"The-web-config-files\"><a href=\"#The-web-config-files\" class=\"headerlink\" title=\"The web.config files\"></a>The web.config files</h2><p>For the older web sites that are built using the full .net framework, the configuration is managed by the web.config file.  In my sample application I have a web.config file that has two different connection strings and they are different for each of my environments that I am deploying to.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/web.config.png\" title=\"web.config with two connection strings\"><br>As I am developing this web site I would have these connection strings pointing to either my local database such as it is showing or this could be a shared database that you are using with the rest of your team for development.  Bottom line here is that you setup the connection string as you need it to work in your workspace.</p>\n<h2 id=\"A-Word-About-the-build\"><a href=\"#A-Word-About-the-build\" class=\"headerlink\" title=\"A Word About the build\"></a>A Word About the build</h2><p>Make sure that when you create the build that you are creating an MSDeploy package, which really is just a zip file will all the pieces needed for MSDeploy to deploy your web site to your target.  If you are creating a new build definition, which the choice of templates comes up choose the ASP.NET (if you are using the older full framework web application) or the ASP.NET Core (if you are building the new dotnet core web application).  This will give you all the necessary tasks to get you up and going quickly.  When you do that it already populates the MSBuild arguments with all the appropriate arguments to create the package for you.</p>\n<p>If you already have a build definition setup and it is not creating the MSDeploy package, then add the following arguments to the MSBuild task.</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:DeployOnBuild=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:WebPublishMethod=Package</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:PackageAsSingleFile=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:SkipInvalidConfigurations=true</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> /p:PackageLocation=”$(build.artifactStagingDirectory)</li>\n</ul>\n<p>Just list this one after the other separated by a space and you are going to land up with a zipped up package in your artifactStagingDirectory.  In the new definition that was created from the template it also included a Publish Artifact task which then takes the output from artifactStagingDirectory and pushes it into the final build artifact.  We will need that for our deployment steps which is where the point of this post takes place.  If you do not have a Artifact Publish task then you will need to add one at the end of your build definition.</p>\n<p>If you search for Publish build artifacts you should see the task.  You can pretty much leave the defaults as they are just make sure that the Path to publish is : <em><strong>$(build.artifactstagingdirectory)</strong></em></p>\n<h2 id=\"The-Release-definition\"><a href=\"#The-Release-definition\" class=\"headerlink\" title=\"The Release definition\"></a>The Release definition</h2><p>I am assuming that you probably already have a release definition setup to deploy this web application to a couple of environments that may or may not include Production.  Likely because you had issues with dealing with the different configurations especially between your testing environments like Dev and QA and the Production environment which is usually wildly different from the other two.  I would also assume that is why you are reading this post so that you can implement these changes and have the peace of mind that each environment will get the correct connection strings as they travel through the pipeline.</p>\n<p>In this next section this is where things differ between the older ASP.Net full framework and dotnet Core.  I will cover each of these in their own section and you just need to follow the instructions to the type of web app you are deploying.</p>\n<h3 id=\"ASP-NET-older-full-framework\"><a href=\"#ASP-NET-older-full-framework\" class=\"headerlink\" title=\"ASP.NET (older full framework)\"></a>ASP.NET (older full framework)</h3><p>For the legacy AGP.Net full framework the connection string is stored in the web.config file.  So lets look at that.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/web.config.png\" title=\"web.config with two connection strings\"><br>In this sample you see that I have two valid connection strings in the connectionStrings section of this web.config file.  One is called <strong>“DefaultConnection”</strong> and the other is called <strong>“SecondConnection”</strong>  It is important to note the name of these two connection strings because we will need them in the next part as add them to the variables table in the release definition.</p>\n<h5 id=\"Variable-Table\"><a href=\"#Variable-Table\" class=\"headerlink\" title=\"Variable Table\"></a>Variable Table</h5><p>In case you may not be aware of what I am talking about, in an Azure DevOps Release Definition there is a menu item at the top that says Variables.  When you click on it a page opens up similar to the following.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/variables.png\" title=\"variables table\"><br>Except that you won’t have the entries that I already have here.  One of the differences between the variable table that is in the Build Definition compared to the Release Definition is the addition of the Scope.  The Scope represents the environment that this change will be applied to.  If you have a variable that is updated the same for all the environments then you can leave it to the default scope which is Release and that will apply to all the environments unless there is one for a particular environment.  Specific scope variables will override the default Release ones.  So that means I can repeat the variable names that I have for each of the connection strings for each environment that I am deploying to like Dev, QA, Staging, Production and  as it goes through the pipeline the correct connection string will be applied.</p>\n<h5 id=\"IIS-Web-App-Deployment-Task\"><a href=\"#IIS-Web-App-Deployment-Task\" class=\"headerlink\" title=\"IIS Web App Deployment Task\"></a>IIS Web App Deployment Task</h5><p>Now, before you run off and try this out there is one more thing we need to do to this definition before it is ready to go.  The magic which makes all this happen is the task “IIS Web App Deploy”  When you are searching for the tasks I just enter IIS and that should be enough to see this task bubble to the top of the list.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/IISWebAppDeployTask.png\" title=\"IIS Web App Deploy\"><br>In the property settings of this task you want to expand the File Transforms &amp; Variable Substitution Options.  Make sure the checkbox XML variable substitution has been checked as this is what will kick off the updating of the web.config with the correct values.</p>\n<p>This just makes things so much simpler and easy to manage no more of these situations where we are managing connection strings in a bunch of places or having to write complicated transform rules, you don’t even need to tokenize the web.config file it all just happens almost like magic.</p>\n<h3 id=\"ASP-NET-newer-Core\"><a href=\"#ASP-NET-newer-Core\" class=\"headerlink\" title=\"ASP.NET (newer Core)\"></a>ASP.NET (newer Core)</h3><p>For the newer ASP.NET Core, things are a little different because the web.config if there even is one is much cleaner as ASP.NET Core is trying to follow a convention over configuration even more so then it has in the past.  You are going to find that the connection strings are stored in a json file called appsettings.json.  So the principle is similar the actual syntax is a bit different.  So lets start by looking at a possible appsettings.json file and go from there.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/Json.png\" title=\"appsettings.json file\"><br>It is pretty clear that I do have two connection strings setup here but how we manage this in the variables table is a bit different.</p>\n<h5 id=\"Variable-Table-1\"><a href=\"#Variable-Table-1\" class=\"headerlink\" title=\"Variable Table\"></a>Variable Table</h5><p>When we add these values to the variable table you will notice that I have included the parent node that holds these connection string as the prefix followed by a “.” then the actual connection string name.  This is something that is different than dealing with the web.config file.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/variable-json.png\" title=\"Variables Table\"><br>You will notice in this example I have given you a bigger example of a release definition where we have a different connection string for each of the different scopes (deployable environments in our pipeline).</p>\n<h5 id=\"IIS-Web-App-Deployment-Task-1\"><a href=\"#IIS-Web-App-Deployment-Task-1\" class=\"headerlink\" title=\"IIS Web App Deployment Task\"></a>IIS Web App Deployment Task</h5><p>One more slight twist that we need to address is in the IIS Web App Deployment task.  This one is probably a bit more obvious as we are transforming a different type of file so there would be a change in the properties of this task for that.<br><img src=\"/2019/10/Easy-Configuration-updates-during-Deployment/IISWebAppDeployJsonTask.png\" title=\"IIS Web App Deploy\"><br>The only thing you need here is to expand the File Transforms &amp; Variable Substitution Options and add the name of the json file.  In this case it is the appsettings.json file and the task will take care of it for you.</p>\n"},{"title":"Goal Tracking","date":"2009-02-09T08:00:00.000Z","_content":"Since about the beginning of the year I have been thinking about goal tracking.  I compiled a long list of technologies that I wanted to learn, experiment with and maybe even build some projects using some of these newly learned skills.  Nothing quite like turning something new into something useful.  I find that this technique provides me with the best understanding of how and why a technology would be used in one scenario over another.  My goals for this year is a long list and some have a dependency of a previous goal being completed before I even begin, like reading the book before I begin my project based on the technology. \n\nHowever, I suffer from the getting bored and just needing a break from a certain goal and then forget to get back to it at the appropriate time illness.  It's like I need something to help me track what my goals are and an easy to see a KPI like indicator to show me which goals I need to pay attention to right now or I might miss my target date altogether.  Before I go much farther I should define KPI:  \n\n\n \nKPI's are Key Performance Indicators which help organizations achieve organizational goals through the definition and measurement of progress. The key indicators are agreed upon by an organization and are indicators which can be measured that will reflect success factors. The KPIs selected must reflect the organization's goals, they must be key to its success, and they must be measurable. Key performance indicators usually are long-term considerations for an organization. \nThis is what I need for my goals, some way to track my progress.  I went to work on it, storing the goals was easy.  Give it a name, what your target date is for completing the goal and some exit criteria.  Okay, so I had to think a little bit about that last one, but I needed something that would tell me when the goal was completed.  So, I started with an easy one, reading a book.  I know when I have completed that goal when my current page is equal to the total number of pages in the book.  Sorry, I just jumped into some logic thinking that a computer program could use to determine if it was completed.  So in the case of tracking the progress for my book reading goals I could keep track of what page I was on each day and how long I spent reading.  The last one is going to help in figuring out how fast I am reading this book and checking this against how much time I have set aside to work on my goals. \n\nOkay, then from that information I could recalculate my goal target date by calculating the rate at which I am going what I should actually reach my goal.  If the new target date is earlier then I had planned then the KPI should show me a green light.  If it is later then this, it should show me a yellow (warning) light if I am just slipping but I still have time in my allocated time frame to meet this goal.  Of course the KPI would be a red light if there was no way that I could meet this goal.  This one is harder to determine as it is an indicator which would come up when I certainly have gone past the target date, how I can determine if I have run out of time before this date is hard to calculate especially if I have alot of goals.  There are things that I cannot really know like sacrificing one goal so that I can put all my effort toward the other goal.  If you are behind I will show the warning light, if we missed the goal I will show the red light...but at least I have something that I can track for my goals. \n\nThere were a couple of other types of goals that I thought of tracking.  My projects that I build are not based on any page number but I thought I would set a goal in the amount of time I would spend on the goal by a certain target date and track it that way.  This also should work quite well and can easily see when I am on and off track but the red can again only be shown if I have already missed the mark.  Then just to throw something different into the goal tracking mix, I thought about setting up some goals for my weight.  This one is really different in that there is no time element here at all.  In stead we are tracking the weight on a regular basis and let the goal tracker estimate and the rate that I am loosing or gaining weight when I should be able to reach my ideal weight.  I think that the KPI's are going to start showing me problem indicators when I am moving in the opposite direction that I was planning.  If this is going to work or not I am not sure, for instance for the past week I have had no change in any direction and the goal tracker is still saying I will reach my ideal weight within the date I have targeted....time will tell. \n{% asset_img image.png \"KPI of a few goals\" %}\nAnyway, as you can probably tell by now I have actually started to put together a goal tracking program.  It is still rough and most certainly is a beta product.  \n\nGood luck with your goals, I am finding that I am a lot more focused on my goals and staying on track then when I wasn't tracking my goals, so I think it is working. \n","source":"_posts/Goal-Tracking.md","raw":"title: Goal Tracking\ndate: 2009-02-09\ntags:\n- Goal Tracker\n- Products\n---\nSince about the beginning of the year I have been thinking about goal tracking.  I compiled a long list of technologies that I wanted to learn, experiment with and maybe even build some projects using some of these newly learned skills.  Nothing quite like turning something new into something useful.  I find that this technique provides me with the best understanding of how and why a technology would be used in one scenario over another.  My goals for this year is a long list and some have a dependency of a previous goal being completed before I even begin, like reading the book before I begin my project based on the technology. \n\nHowever, I suffer from the getting bored and just needing a break from a certain goal and then forget to get back to it at the appropriate time illness.  It's like I need something to help me track what my goals are and an easy to see a KPI like indicator to show me which goals I need to pay attention to right now or I might miss my target date altogether.  Before I go much farther I should define KPI:  \n\n\n \nKPI's are Key Performance Indicators which help organizations achieve organizational goals through the definition and measurement of progress. The key indicators are agreed upon by an organization and are indicators which can be measured that will reflect success factors. The KPIs selected must reflect the organization's goals, they must be key to its success, and they must be measurable. Key performance indicators usually are long-term considerations for an organization. \nThis is what I need for my goals, some way to track my progress.  I went to work on it, storing the goals was easy.  Give it a name, what your target date is for completing the goal and some exit criteria.  Okay, so I had to think a little bit about that last one, but I needed something that would tell me when the goal was completed.  So, I started with an easy one, reading a book.  I know when I have completed that goal when my current page is equal to the total number of pages in the book.  Sorry, I just jumped into some logic thinking that a computer program could use to determine if it was completed.  So in the case of tracking the progress for my book reading goals I could keep track of what page I was on each day and how long I spent reading.  The last one is going to help in figuring out how fast I am reading this book and checking this against how much time I have set aside to work on my goals. \n\nOkay, then from that information I could recalculate my goal target date by calculating the rate at which I am going what I should actually reach my goal.  If the new target date is earlier then I had planned then the KPI should show me a green light.  If it is later then this, it should show me a yellow (warning) light if I am just slipping but I still have time in my allocated time frame to meet this goal.  Of course the KPI would be a red light if there was no way that I could meet this goal.  This one is harder to determine as it is an indicator which would come up when I certainly have gone past the target date, how I can determine if I have run out of time before this date is hard to calculate especially if I have alot of goals.  There are things that I cannot really know like sacrificing one goal so that I can put all my effort toward the other goal.  If you are behind I will show the warning light, if we missed the goal I will show the red light...but at least I have something that I can track for my goals. \n\nThere were a couple of other types of goals that I thought of tracking.  My projects that I build are not based on any page number but I thought I would set a goal in the amount of time I would spend on the goal by a certain target date and track it that way.  This also should work quite well and can easily see when I am on and off track but the red can again only be shown if I have already missed the mark.  Then just to throw something different into the goal tracking mix, I thought about setting up some goals for my weight.  This one is really different in that there is no time element here at all.  In stead we are tracking the weight on a regular basis and let the goal tracker estimate and the rate that I am loosing or gaining weight when I should be able to reach my ideal weight.  I think that the KPI's are going to start showing me problem indicators when I am moving in the opposite direction that I was planning.  If this is going to work or not I am not sure, for instance for the past week I have had no change in any direction and the goal tracker is still saying I will reach my ideal weight within the date I have targeted....time will tell. \n{% asset_img image.png \"KPI of a few goals\" %}\nAnyway, as you can probably tell by now I have actually started to put together a goal tracking program.  It is still rough and most certainly is a beta product.  \n\nGood luck with your goals, I am finding that I am a lot more focused on my goals and staying on track then when I wasn't tracking my goals, so I think it is working. \n","slug":"Goal-Tracking","published":1,"updated":"2020-01-05T00:36:05.009Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgff0007s4uf4oljd5i7","content":"<p>Since about the beginning of the year I have been thinking about goal tracking.  I compiled a long list of technologies that I wanted to learn, experiment with and maybe even build some projects using some of these newly learned skills.  Nothing quite like turning something new into something useful.  I find that this technique provides me with the best understanding of how and why a technology would be used in one scenario over another.  My goals for this year is a long list and some have a dependency of a previous goal being completed before I even begin, like reading the book before I begin my project based on the technology. </p>\n<p>However, I suffer from the getting bored and just needing a break from a certain goal and then forget to get back to it at the appropriate time illness.  It’s like I need something to help me track what my goals are and an easy to see a KPI like indicator to show me which goals I need to pay attention to right now or I might miss my target date altogether.  Before I go much farther I should define KPI:  </p>\n<p>KPI’s are Key Performance Indicators which help organizations achieve organizational goals through the definition and measurement of progress. The key indicators are agreed upon by an organization and are indicators which can be measured that will reflect success factors. The KPIs selected must reflect the organization’s goals, they must be key to its success, and they must be measurable. Key performance indicators usually are long-term considerations for an organization.<br>This is what I need for my goals, some way to track my progress.  I went to work on it, storing the goals was easy.  Give it a name, what your target date is for completing the goal and some exit criteria.  Okay, so I had to think a little bit about that last one, but I needed something that would tell me when the goal was completed.  So, I started with an easy one, reading a book.  I know when I have completed that goal when my current page is equal to the total number of pages in the book.  Sorry, I just jumped into some logic thinking that a computer program could use to determine if it was completed.  So in the case of tracking the progress for my book reading goals I could keep track of what page I was on each day and how long I spent reading.  The last one is going to help in figuring out how fast I am reading this book and checking this against how much time I have set aside to work on my goals. </p>\n<p>Okay, then from that information I could recalculate my goal target date by calculating the rate at which I am going what I should actually reach my goal.  If the new target date is earlier then I had planned then the KPI should show me a green light.  If it is later then this, it should show me a yellow (warning) light if I am just slipping but I still have time in my allocated time frame to meet this goal.  Of course the KPI would be a red light if there was no way that I could meet this goal.  This one is harder to determine as it is an indicator which would come up when I certainly have gone past the target date, how I can determine if I have run out of time before this date is hard to calculate especially if I have alot of goals.  There are things that I cannot really know like sacrificing one goal so that I can put all my effort toward the other goal.  If you are behind I will show the warning light, if we missed the goal I will show the red light…but at least I have something that I can track for my goals. </p>\n<p>There were a couple of other types of goals that I thought of tracking.  My projects that I build are not based on any page number but I thought I would set a goal in the amount of time I would spend on the goal by a certain target date and track it that way.  This also should work quite well and can easily see when I am on and off track but the red can again only be shown if I have already missed the mark.  Then just to throw something different into the goal tracking mix, I thought about setting up some goals for my weight.  This one is really different in that there is no time element here at all.  In stead we are tracking the weight on a regular basis and let the goal tracker estimate and the rate that I am loosing or gaining weight when I should be able to reach my ideal weight.  I think that the KPI’s are going to start showing me problem indicators when I am moving in the opposite direction that I was planning.  If this is going to work or not I am not sure, for instance for the past week I have had no change in any direction and the goal tracker is still saying I will reach my ideal weight within the date I have targeted….time will tell.<br><img src=\"/2009/02/Goal-Tracking/image.png\" title=\"KPI of a few goals\"><br>Anyway, as you can probably tell by now I have actually started to put together a goal tracking program.  It is still rough and most certainly is a beta product.  </p>\n<p>Good luck with your goals, I am finding that I am a lot more focused on my goals and staying on track then when I wasn’t tracking my goals, so I think it is working. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Since about the beginning of the year I have been thinking about goal tracking.  I compiled a long list of technologies that I wanted to learn, experiment with and maybe even build some projects using some of these newly learned skills.  Nothing quite like turning something new into something useful.  I find that this technique provides me with the best understanding of how and why a technology would be used in one scenario over another.  My goals for this year is a long list and some have a dependency of a previous goal being completed before I even begin, like reading the book before I begin my project based on the technology. </p>\n<p>However, I suffer from the getting bored and just needing a break from a certain goal and then forget to get back to it at the appropriate time illness.  It’s like I need something to help me track what my goals are and an easy to see a KPI like indicator to show me which goals I need to pay attention to right now or I might miss my target date altogether.  Before I go much farther I should define KPI:  </p>\n<p>KPI’s are Key Performance Indicators which help organizations achieve organizational goals through the definition and measurement of progress. The key indicators are agreed upon by an organization and are indicators which can be measured that will reflect success factors. The KPIs selected must reflect the organization’s goals, they must be key to its success, and they must be measurable. Key performance indicators usually are long-term considerations for an organization.<br>This is what I need for my goals, some way to track my progress.  I went to work on it, storing the goals was easy.  Give it a name, what your target date is for completing the goal and some exit criteria.  Okay, so I had to think a little bit about that last one, but I needed something that would tell me when the goal was completed.  So, I started with an easy one, reading a book.  I know when I have completed that goal when my current page is equal to the total number of pages in the book.  Sorry, I just jumped into some logic thinking that a computer program could use to determine if it was completed.  So in the case of tracking the progress for my book reading goals I could keep track of what page I was on each day and how long I spent reading.  The last one is going to help in figuring out how fast I am reading this book and checking this against how much time I have set aside to work on my goals. </p>\n<p>Okay, then from that information I could recalculate my goal target date by calculating the rate at which I am going what I should actually reach my goal.  If the new target date is earlier then I had planned then the KPI should show me a green light.  If it is later then this, it should show me a yellow (warning) light if I am just slipping but I still have time in my allocated time frame to meet this goal.  Of course the KPI would be a red light if there was no way that I could meet this goal.  This one is harder to determine as it is an indicator which would come up when I certainly have gone past the target date, how I can determine if I have run out of time before this date is hard to calculate especially if I have alot of goals.  There are things that I cannot really know like sacrificing one goal so that I can put all my effort toward the other goal.  If you are behind I will show the warning light, if we missed the goal I will show the red light…but at least I have something that I can track for my goals. </p>\n<p>There were a couple of other types of goals that I thought of tracking.  My projects that I build are not based on any page number but I thought I would set a goal in the amount of time I would spend on the goal by a certain target date and track it that way.  This also should work quite well and can easily see when I am on and off track but the red can again only be shown if I have already missed the mark.  Then just to throw something different into the goal tracking mix, I thought about setting up some goals for my weight.  This one is really different in that there is no time element here at all.  In stead we are tracking the weight on a regular basis and let the goal tracker estimate and the rate that I am loosing or gaining weight when I should be able to reach my ideal weight.  I think that the KPI’s are going to start showing me problem indicators when I am moving in the opposite direction that I was planning.  If this is going to work or not I am not sure, for instance for the past week I have had no change in any direction and the goal tracker is still saying I will reach my ideal weight within the date I have targeted….time will tell.<br><img src=\"/2009/02/Goal-Tracking/image.png\" title=\"KPI of a few goals\"><br>Anyway, as you can probably tell by now I have actually started to put together a goal tracking program.  It is still rough and most certainly is a beta product.  </p>\n<p>Good luck with your goals, I am finding that I am a lot more focused on my goals and staying on track then when I wasn’t tracking my goals, so I think it is working. </p>\n"},{"title":"How I Use Chocolatey in my Releases","date":"2016-06-11T05:10:44.000Z","_content":"{% img right /images/Chocolatey.jpg 100 100 \"Chocolatey.org\" %}\nI have been using Chocolatey for a while as an ultra easy way to install software.  It has become the prefered way to install tools and utilities from the open source community.  Recently I have started to explore this technology in more depth just to learn more about Chocolatey and found some really great uses for it that I did not expect to find.  This post is about that adventure and how and what I use Chocolatey for.\n\n## Built on NuGet\nFirst off I guess we should talk about what Chocolatey is.  It is another packaged technology based on NuGet.  In fact it is NuGet with some more features and elements added to it.  If you have been around me over the last couple of years, I have declared that NuGet is probably one of the greatest advancements that we have had in the dot net community in the last 10 years.  Initially introduced back in 2010, it was a package tool to help resolve the dependencies in open source software.  Even back then I could see that this technology had legs and indeed it did as it has proven to resolve so many hard development problems that we have worked on for years to resolve.  That being able to have shared code within multiple projects that does not interfere with the development of the underlying projects that depend on them.  I will delve into this subject in a later post as right now I want to focus on Chocolatey.\n\nWhile NuGet was really about installing and resolving dependencies at the source code level as in a new Visual Studio project, Chocolatey takes that same package structure and focuses on the Operating System.  In other words I can create NuGet like packages (they have the very same extension as NuGet *.nupkg) that I can install, uninstall or upgrade in Windows.  I have a couple of utility like programs that run on the desktop that I use to support my applications.  These utilities are never distributed or a part of my application that I distribute through click-once but I need up to date version of these on my test lab machines.  It has always been a problem with having some way to get these installed and up to date on these machines.  However, with the use of Chocolatey this is now an easy solution and a problem that I no longer have.\n\n## Install Chocolatey\nLet's start with how we would go about installing Chocolatey.  If you go to the [Chocolatey.org web-site](http://chocolatey.org) there are about 3 ways listed to download and install the package all of them using PowerShell.\nThis first one assumes nothing, as it will Bypass the ExecutionPolicy and has the best chance of installing on your system.\n```\n@powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\n```\nThis next one is going to assume that you are an administrator on your machine and you have set the Execution Policy to at least RemoteSigned\n```\niex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\n```\nThen this last script is going to assume that you are an administrator, have the Execution Policy set to at least RemoteSigned and have PowerShell v3 or higher.\n```\niwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex\n```\nNot sure what version of PowerShell you have?  Well the easiest way to tell is to bring up the PowerShell console (you will want to run with Administrator elevated rights) and enter the following:\n```\n$PSVersionTable\n```\n## Making my own Package\nOkay so I have Chocolatey installed and I have a product that I want to install so how do I get this package created?  Good question so lets tackle that next.  I start by using file explorer, go to your project and create a new folder.  In my case I was working with a utility program that I called AgpAdmin so at the sibling level of that project I made a folder called AgpAdminInstall and this is where I am going to build my package.\n\n{% asset_img FileStructure.png \"The file structure\"%}\nNow I would bring up PowerShell running as an administrator and navigate over to that new folder that I just created and enter the following Chocolatey command.\n```\nChoco New AGPAdmin\n```\nThis will create the nuspec file with the same name that I entered in that New command as well as a tools folder which will contain two powershell scripts.  There are a couple of ways that you can build this package as the final bits don't even need to be in this package. They could be referenced in other locations where they can be downloaded and installed. There is a lot of documentation and examples that you can find to do that.  I would say that most of the Chocolatey packages that can be found on Chocolatey.org are done this way.  I found that they mention that the assemblies could be embedded but I never found an example and that was the way that I wanted to package this so that is the guidance I am going to show you here. \n\nLets start with the nuspec file.  This is the file that contains the meta data and where all the pieces can be found.  If you are familiar with creating a typical NuGet spec this should all look pretty familiar but there are a couple of things that you must be aware of.  In the Chocolatey version of this spec file you must have a **projectUrl** (in my case I was pointing to my VSTS implementation dashboard page.  You must have a **packageSourceUrl** (in my case I pointed to my source url to my git repository) and a **licenseUrl** which needs to point to a page that describes your license.  I never needed these when building a NuGet package but are required in order to get the Chocolatey package built.  One more thing we need for the nuspec file to be complete is the files section where we tell it what files need to be included in the package.\n\nThere will be one entry there already which is to include all the items found in the folder tools and to place it within the nuget package structure of tools.  We want to add one more file entry where we add a relative path from where we are to include the setup file that is being constructured up one folder and then then down 3 folders through the AGPAdminSetup tree and the target also being within the nuget package structure of tools.  This line is what embeds my setup program into the Chocolatey package.\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!-- Do not remove this test for UTF-8: if “Ω” doesn’t appear as greek uppercase omega letter enclosed in quotation marks, you should use an editor that supports UTF-8, not this one. -->\n<package xmlns=\"http://schemas.microsoft.com/packaging/2015/06/nuspec.xsd\">\n  <metadata>\n    <!-- Read this before publishing packages to chocolatey.org: https://github.com/chocolatey/chocolatey/wiki/CreatePackages -->\n    <id>agpadmin</id>\n    <title>AGPAdmin (Install)</title>\n    <version>2.2.0.2</version>\n    <authors>Donald L. Schulz</authors>\n    <owners>The Web We Weave, Inc.</owners>\n    <summary>Admin tool to help support AGP-Maker</summary>\n    <description>Setup and Install of the AGP-Admin program</description>\n    <projectUrl>https://donald.visualstudio.com/3WInc/AGP-Admin/_dashboards</projectUrl>\n    <packageSourceUrl>https://donald.visualstudio.com/DefaultCollection/3WInc/_git/AGPMaker-Admin</packageSourceUrl>\n    <tags>agpadmin admin</tags>\n    <copyright>2016 The Web We Weave, Inc.</copyright>\n    <licenseUrl>http://www.agpmaker.com/AGPMaker.Install/index.htm</licenseUrl>\n    <requireLicenseAcceptance>false</requireLicenseAcceptance>\n  </metadata>\n  <files>\n    <file src=\"..\\AGPAdminSetup\\bin\\Release\\AGPAdminSetup.exe\" target=\"tools\" />\n    <file src=\"tools\\**\" target=\"tools\" />\n  </files>\n</package>\n```\n\nBefore we move on to the automated steps that we want to implement so that we don't even have to think about building this package every time, we will need to make a couple of changes to the PowerShell scripts that are found in the tools folder.  When you open this powershell script it is well commented and the variable names used are pretty clear in describing what they are for.  You will notice that it seems to be ready out of the box to get you to provide a url where it can get your program to install.  I want to use the embedded solution so un-comment the first $fileLocation line and replace the 'NAME_OF_EMBEDDED_INSTALLER_FILE' with the name of the file you want to run and I will also assume that you have it in this same tools folder (in the compiled nupkg file).  In my package I did create an install program using [the wix toolset](http://wixtoolset.org/) which also gives it the capability to uninstall itself automatically.  Next I commented out the default silentArgs and the validExitCodes found right under the #MSI comment.  There is a long string of commented lines that all start with #silentArgs and what I did was un-comment the last one and set the value as '/quiet' and un-comment the validExistCodes line right below that so the line looks like this:\n```\nsilentArgs = '/quiet'\nvalidExitCodes= @(0)\n```\nThat is really all that there is to it.  The rest of this script file should just work.  There are a number of different cmdlet's that you can call and they are all shown in the InstallChocoletey.ps1 file that appeared when you called the Choco new command and they are all commented fairly well.  I was creating the Chocolatey wrapper around an Install program so I chose the cmdlet \"Install-ChocolateyInstallPackage\".  So to summarize the PowerShell Script ignoring the commented lines the finished PowerShell script looks a lot like this:\n```\n$ErrorActionPreference = 'Stop';\n\n$packageName= 'MyAdminProg' # arbitrary name for the package, used in messages\n$toolsDir   = \"$(Split-Path -parent $MyInvocation.MyCommand.Definition)\"\n$fileLocation = Join-Path $toolsDir 'MyAdminProgSetup.exe'\n\n$packageArgs = @{\n  packageName   = $packageName\n  unzipLocation = $toolsDir\n  fileType      = 'EXE' #only one of these: exe, msi, msu\n  url           = $url\n  url64bit      = $url64\n  file          = $fileLocation\n  silentArgs    = '/quiet'\n  softwareName  = 'MyAdminProg*' #part or all of the Display Name as you see it in Programs and Features. It should be enough to be unique\n  checksum      = ''\n  checksumType  = 'md5' #default is md5, can also be sha1\n  checksum64    = ''\n  checksumType64= 'md5' #default is checksumType\n}\n\nInstall-ChocolateyInstallPackage @packageArgs  \n```\nOne thing that we did not cover in all this is the fileType value.  This is going to be an exe, msi or msu depending on how you created your setup file.  I took the extra step in my wix install program to create a bootstrap which takes the initial msi and checks the prerequists such as the correct version of the dot net framework and turns that into an exe.  You will need to set this to the value of your install program what you want to run.\n\nAnother advantage to using an install package is that it knows how to uninstall itself.  That means I did not need that other PowerShell script that was in the tools directory which was the chocolateyuninstall.ps1 file.  I deleted mine so that it would use the automatic uninstaller that is managed and controlled by windows (msi).  If this file exists in your package than Chocolatey is going to run that script and if you have not set this up properly will give you issues when you run the Choco uninstall command for the package.\n\n## Automating the Build in TFS 2015\nWe want to make sure that we place all these two files folders and the nuspec file into source control.  Besides having this is a place where we can repeat this process and keep track of any changes that might happen between changes we will be able to automate the entire operation.  Our goal here is to make a change which when we check in the code change of the actual utility program will kick off a build create the package and publish it to our private Chocolatey feed.\n\nTo automate the building of the chocolatey package I started with a Build Definition that I already had that was building all these pieces.  It built the program, then created an AGPAdminPackage.msi file and then turned that into a bootstrapper and gave me the AGPAdminSetup.exe file.  Our nuspec file has indicated where to find the finished AGPAdminSetup.exe file so that it will be embedded into the finished .nupkg file.  Just after the steps that compile the code, run the tests, I add a PowerShell script and switch it to run inline and write the following script:\n```\n# You can write your powershell scripts inline here. \n# You can also pass predefined and custom variables to this scripts using arguments\n\ncpack\n```\nThis command will find the .nuspec file and create the .nupkg in the same folder as the nuspec file.  From there the things that I do are to copy the pieces I am interested in having in the drop and place them into the staging work space $(Agent.BuildDirectory)\\b and then for the Copy Publish Artifacts I just push everything I have in staging.\n\n## Private Feed\n\nBecause Chocolatey is based on Nuget technology it works on exactly the same principal of distribution which is a feed but it could also be a network file share.  I have chosen the private feed as I need this to be a feed that I can access from home, the cloud, and when I am on the road.  Okay so you might be in the same or similar situation as myself so how do you setup a Chocolatey Server?  With Chocolatey of course.\n```\nchoco install chocolatey.server -y\n```\nOn the machine that you run this command on, it will create a chocolatey.server folder inside of a folder off of the root drive called tools.  Just point IIS to this folder and you have a Chocolatey feed ready for your packages.  The packages actually go into the App_Data\\packages folder that you will find in this ready to go Chocolatey.server.  However I will make another assumption that this server may not be right next to you but on a different machine or even the cloud so you will want to publish your packages.  To do that you will need to make sure that you give the app pool modify permissions to the App_Data folder.  This in the build definition after the Copy Publish Artifact add another PowerShell script to run inline and this time call the following command:\n```\n# You can write your powershell scripts inline here. \n# You can also pass predefined and custom variables to this scripts using arguments\n\nchoco push --source=\"https://<your server name here>/choco/\" --api-key=\"<your api key here>\" --force\n```\nThat is it really you have a package in a feed that can be installed and upgraded with just a simple Chocolatey command.\n\n## Make it even Better\nI went one step farther to make this even easier and that was to modify the chocolatey configuration file so that it looks in my private repository first before looking at the public one that is set up by default.  This way I can install and upgrade my private packages just as if they were published and exposed to the whole world but it is not.  You find the chocolatey.config file in the C:\\ProgramData\\Chocolatey\\config folder.  When you open the file you will see an area called sources and probably one source listed.  Just add an additional source file give it an id (I called my Choco) and the value should be where your chocolatey feed can be found and set the priority to 1. That is it but you need to do this to all the machines that are going to be getting your program and all the latest updates.  Now when ever you are doing a build to about to run tests on a virtual machine you can call have a simple powershell script do it for you.\n```\nchoco install agpadmin -y\nWrite-Output \"AGPAdmin Installed\"\n\nchoco upgrade agpadmin -y\nWrite-Output \"AGPAdmin Upgraded\"\n\nStart-Sleep 120\n```\nThe program I am installing is called agpadmin and I pass the -y so that it skips the confirm as this is almost always part of a build.  I call both the install and then the upgrade as it does not seem to do both but it just ignores the install if it is already installed and will then do the upgrade if there is a newer version out there.\n\nHope you enjoy Chocolatey as much as I do.","source":"_posts/How-I-Use-Chocolatey-in-my-Releases.md","raw":"title: How I Use Chocolatey in my Releases\ndate: 2016-06-10 22:10:44\ntags:\n- ALM\n- DevOps\n- NuGet\n- PowerShell\n---\n{% img right /images/Chocolatey.jpg 100 100 \"Chocolatey.org\" %}\nI have been using Chocolatey for a while as an ultra easy way to install software.  It has become the prefered way to install tools and utilities from the open source community.  Recently I have started to explore this technology in more depth just to learn more about Chocolatey and found some really great uses for it that I did not expect to find.  This post is about that adventure and how and what I use Chocolatey for.\n\n## Built on NuGet\nFirst off I guess we should talk about what Chocolatey is.  It is another packaged technology based on NuGet.  In fact it is NuGet with some more features and elements added to it.  If you have been around me over the last couple of years, I have declared that NuGet is probably one of the greatest advancements that we have had in the dot net community in the last 10 years.  Initially introduced back in 2010, it was a package tool to help resolve the dependencies in open source software.  Even back then I could see that this technology had legs and indeed it did as it has proven to resolve so many hard development problems that we have worked on for years to resolve.  That being able to have shared code within multiple projects that does not interfere with the development of the underlying projects that depend on them.  I will delve into this subject in a later post as right now I want to focus on Chocolatey.\n\nWhile NuGet was really about installing and resolving dependencies at the source code level as in a new Visual Studio project, Chocolatey takes that same package structure and focuses on the Operating System.  In other words I can create NuGet like packages (they have the very same extension as NuGet *.nupkg) that I can install, uninstall or upgrade in Windows.  I have a couple of utility like programs that run on the desktop that I use to support my applications.  These utilities are never distributed or a part of my application that I distribute through click-once but I need up to date version of these on my test lab machines.  It has always been a problem with having some way to get these installed and up to date on these machines.  However, with the use of Chocolatey this is now an easy solution and a problem that I no longer have.\n\n## Install Chocolatey\nLet's start with how we would go about installing Chocolatey.  If you go to the [Chocolatey.org web-site](http://chocolatey.org) there are about 3 ways listed to download and install the package all of them using PowerShell.\nThis first one assumes nothing, as it will Bypass the ExecutionPolicy and has the best chance of installing on your system.\n```\n@powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\n```\nThis next one is going to assume that you are an administrator on your machine and you have set the Execution Policy to at least RemoteSigned\n```\niex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\n```\nThen this last script is going to assume that you are an administrator, have the Execution Policy set to at least RemoteSigned and have PowerShell v3 or higher.\n```\niwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex\n```\nNot sure what version of PowerShell you have?  Well the easiest way to tell is to bring up the PowerShell console (you will want to run with Administrator elevated rights) and enter the following:\n```\n$PSVersionTable\n```\n## Making my own Package\nOkay so I have Chocolatey installed and I have a product that I want to install so how do I get this package created?  Good question so lets tackle that next.  I start by using file explorer, go to your project and create a new folder.  In my case I was working with a utility program that I called AgpAdmin so at the sibling level of that project I made a folder called AgpAdminInstall and this is where I am going to build my package.\n\n{% asset_img FileStructure.png \"The file structure\"%}\nNow I would bring up PowerShell running as an administrator and navigate over to that new folder that I just created and enter the following Chocolatey command.\n```\nChoco New AGPAdmin\n```\nThis will create the nuspec file with the same name that I entered in that New command as well as a tools folder which will contain two powershell scripts.  There are a couple of ways that you can build this package as the final bits don't even need to be in this package. They could be referenced in other locations where they can be downloaded and installed. There is a lot of documentation and examples that you can find to do that.  I would say that most of the Chocolatey packages that can be found on Chocolatey.org are done this way.  I found that they mention that the assemblies could be embedded but I never found an example and that was the way that I wanted to package this so that is the guidance I am going to show you here. \n\nLets start with the nuspec file.  This is the file that contains the meta data and where all the pieces can be found.  If you are familiar with creating a typical NuGet spec this should all look pretty familiar but there are a couple of things that you must be aware of.  In the Chocolatey version of this spec file you must have a **projectUrl** (in my case I was pointing to my VSTS implementation dashboard page.  You must have a **packageSourceUrl** (in my case I pointed to my source url to my git repository) and a **licenseUrl** which needs to point to a page that describes your license.  I never needed these when building a NuGet package but are required in order to get the Chocolatey package built.  One more thing we need for the nuspec file to be complete is the files section where we tell it what files need to be included in the package.\n\nThere will be one entry there already which is to include all the items found in the folder tools and to place it within the nuget package structure of tools.  We want to add one more file entry where we add a relative path from where we are to include the setup file that is being constructured up one folder and then then down 3 folders through the AGPAdminSetup tree and the target also being within the nuget package structure of tools.  This line is what embeds my setup program into the Chocolatey package.\n\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!-- Do not remove this test for UTF-8: if “Ω” doesn’t appear as greek uppercase omega letter enclosed in quotation marks, you should use an editor that supports UTF-8, not this one. -->\n<package xmlns=\"http://schemas.microsoft.com/packaging/2015/06/nuspec.xsd\">\n  <metadata>\n    <!-- Read this before publishing packages to chocolatey.org: https://github.com/chocolatey/chocolatey/wiki/CreatePackages -->\n    <id>agpadmin</id>\n    <title>AGPAdmin (Install)</title>\n    <version>2.2.0.2</version>\n    <authors>Donald L. Schulz</authors>\n    <owners>The Web We Weave, Inc.</owners>\n    <summary>Admin tool to help support AGP-Maker</summary>\n    <description>Setup and Install of the AGP-Admin program</description>\n    <projectUrl>https://donald.visualstudio.com/3WInc/AGP-Admin/_dashboards</projectUrl>\n    <packageSourceUrl>https://donald.visualstudio.com/DefaultCollection/3WInc/_git/AGPMaker-Admin</packageSourceUrl>\n    <tags>agpadmin admin</tags>\n    <copyright>2016 The Web We Weave, Inc.</copyright>\n    <licenseUrl>http://www.agpmaker.com/AGPMaker.Install/index.htm</licenseUrl>\n    <requireLicenseAcceptance>false</requireLicenseAcceptance>\n  </metadata>\n  <files>\n    <file src=\"..\\AGPAdminSetup\\bin\\Release\\AGPAdminSetup.exe\" target=\"tools\" />\n    <file src=\"tools\\**\" target=\"tools\" />\n  </files>\n</package>\n```\n\nBefore we move on to the automated steps that we want to implement so that we don't even have to think about building this package every time, we will need to make a couple of changes to the PowerShell scripts that are found in the tools folder.  When you open this powershell script it is well commented and the variable names used are pretty clear in describing what they are for.  You will notice that it seems to be ready out of the box to get you to provide a url where it can get your program to install.  I want to use the embedded solution so un-comment the first $fileLocation line and replace the 'NAME_OF_EMBEDDED_INSTALLER_FILE' with the name of the file you want to run and I will also assume that you have it in this same tools folder (in the compiled nupkg file).  In my package I did create an install program using [the wix toolset](http://wixtoolset.org/) which also gives it the capability to uninstall itself automatically.  Next I commented out the default silentArgs and the validExitCodes found right under the #MSI comment.  There is a long string of commented lines that all start with #silentArgs and what I did was un-comment the last one and set the value as '/quiet' and un-comment the validExistCodes line right below that so the line looks like this:\n```\nsilentArgs = '/quiet'\nvalidExitCodes= @(0)\n```\nThat is really all that there is to it.  The rest of this script file should just work.  There are a number of different cmdlet's that you can call and they are all shown in the InstallChocoletey.ps1 file that appeared when you called the Choco new command and they are all commented fairly well.  I was creating the Chocolatey wrapper around an Install program so I chose the cmdlet \"Install-ChocolateyInstallPackage\".  So to summarize the PowerShell Script ignoring the commented lines the finished PowerShell script looks a lot like this:\n```\n$ErrorActionPreference = 'Stop';\n\n$packageName= 'MyAdminProg' # arbitrary name for the package, used in messages\n$toolsDir   = \"$(Split-Path -parent $MyInvocation.MyCommand.Definition)\"\n$fileLocation = Join-Path $toolsDir 'MyAdminProgSetup.exe'\n\n$packageArgs = @{\n  packageName   = $packageName\n  unzipLocation = $toolsDir\n  fileType      = 'EXE' #only one of these: exe, msi, msu\n  url           = $url\n  url64bit      = $url64\n  file          = $fileLocation\n  silentArgs    = '/quiet'\n  softwareName  = 'MyAdminProg*' #part or all of the Display Name as you see it in Programs and Features. It should be enough to be unique\n  checksum      = ''\n  checksumType  = 'md5' #default is md5, can also be sha1\n  checksum64    = ''\n  checksumType64= 'md5' #default is checksumType\n}\n\nInstall-ChocolateyInstallPackage @packageArgs  \n```\nOne thing that we did not cover in all this is the fileType value.  This is going to be an exe, msi or msu depending on how you created your setup file.  I took the extra step in my wix install program to create a bootstrap which takes the initial msi and checks the prerequists such as the correct version of the dot net framework and turns that into an exe.  You will need to set this to the value of your install program what you want to run.\n\nAnother advantage to using an install package is that it knows how to uninstall itself.  That means I did not need that other PowerShell script that was in the tools directory which was the chocolateyuninstall.ps1 file.  I deleted mine so that it would use the automatic uninstaller that is managed and controlled by windows (msi).  If this file exists in your package than Chocolatey is going to run that script and if you have not set this up properly will give you issues when you run the Choco uninstall command for the package.\n\n## Automating the Build in TFS 2015\nWe want to make sure that we place all these two files folders and the nuspec file into source control.  Besides having this is a place where we can repeat this process and keep track of any changes that might happen between changes we will be able to automate the entire operation.  Our goal here is to make a change which when we check in the code change of the actual utility program will kick off a build create the package and publish it to our private Chocolatey feed.\n\nTo automate the building of the chocolatey package I started with a Build Definition that I already had that was building all these pieces.  It built the program, then created an AGPAdminPackage.msi file and then turned that into a bootstrapper and gave me the AGPAdminSetup.exe file.  Our nuspec file has indicated where to find the finished AGPAdminSetup.exe file so that it will be embedded into the finished .nupkg file.  Just after the steps that compile the code, run the tests, I add a PowerShell script and switch it to run inline and write the following script:\n```\n# You can write your powershell scripts inline here. \n# You can also pass predefined and custom variables to this scripts using arguments\n\ncpack\n```\nThis command will find the .nuspec file and create the .nupkg in the same folder as the nuspec file.  From there the things that I do are to copy the pieces I am interested in having in the drop and place them into the staging work space $(Agent.BuildDirectory)\\b and then for the Copy Publish Artifacts I just push everything I have in staging.\n\n## Private Feed\n\nBecause Chocolatey is based on Nuget technology it works on exactly the same principal of distribution which is a feed but it could also be a network file share.  I have chosen the private feed as I need this to be a feed that I can access from home, the cloud, and when I am on the road.  Okay so you might be in the same or similar situation as myself so how do you setup a Chocolatey Server?  With Chocolatey of course.\n```\nchoco install chocolatey.server -y\n```\nOn the machine that you run this command on, it will create a chocolatey.server folder inside of a folder off of the root drive called tools.  Just point IIS to this folder and you have a Chocolatey feed ready for your packages.  The packages actually go into the App_Data\\packages folder that you will find in this ready to go Chocolatey.server.  However I will make another assumption that this server may not be right next to you but on a different machine or even the cloud so you will want to publish your packages.  To do that you will need to make sure that you give the app pool modify permissions to the App_Data folder.  This in the build definition after the Copy Publish Artifact add another PowerShell script to run inline and this time call the following command:\n```\n# You can write your powershell scripts inline here. \n# You can also pass predefined and custom variables to this scripts using arguments\n\nchoco push --source=\"https://<your server name here>/choco/\" --api-key=\"<your api key here>\" --force\n```\nThat is it really you have a package in a feed that can be installed and upgraded with just a simple Chocolatey command.\n\n## Make it even Better\nI went one step farther to make this even easier and that was to modify the chocolatey configuration file so that it looks in my private repository first before looking at the public one that is set up by default.  This way I can install and upgrade my private packages just as if they were published and exposed to the whole world but it is not.  You find the chocolatey.config file in the C:\\ProgramData\\Chocolatey\\config folder.  When you open the file you will see an area called sources and probably one source listed.  Just add an additional source file give it an id (I called my Choco) and the value should be where your chocolatey feed can be found and set the priority to 1. That is it but you need to do this to all the machines that are going to be getting your program and all the latest updates.  Now when ever you are doing a build to about to run tests on a virtual machine you can call have a simple powershell script do it for you.\n```\nchoco install agpadmin -y\nWrite-Output \"AGPAdmin Installed\"\n\nchoco upgrade agpadmin -y\nWrite-Output \"AGPAdmin Upgraded\"\n\nStart-Sleep 120\n```\nThe program I am installing is called agpadmin and I pass the -y so that it skips the confirm as this is almost always part of a build.  I call both the install and then the upgrade as it does not seem to do both but it just ignores the install if it is already installed and will then do the upgrade if there is a newer version out there.\n\nHope you enjoy Chocolatey as much as I do.","slug":"How-I-Use-Chocolatey-in-my-Releases","published":1,"updated":"2020-01-05T00:36:05.012Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfg0008s4ufuywftqh5","content":"<img src=\"/images/Chocolatey.jpg\" class=\"right\" width=\"100\" height=\"100\" title=\"Chocolatey.org\">\n<p>I have been using Chocolatey for a while as an ultra easy way to install software.  It has become the prefered way to install tools and utilities from the open source community.  Recently I have started to explore this technology in more depth just to learn more about Chocolatey and found some really great uses for it that I did not expect to find.  This post is about that adventure and how and what I use Chocolatey for.</p>\n<h2 id=\"Built-on-NuGet\"><a href=\"#Built-on-NuGet\" class=\"headerlink\" title=\"Built on NuGet\"></a>Built on NuGet</h2><p>First off I guess we should talk about what Chocolatey is.  It is another packaged technology based on NuGet.  In fact it is NuGet with some more features and elements added to it.  If you have been around me over the last couple of years, I have declared that NuGet is probably one of the greatest advancements that we have had in the dot net community in the last 10 years.  Initially introduced back in 2010, it was a package tool to help resolve the dependencies in open source software.  Even back then I could see that this technology had legs and indeed it did as it has proven to resolve so many hard development problems that we have worked on for years to resolve.  That being able to have shared code within multiple projects that does not interfere with the development of the underlying projects that depend on them.  I will delve into this subject in a later post as right now I want to focus on Chocolatey.</p>\n<p>While NuGet was really about installing and resolving dependencies at the source code level as in a new Visual Studio project, Chocolatey takes that same package structure and focuses on the Operating System.  In other words I can create NuGet like packages (they have the very same extension as NuGet *.nupkg) that I can install, uninstall or upgrade in Windows.  I have a couple of utility like programs that run on the desktop that I use to support my applications.  These utilities are never distributed or a part of my application that I distribute through click-once but I need up to date version of these on my test lab machines.  It has always been a problem with having some way to get these installed and up to date on these machines.  However, with the use of Chocolatey this is now an easy solution and a problem that I no longer have.</p>\n<h2 id=\"Install-Chocolatey\"><a href=\"#Install-Chocolatey\" class=\"headerlink\" title=\"Install Chocolatey\"></a>Install Chocolatey</h2><p>Let’s start with how we would go about installing Chocolatey.  If you go to the <a href=\"http://chocolatey.org\" target=\"_blank\" rel=\"noopener\">Chocolatey.org web-site</a> there are about 3 ways listed to download and install the package all of them using PowerShell.<br>This first one assumes nothing, as it will Bypass the ExecutionPolicy and has the best chance of installing on your system.<br><figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@powershell -NoProfile -ExecutionPolicy Bypass -Command <span class=\"string\">\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\"</span> &amp;&amp; SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin</span><br></pre></td></tr></table></figure></p>\n<p>This next one is going to assume that you are an administrator on your machine and you have set the Execution Policy to at least RemoteSigned<br><figure class=\"highlight lisp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iex ((<span class=\"name\">new-object</span> net.webclient).DownloadString('https<span class=\"symbol\">://chocolatey</span>.org/install.ps1'))</span><br></pre></td></tr></table></figure></p>\n<p>Then this last script is going to assume that you are an administrator, have the Execution Policy set to at least RemoteSigned and have PowerShell v3 or higher.<br><figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iwr <span class=\"string\">https:</span><span class=\"comment\">//chocolatey.org/install.ps1 -UseBasicParsing | iex</span></span><br></pre></td></tr></table></figure></p>\n<p>Not sure what version of PowerShell you have?  Well the easiest way to tell is to bring up the PowerShell console (you will want to run with Administrator elevated rights) and enter the following:<br><figure class=\"highlight gams\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"><span class=\"meta-keyword\">$PSVersionTable</span></span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Making-my-own-Package\"><a href=\"#Making-my-own-Package\" class=\"headerlink\" title=\"Making my own Package\"></a>Making my own Package</h2><p>Okay so I have Chocolatey installed and I have a product that I want to install so how do I get this package created?  Good question so lets tackle that next.  I start by using file explorer, go to your project and create a new folder.  In my case I was working with a utility program that I called AgpAdmin so at the sibling level of that project I made a folder called AgpAdminInstall and this is where I am going to build my package.</p>\n<img src=\"/2016/06/How-I-Use-Chocolatey-in-my-Releases/FileStructure.png\" title=\"The file structure\">\n<p>Now I would bring up PowerShell running as an administrator and navigate over to that new folder that I just created and enter the following Chocolatey command.<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Choco <span class=\"keyword\">New</span> AGPAdmin</span><br></pre></td></tr></table></figure></p>\n<p>This will create the nuspec file with the same name that I entered in that New command as well as a tools folder which will contain two powershell scripts.  There are a couple of ways that you can build this package as the final bits don’t even need to be in this package. They could be referenced in other locations where they can be downloaded and installed. There is a lot of documentation and examples that you can find to do that.  I would say that most of the Chocolatey packages that can be found on Chocolatey.org are done this way.  I found that they mention that the assemblies could be embedded but I never found an example and that was the way that I wanted to package this so that is the guidance I am going to show you here. </p>\n<p>Lets start with the nuspec file.  This is the file that contains the meta data and where all the pieces can be found.  If you are familiar with creating a typical NuGet spec this should all look pretty familiar but there are a couple of things that you must be aware of.  In the Chocolatey version of this spec file you must have a <strong>projectUrl</strong> (in my case I was pointing to my VSTS implementation dashboard page.  You must have a <strong>packageSourceUrl</strong> (in my case I pointed to my source url to my git repository) and a <strong>licenseUrl</strong> which needs to point to a page that describes your license.  I never needed these when building a NuGet package but are required in order to get the Chocolatey package built.  One more thing we need for the nuspec file to be complete is the files section where we tell it what files need to be included in the package.</p>\n<p>There will be one entry there already which is to include all the items found in the folder tools and to place it within the nuget package structure of tools.  We want to add one more file entry where we add a relative path from where we are to include the setup file that is being constructured up one folder and then then down 3 folders through the AGPAdminSetup tree and the target also being within the nuget package structure of tools.  This line is what embeds my setup program into the Chocolatey package.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Do not remove this test for UTF-8: if “Ω” doesn’t appear as greek uppercase omega letter enclosed in quotation marks, you should use an editor that supports UTF-8, not this one. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">package</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/packaging/2015/06/nuspec.xsd\"</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">metadata</span>&gt;</span></span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- Read this before publishing packages to chocolatey.org: https://github.com/chocolatey/chocolatey/wiki/CreatePackages --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>agpadmin<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>AGPAdmin (Install)<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.2.0.2<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">authors</span>&gt;</span>Donald L. Schulz<span class=\"tag\">&lt;/<span class=\"name\">authors</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">owners</span>&gt;</span>The Web We Weave, Inc.<span class=\"tag\">&lt;/<span class=\"name\">owners</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">summary</span>&gt;</span>Admin tool to help support AGP-Maker<span class=\"tag\">&lt;/<span class=\"name\">summary</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Setup and Install of the AGP-Admin program<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">projectUrl</span>&gt;</span>https://donald.visualstudio.com/3WInc/AGP-Admin/_dashboards<span class=\"tag\">&lt;/<span class=\"name\">projectUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">packageSourceUrl</span>&gt;</span>https://donald.visualstudio.com/DefaultCollection/3WInc/_git/AGPMaker-Admin<span class=\"tag\">&lt;/<span class=\"name\">packageSourceUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">tags</span>&gt;</span>agpadmin admin<span class=\"tag\">&lt;/<span class=\"name\">tags</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">copyright</span>&gt;</span>2016 The Web We Weave, Inc.<span class=\"tag\">&lt;/<span class=\"name\">copyright</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">licenseUrl</span>&gt;</span>http://www.agpmaker.com/AGPMaker.Install/index.htm<span class=\"tag\">&lt;/<span class=\"name\">licenseUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">requireLicenseAcceptance</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">requireLicenseAcceptance</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">metadata</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">files</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">file</span> <span class=\"attr\">src</span>=<span class=\"string\">\"..\\AGPAdminSetup\\bin\\Release\\AGPAdminSetup.exe\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"tools\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">file</span> <span class=\"attr\">src</span>=<span class=\"string\">\"tools\\**\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"tools\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">files</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">package</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Before we move on to the automated steps that we want to implement so that we don’t even have to think about building this package every time, we will need to make a couple of changes to the PowerShell scripts that are found in the tools folder.  When you open this powershell script it is well commented and the variable names used are pretty clear in describing what they are for.  You will notice that it seems to be ready out of the box to get you to provide a url where it can get your program to install.  I want to use the embedded solution so un-comment the first $fileLocation line and replace the ‘NAME_OF_EMBEDDED_INSTALLER_FILE’ with the name of the file you want to run and I will also assume that you have it in this same tools folder (in the compiled nupkg file).  In my package I did create an install program using <a href=\"http://wixtoolset.org/\" target=\"_blank\" rel=\"noopener\">the wix toolset</a> which also gives it the capability to uninstall itself automatically.  Next I commented out the default silentArgs and the validExitCodes found right under the #MSI comment.  There is a long string of commented lines that all start with #silentArgs and what I did was un-comment the last one and set the value as ‘/quiet’ and un-comment the validExistCodes line right below that so the line looks like this:<br><figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">silentArgs</span> = <span class=\"string\">'/quiet'</span></span><br><span class=\"line\"><span class=\"attr\">validExitCodes</span>= @(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p>That is really all that there is to it.  The rest of this script file should just work.  There are a number of different cmdlet’s that you can call and they are all shown in the InstallChocoletey.ps1 file that appeared when you called the Choco new command and they are all commented fairly well.  I was creating the Chocolatey wrapper around an Install program so I chose the cmdlet “Install-ChocolateyInstallPackage”.  So to summarize the PowerShell Script ignoring the commented lines the finished PowerShell script looks a lot like this:<br><figure class=\"highlight perl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ErrorActionPreference = <span class=\"string\">'Stop'</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">$packageName= <span class=\"string\">'MyAdminProg'</span> <span class=\"comment\"># arbitrary name for the package, used in messages</span></span><br><span class=\"line\">$toolsDir   = <span class=\"string\">\"$(Split-Path -parent $MyInvocation.MyCommand.Definition)\"</span></span><br><span class=\"line\">$fileLocation = Join-Path $toolsDir <span class=\"string\">'MyAdminProgSetup.exe'</span></span><br><span class=\"line\"></span><br><span class=\"line\">$packageArgs = @&#123;</span><br><span class=\"line\">  packageName   = $packageName</span><br><span class=\"line\">  unzipLocation = $toolsDir</span><br><span class=\"line\">  fileType      = <span class=\"string\">'EXE'</span> <span class=\"comment\">#only one of these: exe, msi, msu</span></span><br><span class=\"line\">  url           = $url</span><br><span class=\"line\">  url64bit      = $url64</span><br><span class=\"line\">  file          = $fileLocation</span><br><span class=\"line\">  silentArgs    = <span class=\"string\">'/quiet'</span></span><br><span class=\"line\">  softwareName  = <span class=\"string\">'MyAdminProg*'</span> <span class=\"comment\">#part or all of the Display Name as you see it in Programs and Features. It should be enough to be unique</span></span><br><span class=\"line\">  checksum      = <span class=\"string\">''</span></span><br><span class=\"line\">  checksumType  = <span class=\"string\">'md5'</span> <span class=\"comment\">#default is md5, can also be sha1</span></span><br><span class=\"line\">  checksum64    = <span class=\"string\">''</span></span><br><span class=\"line\">  checksumType64= <span class=\"string\">'md5'</span> <span class=\"comment\">#default is checksumType</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Install-ChocolateyInstallPackage @packageArgs</span><br></pre></td></tr></table></figure></p>\n<p>One thing that we did not cover in all this is the fileType value.  This is going to be an exe, msi or msu depending on how you created your setup file.  I took the extra step in my wix install program to create a bootstrap which takes the initial msi and checks the prerequists such as the correct version of the dot net framework and turns that into an exe.  You will need to set this to the value of your install program what you want to run.</p>\n<p>Another advantage to using an install package is that it knows how to uninstall itself.  That means I did not need that other PowerShell script that was in the tools directory which was the chocolateyuninstall.ps1 file.  I deleted mine so that it would use the automatic uninstaller that is managed and controlled by windows (msi).  If this file exists in your package than Chocolatey is going to run that script and if you have not set this up properly will give you issues when you run the Choco uninstall command for the package.</p>\n<h2 id=\"Automating-the-Build-in-TFS-2015\"><a href=\"#Automating-the-Build-in-TFS-2015\" class=\"headerlink\" title=\"Automating the Build in TFS 2015\"></a>Automating the Build in TFS 2015</h2><p>We want to make sure that we place all these two files folders and the nuspec file into source control.  Besides having this is a place where we can repeat this process and keep track of any changes that might happen between changes we will be able to automate the entire operation.  Our goal here is to make a change which when we check in the code change of the actual utility program will kick off a build create the package and publish it to our private Chocolatey feed.</p>\n<p>To automate the building of the chocolatey package I started with a Build Definition that I already had that was building all these pieces.  It built the program, then created an AGPAdminPackage.msi file and then turned that into a bootstrapper and gave me the AGPAdminSetup.exe file.  Our nuspec file has indicated where to find the finished AGPAdminSetup.exe file so that it will be embedded into the finished .nupkg file.  Just after the steps that compile the code, run the tests, I add a PowerShell script and switch it to run inline and write the following script:<br><figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># You can write your powershell scripts inline here. </span></span><br><span class=\"line\"><span class=\"comment\"># You can also pass predefined and custom variables to this scripts using arguments</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"attribute\">cpack</span></span><br></pre></td></tr></table></figure></p>\n<p>This command will find the .nuspec file and create the .nupkg in the same folder as the nuspec file.  From there the things that I do are to copy the pieces I am interested in having in the drop and place them into the staging work space $(Agent.BuildDirectory)\\b and then for the Copy Publish Artifacts I just push everything I have in staging.</p>\n<h2 id=\"Private-Feed\"><a href=\"#Private-Feed\" class=\"headerlink\" title=\"Private Feed\"></a>Private Feed</h2><p>Because Chocolatey is based on Nuget technology it works on exactly the same principal of distribution which is a feed but it could also be a network file share.  I have chosen the private feed as I need this to be a feed that I can access from home, the cloud, and when I am on the road.  Okay so you might be in the same or similar situation as myself so how do you setup a Chocolatey Server?  With Chocolatey of course.<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco install chocolatey<span class=\"selector-class\">.server</span> -y</span><br></pre></td></tr></table></figure></p>\n<p>On the machine that you run this command on, it will create a chocolatey.server folder inside of a folder off of the root drive called tools.  Just point IIS to this folder and you have a Chocolatey feed ready for your packages.  The packages actually go into the App_Data\\packages folder that you will find in this ready to go Chocolatey.server.  However I will make another assumption that this server may not be right next to you but on a different machine or even the cloud so you will want to publish your packages.  To do that you will need to make sure that you give the app pool modify permissions to the App_Data folder.  This in the build definition after the Copy Publish Artifact add another PowerShell script to run inline and this time call the following command:<br><figure class=\"highlight jboss-cli\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># You can write your powershell scripts inline here. </span></span><br><span class=\"line\"><span class=\"comment\"># You can also pass predefined and custom variables to this scripts using arguments</span></span><br><span class=\"line\"></span><br><span class=\"line\">choco push <span class=\"params\">--source=</span><span class=\"string\">\"https://&lt;your server name here&gt;/choco/\"</span> <span class=\"params\">--api-key=</span><span class=\"string\">\"&lt;your api key here&gt;\"</span> <span class=\"params\">--force</span></span><br></pre></td></tr></table></figure></p>\n<p>That is it really you have a package in a feed that can be installed and upgraded with just a simple Chocolatey command.</p>\n<h2 id=\"Make-it-even-Better\"><a href=\"#Make-it-even-Better\" class=\"headerlink\" title=\"Make it even Better\"></a>Make it even Better</h2><p>I went one step farther to make this even easier and that was to modify the chocolatey configuration file so that it looks in my private repository first before looking at the public one that is set up by default.  This way I can install and upgrade my private packages just as if they were published and exposed to the whole world but it is not.  You find the chocolatey.config file in the C:\\ProgramData\\Chocolatey\\config folder.  When you open the file you will see an area called sources and probably one source listed.  Just add an additional source file give it an id (I called my Choco) and the value should be where your chocolatey feed can be found and set the priority to 1. That is it but you need to do this to all the machines that are going to be getting your program and all the latest updates.  Now when ever you are doing a build to about to run tests on a virtual machine you can call have a simple powershell script do it for you.<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco install agpadmin -y</span><br><span class=\"line\">Write-Output <span class=\"string\">\"AGPAdmin Installed\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">choco<span class=\"built_in\"> upgrade </span>agpadmin -y</span><br><span class=\"line\">Write-Output <span class=\"string\">\"AGPAdmin Upgraded\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">Start-Sleep 120</span><br></pre></td></tr></table></figure></p>\n<p>The program I am installing is called agpadmin and I pass the -y so that it skips the confirm as this is almost always part of a build.  I call both the install and then the upgrade as it does not seem to do both but it just ignores the install if it is already installed and will then do the upgrade if there is a newer version out there.</p>\n<p>Hope you enjoy Chocolatey as much as I do.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/Chocolatey.jpg\" class=\"right\" width=\"100\" height=\"100\" title=\"Chocolatey.org\">\n<p>I have been using Chocolatey for a while as an ultra easy way to install software.  It has become the prefered way to install tools and utilities from the open source community.  Recently I have started to explore this technology in more depth just to learn more about Chocolatey and found some really great uses for it that I did not expect to find.  This post is about that adventure and how and what I use Chocolatey for.</p>\n<h2 id=\"Built-on-NuGet\"><a href=\"#Built-on-NuGet\" class=\"headerlink\" title=\"Built on NuGet\"></a>Built on NuGet</h2><p>First off I guess we should talk about what Chocolatey is.  It is another packaged technology based on NuGet.  In fact it is NuGet with some more features and elements added to it.  If you have been around me over the last couple of years, I have declared that NuGet is probably one of the greatest advancements that we have had in the dot net community in the last 10 years.  Initially introduced back in 2010, it was a package tool to help resolve the dependencies in open source software.  Even back then I could see that this technology had legs and indeed it did as it has proven to resolve so many hard development problems that we have worked on for years to resolve.  That being able to have shared code within multiple projects that does not interfere with the development of the underlying projects that depend on them.  I will delve into this subject in a later post as right now I want to focus on Chocolatey.</p>\n<p>While NuGet was really about installing and resolving dependencies at the source code level as in a new Visual Studio project, Chocolatey takes that same package structure and focuses on the Operating System.  In other words I can create NuGet like packages (they have the very same extension as NuGet *.nupkg) that I can install, uninstall or upgrade in Windows.  I have a couple of utility like programs that run on the desktop that I use to support my applications.  These utilities are never distributed or a part of my application that I distribute through click-once but I need up to date version of these on my test lab machines.  It has always been a problem with having some way to get these installed and up to date on these machines.  However, with the use of Chocolatey this is now an easy solution and a problem that I no longer have.</p>\n<h2 id=\"Install-Chocolatey\"><a href=\"#Install-Chocolatey\" class=\"headerlink\" title=\"Install Chocolatey\"></a>Install Chocolatey</h2><p>Let’s start with how we would go about installing Chocolatey.  If you go to the <a href=\"http://chocolatey.org\" target=\"_blank\" rel=\"noopener\">Chocolatey.org web-site</a> there are about 3 ways listed to download and install the package all of them using PowerShell.<br>This first one assumes nothing, as it will Bypass the ExecutionPolicy and has the best chance of installing on your system.<br><figure class=\"highlight mel\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@powershell -NoProfile -ExecutionPolicy Bypass -Command <span class=\"string\">\"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\"</span> &amp;&amp; SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin</span><br></pre></td></tr></table></figure></p>\n<p>This next one is going to assume that you are an administrator on your machine and you have set the Execution Policy to at least RemoteSigned<br><figure class=\"highlight lisp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iex ((<span class=\"name\">new-object</span> net.webclient).DownloadString('https<span class=\"symbol\">://chocolatey</span>.org/install.ps1'))</span><br></pre></td></tr></table></figure></p>\n<p>Then this last script is going to assume that you are an administrator, have the Execution Policy set to at least RemoteSigned and have PowerShell v3 or higher.<br><figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iwr <span class=\"string\">https:</span><span class=\"comment\">//chocolatey.org/install.ps1 -UseBasicParsing | iex</span></span><br></pre></td></tr></table></figure></p>\n<p>Not sure what version of PowerShell you have?  Well the easiest way to tell is to bring up the PowerShell console (you will want to run with Administrator elevated rights) and enter the following:<br><figure class=\"highlight gams\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\"><span class=\"meta-keyword\">$PSVersionTable</span></span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"Making-my-own-Package\"><a href=\"#Making-my-own-Package\" class=\"headerlink\" title=\"Making my own Package\"></a>Making my own Package</h2><p>Okay so I have Chocolatey installed and I have a product that I want to install so how do I get this package created?  Good question so lets tackle that next.  I start by using file explorer, go to your project and create a new folder.  In my case I was working with a utility program that I called AgpAdmin so at the sibling level of that project I made a folder called AgpAdminInstall and this is where I am going to build my package.</p>\n<img src=\"/2016/06/How-I-Use-Chocolatey-in-my-Releases/FileStructure.png\" title=\"The file structure\">\n<p>Now I would bring up PowerShell running as an administrator and navigate over to that new folder that I just created and enter the following Chocolatey command.<br><figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Choco <span class=\"keyword\">New</span> AGPAdmin</span><br></pre></td></tr></table></figure></p>\n<p>This will create the nuspec file with the same name that I entered in that New command as well as a tools folder which will contain two powershell scripts.  There are a couple of ways that you can build this package as the final bits don’t even need to be in this package. They could be referenced in other locations where they can be downloaded and installed. There is a lot of documentation and examples that you can find to do that.  I would say that most of the Chocolatey packages that can be found on Chocolatey.org are done this way.  I found that they mention that the assemblies could be embedded but I never found an example and that was the way that I wanted to package this so that is the guidance I am going to show you here. </p>\n<p>Lets start with the nuspec file.  This is the file that contains the meta data and where all the pieces can be found.  If you are familiar with creating a typical NuGet spec this should all look pretty familiar but there are a couple of things that you must be aware of.  In the Chocolatey version of this spec file you must have a <strong>projectUrl</strong> (in my case I was pointing to my VSTS implementation dashboard page.  You must have a <strong>packageSourceUrl</strong> (in my case I pointed to my source url to my git repository) and a <strong>licenseUrl</strong> which needs to point to a page that describes your license.  I never needed these when building a NuGet package but are required in order to get the Chocolatey package built.  One more thing we need for the nuspec file to be complete is the files section where we tell it what files need to be included in the package.</p>\n<p>There will be one entry there already which is to include all the items found in the folder tools and to place it within the nuget package structure of tools.  We want to add one more file entry where we add a relative path from where we are to include the setup file that is being constructured up one folder and then then down 3 folders through the AGPAdminSetup tree and the target also being within the nuget package structure of tools.  This line is what embeds my setup program into the Chocolatey package.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Do not remove this test for UTF-8: if “Ω” doesn’t appear as greek uppercase omega letter enclosed in quotation marks, you should use an editor that supports UTF-8, not this one. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">package</span> <span class=\"attr\">xmlns</span>=<span class=\"string\">\"http://schemas.microsoft.com/packaging/2015/06/nuspec.xsd\"</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">metadata</span>&gt;</span></span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- Read this before publishing packages to chocolatey.org: https://github.com/chocolatey/chocolatey/wiki/CreatePackages --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">id</span>&gt;</span>agpadmin<span class=\"tag\">&lt;/<span class=\"name\">id</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>AGPAdmin (Install)<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>2.2.0.2<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">authors</span>&gt;</span>Donald L. Schulz<span class=\"tag\">&lt;/<span class=\"name\">authors</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">owners</span>&gt;</span>The Web We Weave, Inc.<span class=\"tag\">&lt;/<span class=\"name\">owners</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">summary</span>&gt;</span>Admin tool to help support AGP-Maker<span class=\"tag\">&lt;/<span class=\"name\">summary</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>Setup and Install of the AGP-Admin program<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">projectUrl</span>&gt;</span>https://donald.visualstudio.com/3WInc/AGP-Admin/_dashboards<span class=\"tag\">&lt;/<span class=\"name\">projectUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">packageSourceUrl</span>&gt;</span>https://donald.visualstudio.com/DefaultCollection/3WInc/_git/AGPMaker-Admin<span class=\"tag\">&lt;/<span class=\"name\">packageSourceUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">tags</span>&gt;</span>agpadmin admin<span class=\"tag\">&lt;/<span class=\"name\">tags</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">copyright</span>&gt;</span>2016 The Web We Weave, Inc.<span class=\"tag\">&lt;/<span class=\"name\">copyright</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">licenseUrl</span>&gt;</span>http://www.agpmaker.com/AGPMaker.Install/index.htm<span class=\"tag\">&lt;/<span class=\"name\">licenseUrl</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">requireLicenseAcceptance</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">requireLicenseAcceptance</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">metadata</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">files</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">file</span> <span class=\"attr\">src</span>=<span class=\"string\">\"..\\AGPAdminSetup\\bin\\Release\\AGPAdminSetup.exe\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"tools\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">file</span> <span class=\"attr\">src</span>=<span class=\"string\">\"tools\\**\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"tools\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">files</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">package</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Before we move on to the automated steps that we want to implement so that we don’t even have to think about building this package every time, we will need to make a couple of changes to the PowerShell scripts that are found in the tools folder.  When you open this powershell script it is well commented and the variable names used are pretty clear in describing what they are for.  You will notice that it seems to be ready out of the box to get you to provide a url where it can get your program to install.  I want to use the embedded solution so un-comment the first $fileLocation line and replace the ‘NAME_OF_EMBEDDED_INSTALLER_FILE’ with the name of the file you want to run and I will also assume that you have it in this same tools folder (in the compiled nupkg file).  In my package I did create an install program using <a href=\"http://wixtoolset.org/\" target=\"_blank\" rel=\"noopener\">the wix toolset</a> which also gives it the capability to uninstall itself automatically.  Next I commented out the default silentArgs and the validExitCodes found right under the #MSI comment.  There is a long string of commented lines that all start with #silentArgs and what I did was un-comment the last one and set the value as ‘/quiet’ and un-comment the validExistCodes line right below that so the line looks like this:<br><figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">silentArgs</span> = <span class=\"string\">'/quiet'</span></span><br><span class=\"line\"><span class=\"attr\">validExitCodes</span>= @(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure></p>\n<p>That is really all that there is to it.  The rest of this script file should just work.  There are a number of different cmdlet’s that you can call and they are all shown in the InstallChocoletey.ps1 file that appeared when you called the Choco new command and they are all commented fairly well.  I was creating the Chocolatey wrapper around an Install program so I chose the cmdlet “Install-ChocolateyInstallPackage”.  So to summarize the PowerShell Script ignoring the commented lines the finished PowerShell script looks a lot like this:<br><figure class=\"highlight perl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ErrorActionPreference = <span class=\"string\">'Stop'</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">$packageName= <span class=\"string\">'MyAdminProg'</span> <span class=\"comment\"># arbitrary name for the package, used in messages</span></span><br><span class=\"line\">$toolsDir   = <span class=\"string\">\"$(Split-Path -parent $MyInvocation.MyCommand.Definition)\"</span></span><br><span class=\"line\">$fileLocation = Join-Path $toolsDir <span class=\"string\">'MyAdminProgSetup.exe'</span></span><br><span class=\"line\"></span><br><span class=\"line\">$packageArgs = @&#123;</span><br><span class=\"line\">  packageName   = $packageName</span><br><span class=\"line\">  unzipLocation = $toolsDir</span><br><span class=\"line\">  fileType      = <span class=\"string\">'EXE'</span> <span class=\"comment\">#only one of these: exe, msi, msu</span></span><br><span class=\"line\">  url           = $url</span><br><span class=\"line\">  url64bit      = $url64</span><br><span class=\"line\">  file          = $fileLocation</span><br><span class=\"line\">  silentArgs    = <span class=\"string\">'/quiet'</span></span><br><span class=\"line\">  softwareName  = <span class=\"string\">'MyAdminProg*'</span> <span class=\"comment\">#part or all of the Display Name as you see it in Programs and Features. It should be enough to be unique</span></span><br><span class=\"line\">  checksum      = <span class=\"string\">''</span></span><br><span class=\"line\">  checksumType  = <span class=\"string\">'md5'</span> <span class=\"comment\">#default is md5, can also be sha1</span></span><br><span class=\"line\">  checksum64    = <span class=\"string\">''</span></span><br><span class=\"line\">  checksumType64= <span class=\"string\">'md5'</span> <span class=\"comment\">#default is checksumType</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Install-ChocolateyInstallPackage @packageArgs</span><br></pre></td></tr></table></figure></p>\n<p>One thing that we did not cover in all this is the fileType value.  This is going to be an exe, msi or msu depending on how you created your setup file.  I took the extra step in my wix install program to create a bootstrap which takes the initial msi and checks the prerequists such as the correct version of the dot net framework and turns that into an exe.  You will need to set this to the value of your install program what you want to run.</p>\n<p>Another advantage to using an install package is that it knows how to uninstall itself.  That means I did not need that other PowerShell script that was in the tools directory which was the chocolateyuninstall.ps1 file.  I deleted mine so that it would use the automatic uninstaller that is managed and controlled by windows (msi).  If this file exists in your package than Chocolatey is going to run that script and if you have not set this up properly will give you issues when you run the Choco uninstall command for the package.</p>\n<h2 id=\"Automating-the-Build-in-TFS-2015\"><a href=\"#Automating-the-Build-in-TFS-2015\" class=\"headerlink\" title=\"Automating the Build in TFS 2015\"></a>Automating the Build in TFS 2015</h2><p>We want to make sure that we place all these two files folders and the nuspec file into source control.  Besides having this is a place where we can repeat this process and keep track of any changes that might happen between changes we will be able to automate the entire operation.  Our goal here is to make a change which when we check in the code change of the actual utility program will kick off a build create the package and publish it to our private Chocolatey feed.</p>\n<p>To automate the building of the chocolatey package I started with a Build Definition that I already had that was building all these pieces.  It built the program, then created an AGPAdminPackage.msi file and then turned that into a bootstrapper and gave me the AGPAdminSetup.exe file.  Our nuspec file has indicated where to find the finished AGPAdminSetup.exe file so that it will be embedded into the finished .nupkg file.  Just after the steps that compile the code, run the tests, I add a PowerShell script and switch it to run inline and write the following script:<br><figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># You can write your powershell scripts inline here. </span></span><br><span class=\"line\"><span class=\"comment\"># You can also pass predefined and custom variables to this scripts using arguments</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"attribute\">cpack</span></span><br></pre></td></tr></table></figure></p>\n<p>This command will find the .nuspec file and create the .nupkg in the same folder as the nuspec file.  From there the things that I do are to copy the pieces I am interested in having in the drop and place them into the staging work space $(Agent.BuildDirectory)\\b and then for the Copy Publish Artifacts I just push everything I have in staging.</p>\n<h2 id=\"Private-Feed\"><a href=\"#Private-Feed\" class=\"headerlink\" title=\"Private Feed\"></a>Private Feed</h2><p>Because Chocolatey is based on Nuget technology it works on exactly the same principal of distribution which is a feed but it could also be a network file share.  I have chosen the private feed as I need this to be a feed that I can access from home, the cloud, and when I am on the road.  Okay so you might be in the same or similar situation as myself so how do you setup a Chocolatey Server?  With Chocolatey of course.<br><figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco install chocolatey<span class=\"selector-class\">.server</span> -y</span><br></pre></td></tr></table></figure></p>\n<p>On the machine that you run this command on, it will create a chocolatey.server folder inside of a folder off of the root drive called tools.  Just point IIS to this folder and you have a Chocolatey feed ready for your packages.  The packages actually go into the App_Data\\packages folder that you will find in this ready to go Chocolatey.server.  However I will make another assumption that this server may not be right next to you but on a different machine or even the cloud so you will want to publish your packages.  To do that you will need to make sure that you give the app pool modify permissions to the App_Data folder.  This in the build definition after the Copy Publish Artifact add another PowerShell script to run inline and this time call the following command:<br><figure class=\"highlight jboss-cli\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># You can write your powershell scripts inline here. </span></span><br><span class=\"line\"><span class=\"comment\"># You can also pass predefined and custom variables to this scripts using arguments</span></span><br><span class=\"line\"></span><br><span class=\"line\">choco push <span class=\"params\">--source=</span><span class=\"string\">\"https://&lt;your server name here&gt;/choco/\"</span> <span class=\"params\">--api-key=</span><span class=\"string\">\"&lt;your api key here&gt;\"</span> <span class=\"params\">--force</span></span><br></pre></td></tr></table></figure></p>\n<p>That is it really you have a package in a feed that can be installed and upgraded with just a simple Chocolatey command.</p>\n<h2 id=\"Make-it-even-Better\"><a href=\"#Make-it-even-Better\" class=\"headerlink\" title=\"Make it even Better\"></a>Make it even Better</h2><p>I went one step farther to make this even easier and that was to modify the chocolatey configuration file so that it looks in my private repository first before looking at the public one that is set up by default.  This way I can install and upgrade my private packages just as if they were published and exposed to the whole world but it is not.  You find the chocolatey.config file in the C:\\ProgramData\\Chocolatey\\config folder.  When you open the file you will see an area called sources and probably one source listed.  Just add an additional source file give it an id (I called my Choco) and the value should be where your chocolatey feed can be found and set the priority to 1. That is it but you need to do this to all the machines that are going to be getting your program and all the latest updates.  Now when ever you are doing a build to about to run tests on a virtual machine you can call have a simple powershell script do it for you.<br><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco install agpadmin -y</span><br><span class=\"line\">Write-Output <span class=\"string\">\"AGPAdmin Installed\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">choco<span class=\"built_in\"> upgrade </span>agpadmin -y</span><br><span class=\"line\">Write-Output <span class=\"string\">\"AGPAdmin Upgraded\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">Start-Sleep 120</span><br></pre></td></tr></table></figure></p>\n<p>The program I am installing is called agpadmin and I pass the -y so that it skips the confirm as this is almost always part of a build.  I call both the install and then the upgrade as it does not seem to do both but it just ignores the install if it is already installed and will then do the upgrade if there is a newer version out there.</p>\n<p>Hope you enjoy Chocolatey as much as I do.</p>\n"},{"title":"Let the Test Plan Tell the Story","date":"2016-04-05T16:02:37.000Z","_content":"{% img right /images/TFSLogo.jpg 300 300 \"TFS\" %}\nThis post has been the result of some discussions that I have had lately when trying to determine the work flow for a client but this often comes up with others in the past but what I had never used as an argument was the role of the test plan in all this.  Besides being an eye opener and an aha moment for the client and myself I thought I would explore this thought a little more as others might also find this helpful in understanding and getting better control of your flows.\n## What is this flow?\nThere is a flow in the way that software is developed and tested no matter how you manage your projects.  Things typically start from some sort of requirement type of work item that describes the business problem and what the client desires to do and should include some benefit that the client would receive if this was implemented.  Yea I just described the basics of a user story which is where we should all be by now when it comes to software development.  The developers and testers and whoever else might be contributing to making this work item a reality start breaking down this requirement type into tasks that they are going to work on to make it happen.\n\nThe developers get to work as they start writing the code and completing their tasks while the testers start writing test cases that they will use to either prove that the new requirement is working as planned or if it has not and simply is not working.  These test cases would all go into a test plan that would represent the current release that you are working on.  As the developers complete their coding the testers will start testing and any test cases that are not passing is going to go back to the developers for re-work.  Now how this is managed is going to depend on how the teams are structured.  Typically in a scrum team where you have developers and testers on the same team this would be a conversation and the developer might just add more tasks because this is work that got missed.  In some situations where the flow between developers and testers is still a separate hand off, a hold out from the waterfall days, then a bug might be issued that goes back to the developers and you follow that through to completion.\n\nAs the work items move from the business to the developers they become Active.  When the developers are code complete the work items should become resolved and as the testers confirm that the code is working properly they become closed.  Any time that the work item is not really resolved (developer wishful thinking) the state would move back to Active.  In TFS (Team Foundation Server) there is an out of the box report called Reactivations which keeps track of the work items that moved from resolved or closed back to active.  This is the first sign that there are some serious communication problems going on between development and test.\n## With all the Requirements and Bugs Closed How will I know what to test?\nThis is where I find many teams start to get a little weird and over complicate their work flows.  I have seen far to many clients take the approach of having additional states that say where the bug is by including the environment that they are testing it in.  For instance they might have something that says Ready for System Testing or Ready for UAT and so on.  Initially this might sound sensible and the right thing to do.  However, I am here to tell you that this is not beneficial, and loses the purpose of the states and this work flow is going to drown you in the amount of work that it takes to manage this.  Let me tell you why.\n\nThink of the state as a control on how developed that requirement or bug is.  For instance it would start off as New or Proposed, depending on your Process template, from there we approve it by changing the state to approved or active.  Those that use active in their work flow don't start working on it until it is moved into the current iteration.  The process that moves it to approved also moves it into a current iterationn to start working on it but they then move the state to committed when they start working on it.  At code completion the active ones go to resolved where the testers will then begin their testing and if satisfied will close the work item.  In the committed group they always work very close to the testers who have been testing all along here so when the test cases are passing then the work item moves to done.  The work on these work items are done, so what happens next is that we start moving this build that represents all the work that has been completed and move it through the release pipeline.  Are you with me so far?\n\nThis is where I typically hear confusion, as the next question is usually something like this: If all the requirement and bug types have been closed how do we know what to test?  The test plan of course, this should be the report that tells you what state that these builds are in.  It should be from this one report, the results of the test plan that we base our approvals for the build to move onto the next environment and eventually to production.  Let the Test Plan Tell the Story.  From the test plan we can not only see how the current functionality is working and matches our expectations but there should also be a certain amount of regression testing going on to make sure features that have worked in the past are still working.  We get all that information from this one single report, the test plan.\n\n{% asset_img TestPlanResults.png \"Test Plan Results\" %}\n\n## The Test Impact Report\nAs we test the various builds throughout the current iteration as new requirements are completed and bugs fixed the testers are running those test cases to verify that this work truly is completed.  If you have been using the Microsoft Test Manager (MTM) and this is a dot net application, you have turned on the test impact instrumentation through the test settings we have the added benefit of the Test Impact Report.  In MTM as you update the build that you are testing it does a comparison to the previous build and what has been tested before.  When it detects that some code has changed near the code that we previously tested and probably passed it is going to include those test cases in the test impact report as tests that you might want to rerun just to make sure that the changes that were made do not affect your passed tests.\n\n{% asset_img testImpactResults.jpg \"Test Impact Results\" %}\n\nThe end result is that we have a test plan that tells the story on the quality of the code written in this iteration and specifically lists the build that we might want to consider to push into production.","source":"_posts/Let-the-Test-Plan-Tell-the-Story.md","raw":"title: Let the Test Plan Tell the Story\ndate: 2016-04-05 09:02:37\ntags:\n- ALM\n- Testing\n- dotNet\n---\n{% img right /images/TFSLogo.jpg 300 300 \"TFS\" %}\nThis post has been the result of some discussions that I have had lately when trying to determine the work flow for a client but this often comes up with others in the past but what I had never used as an argument was the role of the test plan in all this.  Besides being an eye opener and an aha moment for the client and myself I thought I would explore this thought a little more as others might also find this helpful in understanding and getting better control of your flows.\n## What is this flow?\nThere is a flow in the way that software is developed and tested no matter how you manage your projects.  Things typically start from some sort of requirement type of work item that describes the business problem and what the client desires to do and should include some benefit that the client would receive if this was implemented.  Yea I just described the basics of a user story which is where we should all be by now when it comes to software development.  The developers and testers and whoever else might be contributing to making this work item a reality start breaking down this requirement type into tasks that they are going to work on to make it happen.\n\nThe developers get to work as they start writing the code and completing their tasks while the testers start writing test cases that they will use to either prove that the new requirement is working as planned or if it has not and simply is not working.  These test cases would all go into a test plan that would represent the current release that you are working on.  As the developers complete their coding the testers will start testing and any test cases that are not passing is going to go back to the developers for re-work.  Now how this is managed is going to depend on how the teams are structured.  Typically in a scrum team where you have developers and testers on the same team this would be a conversation and the developer might just add more tasks because this is work that got missed.  In some situations where the flow between developers and testers is still a separate hand off, a hold out from the waterfall days, then a bug might be issued that goes back to the developers and you follow that through to completion.\n\nAs the work items move from the business to the developers they become Active.  When the developers are code complete the work items should become resolved and as the testers confirm that the code is working properly they become closed.  Any time that the work item is not really resolved (developer wishful thinking) the state would move back to Active.  In TFS (Team Foundation Server) there is an out of the box report called Reactivations which keeps track of the work items that moved from resolved or closed back to active.  This is the first sign that there are some serious communication problems going on between development and test.\n## With all the Requirements and Bugs Closed How will I know what to test?\nThis is where I find many teams start to get a little weird and over complicate their work flows.  I have seen far to many clients take the approach of having additional states that say where the bug is by including the environment that they are testing it in.  For instance they might have something that says Ready for System Testing or Ready for UAT and so on.  Initially this might sound sensible and the right thing to do.  However, I am here to tell you that this is not beneficial, and loses the purpose of the states and this work flow is going to drown you in the amount of work that it takes to manage this.  Let me tell you why.\n\nThink of the state as a control on how developed that requirement or bug is.  For instance it would start off as New or Proposed, depending on your Process template, from there we approve it by changing the state to approved or active.  Those that use active in their work flow don't start working on it until it is moved into the current iteration.  The process that moves it to approved also moves it into a current iterationn to start working on it but they then move the state to committed when they start working on it.  At code completion the active ones go to resolved where the testers will then begin their testing and if satisfied will close the work item.  In the committed group they always work very close to the testers who have been testing all along here so when the test cases are passing then the work item moves to done.  The work on these work items are done, so what happens next is that we start moving this build that represents all the work that has been completed and move it through the release pipeline.  Are you with me so far?\n\nThis is where I typically hear confusion, as the next question is usually something like this: If all the requirement and bug types have been closed how do we know what to test?  The test plan of course, this should be the report that tells you what state that these builds are in.  It should be from this one report, the results of the test plan that we base our approvals for the build to move onto the next environment and eventually to production.  Let the Test Plan Tell the Story.  From the test plan we can not only see how the current functionality is working and matches our expectations but there should also be a certain amount of regression testing going on to make sure features that have worked in the past are still working.  We get all that information from this one single report, the test plan.\n\n{% asset_img TestPlanResults.png \"Test Plan Results\" %}\n\n## The Test Impact Report\nAs we test the various builds throughout the current iteration as new requirements are completed and bugs fixed the testers are running those test cases to verify that this work truly is completed.  If you have been using the Microsoft Test Manager (MTM) and this is a dot net application, you have turned on the test impact instrumentation through the test settings we have the added benefit of the Test Impact Report.  In MTM as you update the build that you are testing it does a comparison to the previous build and what has been tested before.  When it detects that some code has changed near the code that we previously tested and probably passed it is going to include those test cases in the test impact report as tests that you might want to rerun just to make sure that the changes that were made do not affect your passed tests.\n\n{% asset_img testImpactResults.jpg \"Test Impact Results\" %}\n\nThe end result is that we have a test plan that tells the story on the quality of the code written in this iteration and specifically lists the build that we might want to consider to push into production.","slug":"Let-the-Test-Plan-Tell-the-Story","published":1,"updated":"2020-01-05T00:36:05.014Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfh0009s4uf69g3i0lu","content":"<img src=\"/images/TFSLogo.jpg\" class=\"right\" width=\"300\" height=\"300\" title=\"TFS\">\n<p>This post has been the result of some discussions that I have had lately when trying to determine the work flow for a client but this often comes up with others in the past but what I had never used as an argument was the role of the test plan in all this.  Besides being an eye opener and an aha moment for the client and myself I thought I would explore this thought a little more as others might also find this helpful in understanding and getting better control of your flows.</p>\n<h2 id=\"What-is-this-flow\"><a href=\"#What-is-this-flow\" class=\"headerlink\" title=\"What is this flow?\"></a>What is this flow?</h2><p>There is a flow in the way that software is developed and tested no matter how you manage your projects.  Things typically start from some sort of requirement type of work item that describes the business problem and what the client desires to do and should include some benefit that the client would receive if this was implemented.  Yea I just described the basics of a user story which is where we should all be by now when it comes to software development.  The developers and testers and whoever else might be contributing to making this work item a reality start breaking down this requirement type into tasks that they are going to work on to make it happen.</p>\n<p>The developers get to work as they start writing the code and completing their tasks while the testers start writing test cases that they will use to either prove that the new requirement is working as planned or if it has not and simply is not working.  These test cases would all go into a test plan that would represent the current release that you are working on.  As the developers complete their coding the testers will start testing and any test cases that are not passing is going to go back to the developers for re-work.  Now how this is managed is going to depend on how the teams are structured.  Typically in a scrum team where you have developers and testers on the same team this would be a conversation and the developer might just add more tasks because this is work that got missed.  In some situations where the flow between developers and testers is still a separate hand off, a hold out from the waterfall days, then a bug might be issued that goes back to the developers and you follow that through to completion.</p>\n<p>As the work items move from the business to the developers they become Active.  When the developers are code complete the work items should become resolved and as the testers confirm that the code is working properly they become closed.  Any time that the work item is not really resolved (developer wishful thinking) the state would move back to Active.  In TFS (Team Foundation Server) there is an out of the box report called Reactivations which keeps track of the work items that moved from resolved or closed back to active.  This is the first sign that there are some serious communication problems going on between development and test.</p>\n<h2 id=\"With-all-the-Requirements-and-Bugs-Closed-How-will-I-know-what-to-test\"><a href=\"#With-all-the-Requirements-and-Bugs-Closed-How-will-I-know-what-to-test\" class=\"headerlink\" title=\"With all the Requirements and Bugs Closed How will I know what to test?\"></a>With all the Requirements and Bugs Closed How will I know what to test?</h2><p>This is where I find many teams start to get a little weird and over complicate their work flows.  I have seen far to many clients take the approach of having additional states that say where the bug is by including the environment that they are testing it in.  For instance they might have something that says Ready for System Testing or Ready for UAT and so on.  Initially this might sound sensible and the right thing to do.  However, I am here to tell you that this is not beneficial, and loses the purpose of the states and this work flow is going to drown you in the amount of work that it takes to manage this.  Let me tell you why.</p>\n<p>Think of the state as a control on how developed that requirement or bug is.  For instance it would start off as New or Proposed, depending on your Process template, from there we approve it by changing the state to approved or active.  Those that use active in their work flow don’t start working on it until it is moved into the current iteration.  The process that moves it to approved also moves it into a current iterationn to start working on it but they then move the state to committed when they start working on it.  At code completion the active ones go to resolved where the testers will then begin their testing and if satisfied will close the work item.  In the committed group they always work very close to the testers who have been testing all along here so when the test cases are passing then the work item moves to done.  The work on these work items are done, so what happens next is that we start moving this build that represents all the work that has been completed and move it through the release pipeline.  Are you with me so far?</p>\n<p>This is where I typically hear confusion, as the next question is usually something like this: If all the requirement and bug types have been closed how do we know what to test?  The test plan of course, this should be the report that tells you what state that these builds are in.  It should be from this one report, the results of the test plan that we base our approvals for the build to move onto the next environment and eventually to production.  Let the Test Plan Tell the Story.  From the test plan we can not only see how the current functionality is working and matches our expectations but there should also be a certain amount of regression testing going on to make sure features that have worked in the past are still working.  We get all that information from this one single report, the test plan.</p>\n<img src=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/TestPlanResults.png\" title=\"Test Plan Results\">\n<h2 id=\"The-Test-Impact-Report\"><a href=\"#The-Test-Impact-Report\" class=\"headerlink\" title=\"The Test Impact Report\"></a>The Test Impact Report</h2><p>As we test the various builds throughout the current iteration as new requirements are completed and bugs fixed the testers are running those test cases to verify that this work truly is completed.  If you have been using the Microsoft Test Manager (MTM) and this is a dot net application, you have turned on the test impact instrumentation through the test settings we have the added benefit of the Test Impact Report.  In MTM as you update the build that you are testing it does a comparison to the previous build and what has been tested before.  When it detects that some code has changed near the code that we previously tested and probably passed it is going to include those test cases in the test impact report as tests that you might want to rerun just to make sure that the changes that were made do not affect your passed tests.</p>\n<img src=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/testImpactResults.jpg\" title=\"Test Impact Results\">\n<p>The end result is that we have a test plan that tells the story on the quality of the code written in this iteration and specifically lists the build that we might want to consider to push into production.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/TFSLogo.jpg\" class=\"right\" width=\"300\" height=\"300\" title=\"TFS\">\n<p>This post has been the result of some discussions that I have had lately when trying to determine the work flow for a client but this often comes up with others in the past but what I had never used as an argument was the role of the test plan in all this.  Besides being an eye opener and an aha moment for the client and myself I thought I would explore this thought a little more as others might also find this helpful in understanding and getting better control of your flows.</p>\n<h2 id=\"What-is-this-flow\"><a href=\"#What-is-this-flow\" class=\"headerlink\" title=\"What is this flow?\"></a>What is this flow?</h2><p>There is a flow in the way that software is developed and tested no matter how you manage your projects.  Things typically start from some sort of requirement type of work item that describes the business problem and what the client desires to do and should include some benefit that the client would receive if this was implemented.  Yea I just described the basics of a user story which is where we should all be by now when it comes to software development.  The developers and testers and whoever else might be contributing to making this work item a reality start breaking down this requirement type into tasks that they are going to work on to make it happen.</p>\n<p>The developers get to work as they start writing the code and completing their tasks while the testers start writing test cases that they will use to either prove that the new requirement is working as planned or if it has not and simply is not working.  These test cases would all go into a test plan that would represent the current release that you are working on.  As the developers complete their coding the testers will start testing and any test cases that are not passing is going to go back to the developers for re-work.  Now how this is managed is going to depend on how the teams are structured.  Typically in a scrum team where you have developers and testers on the same team this would be a conversation and the developer might just add more tasks because this is work that got missed.  In some situations where the flow between developers and testers is still a separate hand off, a hold out from the waterfall days, then a bug might be issued that goes back to the developers and you follow that through to completion.</p>\n<p>As the work items move from the business to the developers they become Active.  When the developers are code complete the work items should become resolved and as the testers confirm that the code is working properly they become closed.  Any time that the work item is not really resolved (developer wishful thinking) the state would move back to Active.  In TFS (Team Foundation Server) there is an out of the box report called Reactivations which keeps track of the work items that moved from resolved or closed back to active.  This is the first sign that there are some serious communication problems going on between development and test.</p>\n<h2 id=\"With-all-the-Requirements-and-Bugs-Closed-How-will-I-know-what-to-test\"><a href=\"#With-all-the-Requirements-and-Bugs-Closed-How-will-I-know-what-to-test\" class=\"headerlink\" title=\"With all the Requirements and Bugs Closed How will I know what to test?\"></a>With all the Requirements and Bugs Closed How will I know what to test?</h2><p>This is where I find many teams start to get a little weird and over complicate their work flows.  I have seen far to many clients take the approach of having additional states that say where the bug is by including the environment that they are testing it in.  For instance they might have something that says Ready for System Testing or Ready for UAT and so on.  Initially this might sound sensible and the right thing to do.  However, I am here to tell you that this is not beneficial, and loses the purpose of the states and this work flow is going to drown you in the amount of work that it takes to manage this.  Let me tell you why.</p>\n<p>Think of the state as a control on how developed that requirement or bug is.  For instance it would start off as New or Proposed, depending on your Process template, from there we approve it by changing the state to approved or active.  Those that use active in their work flow don’t start working on it until it is moved into the current iteration.  The process that moves it to approved also moves it into a current iterationn to start working on it but they then move the state to committed when they start working on it.  At code completion the active ones go to resolved where the testers will then begin their testing and if satisfied will close the work item.  In the committed group they always work very close to the testers who have been testing all along here so when the test cases are passing then the work item moves to done.  The work on these work items are done, so what happens next is that we start moving this build that represents all the work that has been completed and move it through the release pipeline.  Are you with me so far?</p>\n<p>This is where I typically hear confusion, as the next question is usually something like this: If all the requirement and bug types have been closed how do we know what to test?  The test plan of course, this should be the report that tells you what state that these builds are in.  It should be from this one report, the results of the test plan that we base our approvals for the build to move onto the next environment and eventually to production.  Let the Test Plan Tell the Story.  From the test plan we can not only see how the current functionality is working and matches our expectations but there should also be a certain amount of regression testing going on to make sure features that have worked in the past are still working.  We get all that information from this one single report, the test plan.</p>\n<img src=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/TestPlanResults.png\" title=\"Test Plan Results\">\n<h2 id=\"The-Test-Impact-Report\"><a href=\"#The-Test-Impact-Report\" class=\"headerlink\" title=\"The Test Impact Report\"></a>The Test Impact Report</h2><p>As we test the various builds throughout the current iteration as new requirements are completed and bugs fixed the testers are running those test cases to verify that this work truly is completed.  If you have been using the Microsoft Test Manager (MTM) and this is a dot net application, you have turned on the test impact instrumentation through the test settings we have the added benefit of the Test Impact Report.  In MTM as you update the build that you are testing it does a comparison to the previous build and what has been tested before.  When it detects that some code has changed near the code that we previously tested and probably passed it is going to include those test cases in the test impact report as tests that you might want to rerun just to make sure that the changes that were made do not affect your passed tests.</p>\n<img src=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/testImpactResults.jpg\" title=\"Test Impact Results\">\n<p>The end result is that we have a test plan that tells the story on the quality of the code written in this iteration and specifically lists the build that we might want to consider to push into production.</p>\n"},{"title":"Linking the Iterations to all your Teams","date":"2018-03-02T23:03:17.000Z","_content":" I am sure that there are several development teams out there that work similar to me.  I am a big fan of the one TFS Project to rule them all and then using teams to separate the work.  In my case I am working on my own but support several products, so I have a team for each of the products even though I am pretty much the only member on all those teams.  This gives me great visibility at the root where I can see everything that is going on and in many cases I might have several products that I am working on in the same sprint.\n {% img center /images/vstsLogo.png 200 200 \"vsts\" %}\nYou are probably a larger organization than I and you might actually have teams associated with the Teams but similar to me you are all sharing the one set of iterations.  This way everyone in the TFS Project are all on the same sprint which really makes the whole one TFS Project to rule them all really powerful.  We can easily jump around and see how progress is going overall and yet still have the individual teams with their separate backlog list and burn down charts.\n\n### Adding New Iterations\nThe problem appears when ever I add additional iterations to my TFS Project.  This doesn't happen all that often, maybe once a year or so but once I have the Iterations defined I have to go to each team and select them into their Iteration pool.  Doing this manually can be quite tedious, and I did it this way for a while, \"I only do this once a year\", was my justification.  However I would be thinking that there should be an easy way to automate this.  Well today, I thought I would tackle this and either write some scripts to accomplish this or find a good solution out there that maybe someone else has already accomplished.\n\n### AIT.VSTS.Scripts\nTurns out there is an open source project out there that does exactly that.  I just needed to create some scripts to load and then call the functions to go through all my teams and set them up with all the iterations that I have.  The nice thing about this script is that if an iteration already exists for that team it just displays the information without changing it and adds the ones that are missing.  First things first, lets get this open source project which is in GitHub and I would suggest cloning it.\n```\ngit clone https://github.com/AITGmbH/AIT.VSTS.Scripts.git\n```\nWith the repository cloned to our local computer we are ready to start to build our own script that we will run using the scripts that are in this open source project to do the actual work.  Okay, so before we start writing this script lets make sure that we have a bunch of Iterations setup and in case you are not sure what I am talking about here or how to do that, lets cover that first.\n### Setting up the Iterations at the Project level\nFirst make sure you are at the Project level of the TFS Project you want to add Iterations to.  You should not be on a team within the Project, you want to be at the root of the Project container.  Then click on the Gear icon and in that drop down select Work.{% img left /images/GettingToWorkArea.png 600 300 \"work menu\" %}\nWhile in the Work menu select the Iteration sub menu and then the next step will depend if you are on the top level or on one of the existing Sprints.  As you can see my selection is on the Root of the Iteration (indicated by the light blue background bar through 3WInc) so I would click on the New Child button as all Iterations fall under this root.  If however I was on Sprint 1 or Sprint 2 I would then click on New which would then give me a new iteration at the same level.  If while on one of these Child Iterations and I clicked on the New Child button, the Iteration would be a Child of that Iteration.\n{% img left /images/NewIterationChild.png 600 300 \"work menu\" %}\nEnter the name of your new Iteration, then the Start and End dates, these dates should all ready be known by TFS and as soon as you click on these boxes the dates show up with what {% img right /images/SaveNewIteration.png 300 300 \"work menu\" %} the next Iteration start should be and then when you click on the end date it should show you what the end date should be as this is based on the pattern you may have started.  If this is your very first Iteration you will need to set this manually and then every Iteration after that TFS will know and understand the pattern.  Click on the Save and Close button.  Add as many Iterations as you need, as you can see by my images I am already making Iterations for next year.\n\n### Getting a Token from TFS or VSTS\nOne of the other things that we are going to need before we really get into the custom PowerShell script that we are going to use to apply all these Iterations to all the teams is a Token. The Token will be the way that the PowerShell tool can authenticate against your version of TFS or your instance of VSTS to perform the work.  From your instance of TFS or VSTS click on your Profile.  This is on the right hand side of the web page and represented by your picture if you have one in your profile or it could just be your initials.  Click on it and select Security.\n\n{% img left /images/OpenProfile.png 600 300 \"profile\" %}\n\n\nOn the Security page you want to make sure you have Personal Access Token (PAT) selected and then click on the Add button.\n\n{% img left /images/AddNewToken.png 600 300 \"pat\" %}\n\nGive your token a name, select the length of time that the Token can last (the maximum is a year).  If you are on VSTS the Account will already be filled in for you.  You can accept the default scope which is not to restrict the scope at all, and keep in mind that this Token is based on your Profile so you can't create a Token that has more permissions than your role provides.  At the bottom of this screen there is a button to Create Token.\n{% img left /images/MyNewToken.png 610 300 \"profile\" %}\nA Guid like string will appear and you want to copy this and store it somewhere because this is the only time that TFS or VSTS will show you this value and this is your Token that we will need in the next step.\n\n### Now We Put Together the Actual Script That we Run\nWith all these pieces in hand we begin the process of building our PowerShell Script that will iterate through our Teams and update the Iterations.\nCreate a new PowerShell script, I called mine \"UpdateIterations.ps1\".  The first line is just a comment that says that we want to load the module that we are going to use into memory.  The second line will do a Change Directory (cd) to the location on your computer where you have the GitHub project cloned to.  The third line will load the module Releate-VstsIteration.ps1 into memory.  It is important that you have written this line exactly as shown. It is a \"period space period backslash Relate-VstsIteration.ps1\"  The next part of the script is an assignment of the variable $token with the PAT that you stored in the previous step.\n```\n# This loads the Releate-VstsIteration scripts and modules into memory\ncd C:\\git\\GitHub\\AIT.VSTS.Scripts\\Iterations\\TeamAssignment\\\n. .\\Relate-VstsIteration.ps1\n\n# Before we really get started lets setup some variable that are sure to change like the token.\n$token = \"<Your Token Goes Here>\"\n```\nAll that is left is a line for each team that you want within your TFS Project to update with the list of iterations that you have created in the Project.  In the sample below you would replace the {account} with your actual VSTS account name and the {TFS Project} with the actual name of the TFS Project.  For TFS you would replace that whole connection to the URL of your TFS instance with a forward slash and the name of the TFS Project.  What you have in the -Username parameter does not matter.  For the -TeamList paramter this is where you put the name of your team.\n```\nRelate-VstsIteration -Projecturi \"https://{account}.visualstudio.com/{TFS Project}\" -Username \"user@example.com\" -Token $token -AuthentificationType \"Token\" -TeamList \"Your First Team\"\nRelate-VstsIteration -Projecturi \"https://{account}.visualstudio.com/{TFS Project}\" -Username \"user@example.com\" -Token $token -AuthentificationType \"Token\" -TeamList \"Your Second Team\"\n```\nMake as many lines of this line of each of your Teams, I just have two listed here but my real set of Teams is really around 12.  With all that in place you just need to run this script which will jump into the location where the open sourced AIT.VSTS.Scripts reside, load the module and run your scripts.  In PowerShell you will see messages about Iterations being updated for each team as it goes through the list.  Once the script has completed go to one of your teams and open the Backlog list and refresh the page, and like magic all the iterations appear.","source":"_posts/Linking-the-Iterations-to-all-your-Teams.md","raw":"---\ntitle: Linking the Iterations to all your Teams\ndate: 2018-03-02 15:03:17\ntags:\n- ALM\n- PowerShell\n- TFS\n---\n I am sure that there are several development teams out there that work similar to me.  I am a big fan of the one TFS Project to rule them all and then using teams to separate the work.  In my case I am working on my own but support several products, so I have a team for each of the products even though I am pretty much the only member on all those teams.  This gives me great visibility at the root where I can see everything that is going on and in many cases I might have several products that I am working on in the same sprint.\n {% img center /images/vstsLogo.png 200 200 \"vsts\" %}\nYou are probably a larger organization than I and you might actually have teams associated with the Teams but similar to me you are all sharing the one set of iterations.  This way everyone in the TFS Project are all on the same sprint which really makes the whole one TFS Project to rule them all really powerful.  We can easily jump around and see how progress is going overall and yet still have the individual teams with their separate backlog list and burn down charts.\n\n### Adding New Iterations\nThe problem appears when ever I add additional iterations to my TFS Project.  This doesn't happen all that often, maybe once a year or so but once I have the Iterations defined I have to go to each team and select them into their Iteration pool.  Doing this manually can be quite tedious, and I did it this way for a while, \"I only do this once a year\", was my justification.  However I would be thinking that there should be an easy way to automate this.  Well today, I thought I would tackle this and either write some scripts to accomplish this or find a good solution out there that maybe someone else has already accomplished.\n\n### AIT.VSTS.Scripts\nTurns out there is an open source project out there that does exactly that.  I just needed to create some scripts to load and then call the functions to go through all my teams and set them up with all the iterations that I have.  The nice thing about this script is that if an iteration already exists for that team it just displays the information without changing it and adds the ones that are missing.  First things first, lets get this open source project which is in GitHub and I would suggest cloning it.\n```\ngit clone https://github.com/AITGmbH/AIT.VSTS.Scripts.git\n```\nWith the repository cloned to our local computer we are ready to start to build our own script that we will run using the scripts that are in this open source project to do the actual work.  Okay, so before we start writing this script lets make sure that we have a bunch of Iterations setup and in case you are not sure what I am talking about here or how to do that, lets cover that first.\n### Setting up the Iterations at the Project level\nFirst make sure you are at the Project level of the TFS Project you want to add Iterations to.  You should not be on a team within the Project, you want to be at the root of the Project container.  Then click on the Gear icon and in that drop down select Work.{% img left /images/GettingToWorkArea.png 600 300 \"work menu\" %}\nWhile in the Work menu select the Iteration sub menu and then the next step will depend if you are on the top level or on one of the existing Sprints.  As you can see my selection is on the Root of the Iteration (indicated by the light blue background bar through 3WInc) so I would click on the New Child button as all Iterations fall under this root.  If however I was on Sprint 1 or Sprint 2 I would then click on New which would then give me a new iteration at the same level.  If while on one of these Child Iterations and I clicked on the New Child button, the Iteration would be a Child of that Iteration.\n{% img left /images/NewIterationChild.png 600 300 \"work menu\" %}\nEnter the name of your new Iteration, then the Start and End dates, these dates should all ready be known by TFS and as soon as you click on these boxes the dates show up with what {% img right /images/SaveNewIteration.png 300 300 \"work menu\" %} the next Iteration start should be and then when you click on the end date it should show you what the end date should be as this is based on the pattern you may have started.  If this is your very first Iteration you will need to set this manually and then every Iteration after that TFS will know and understand the pattern.  Click on the Save and Close button.  Add as many Iterations as you need, as you can see by my images I am already making Iterations for next year.\n\n### Getting a Token from TFS or VSTS\nOne of the other things that we are going to need before we really get into the custom PowerShell script that we are going to use to apply all these Iterations to all the teams is a Token. The Token will be the way that the PowerShell tool can authenticate against your version of TFS or your instance of VSTS to perform the work.  From your instance of TFS or VSTS click on your Profile.  This is on the right hand side of the web page and represented by your picture if you have one in your profile or it could just be your initials.  Click on it and select Security.\n\n{% img left /images/OpenProfile.png 600 300 \"profile\" %}\n\n\nOn the Security page you want to make sure you have Personal Access Token (PAT) selected and then click on the Add button.\n\n{% img left /images/AddNewToken.png 600 300 \"pat\" %}\n\nGive your token a name, select the length of time that the Token can last (the maximum is a year).  If you are on VSTS the Account will already be filled in for you.  You can accept the default scope which is not to restrict the scope at all, and keep in mind that this Token is based on your Profile so you can't create a Token that has more permissions than your role provides.  At the bottom of this screen there is a button to Create Token.\n{% img left /images/MyNewToken.png 610 300 \"profile\" %}\nA Guid like string will appear and you want to copy this and store it somewhere because this is the only time that TFS or VSTS will show you this value and this is your Token that we will need in the next step.\n\n### Now We Put Together the Actual Script That we Run\nWith all these pieces in hand we begin the process of building our PowerShell Script that will iterate through our Teams and update the Iterations.\nCreate a new PowerShell script, I called mine \"UpdateIterations.ps1\".  The first line is just a comment that says that we want to load the module that we are going to use into memory.  The second line will do a Change Directory (cd) to the location on your computer where you have the GitHub project cloned to.  The third line will load the module Releate-VstsIteration.ps1 into memory.  It is important that you have written this line exactly as shown. It is a \"period space period backslash Relate-VstsIteration.ps1\"  The next part of the script is an assignment of the variable $token with the PAT that you stored in the previous step.\n```\n# This loads the Releate-VstsIteration scripts and modules into memory\ncd C:\\git\\GitHub\\AIT.VSTS.Scripts\\Iterations\\TeamAssignment\\\n. .\\Relate-VstsIteration.ps1\n\n# Before we really get started lets setup some variable that are sure to change like the token.\n$token = \"<Your Token Goes Here>\"\n```\nAll that is left is a line for each team that you want within your TFS Project to update with the list of iterations that you have created in the Project.  In the sample below you would replace the {account} with your actual VSTS account name and the {TFS Project} with the actual name of the TFS Project.  For TFS you would replace that whole connection to the URL of your TFS instance with a forward slash and the name of the TFS Project.  What you have in the -Username parameter does not matter.  For the -TeamList paramter this is where you put the name of your team.\n```\nRelate-VstsIteration -Projecturi \"https://{account}.visualstudio.com/{TFS Project}\" -Username \"user@example.com\" -Token $token -AuthentificationType \"Token\" -TeamList \"Your First Team\"\nRelate-VstsIteration -Projecturi \"https://{account}.visualstudio.com/{TFS Project}\" -Username \"user@example.com\" -Token $token -AuthentificationType \"Token\" -TeamList \"Your Second Team\"\n```\nMake as many lines of this line of each of your Teams, I just have two listed here but my real set of Teams is really around 12.  With all that in place you just need to run this script which will jump into the location where the open sourced AIT.VSTS.Scripts reside, load the module and run your scripts.  In PowerShell you will see messages about Iterations being updated for each team as it goes through the list.  Once the script has completed go to one of your teams and open the Backlog list and refresh the page, and like magic all the iterations appear.","slug":"Linking-the-Iterations-to-all-your-Teams","published":1,"updated":"2020-01-05T00:36:05.018Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfj000as4uf44i7dyoa","content":"<p> I am sure that there are several development teams out there that work similar to me.  I am a big fan of the one TFS Project to rule them all and then using teams to separate the work.  In my case I am working on my own but support several products, so I have a team for each of the products even though I am pretty much the only member on all those teams.  This gives me great visibility at the root where I can see everything that is going on and in many cases I might have several products that I am working on in the same sprint.<br> <img src=\"/images/vstsLogo.png\" class=\"center\" width=\"200\" height=\"200\" title=\"vsts\"><br>You are probably a larger organization than I and you might actually have teams associated with the Teams but similar to me you are all sharing the one set of iterations.  This way everyone in the TFS Project are all on the same sprint which really makes the whole one TFS Project to rule them all really powerful.  We can easily jump around and see how progress is going overall and yet still have the individual teams with their separate backlog list and burn down charts.</p>\n<h3 id=\"Adding-New-Iterations\"><a href=\"#Adding-New-Iterations\" class=\"headerlink\" title=\"Adding New Iterations\"></a>Adding New Iterations</h3><p>The problem appears when ever I add additional iterations to my TFS Project.  This doesn’t happen all that often, maybe once a year or so but once I have the Iterations defined I have to go to each team and select them into their Iteration pool.  Doing this manually can be quite tedious, and I did it this way for a while, “I only do this once a year”, was my justification.  However I would be thinking that there should be an easy way to automate this.  Well today, I thought I would tackle this and either write some scripts to accomplish this or find a good solution out there that maybe someone else has already accomplished.</p>\n<h3 id=\"AIT-VSTS-Scripts\"><a href=\"#AIT-VSTS-Scripts\" class=\"headerlink\" title=\"AIT.VSTS.Scripts\"></a>AIT.VSTS.Scripts</h3><p>Turns out there is an open source project out there that does exactly that.  I just needed to create some scripts to load and then call the functions to go through all my teams and set them up with all the iterations that I have.  The nice thing about this script is that if an iteration already exists for that team it just displays the information without changing it and adds the ones that are missing.  First things first, lets get this open source project which is in GitHub and I would suggest cloning it.<br><figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"keyword\">clone</span> <span class=\"title\">https</span>://github.com/AITGmbH/AIT.VSTS.Scripts.git</span><br></pre></td></tr></table></figure></p>\n<p>With the repository cloned to our local computer we are ready to start to build our own script that we will run using the scripts that are in this open source project to do the actual work.  Okay, so before we start writing this script lets make sure that we have a bunch of Iterations setup and in case you are not sure what I am talking about here or how to do that, lets cover that first.</p>\n<h3 id=\"Setting-up-the-Iterations-at-the-Project-level\"><a href=\"#Setting-up-the-Iterations-at-the-Project-level\" class=\"headerlink\" title=\"Setting up the Iterations at the Project level\"></a>Setting up the Iterations at the Project level</h3><p>First make sure you are at the Project level of the TFS Project you want to add Iterations to.  You should not be on a team within the Project, you want to be at the root of the Project container.  Then click on the Gear icon and in that drop down select Work.<img src=\"/images/GettingToWorkArea.png\" class=\"left\" width=\"600\" height=\"300\" title=\"work menu\"><br>While in the Work menu select the Iteration sub menu and then the next step will depend if you are on the top level or on one of the existing Sprints.  As you can see my selection is on the Root of the Iteration (indicated by the light blue background bar through 3WInc) so I would click on the New Child button as all Iterations fall under this root.  If however I was on Sprint 1 or Sprint 2 I would then click on New which would then give me a new iteration at the same level.  If while on one of these Child Iterations and I clicked on the New Child button, the Iteration would be a Child of that Iteration.<br><img src=\"/images/NewIterationChild.png\" class=\"left\" width=\"600\" height=\"300\" title=\"work menu\"><br>Enter the name of your new Iteration, then the Start and End dates, these dates should all ready be known by TFS and as soon as you click on these boxes the dates show up with what <img src=\"/images/SaveNewIteration.png\" class=\"right\" width=\"300\" height=\"300\" title=\"work menu\"> the next Iteration start should be and then when you click on the end date it should show you what the end date should be as this is based on the pattern you may have started.  If this is your very first Iteration you will need to set this manually and then every Iteration after that TFS will know and understand the pattern.  Click on the Save and Close button.  Add as many Iterations as you need, as you can see by my images I am already making Iterations for next year.</p>\n<h3 id=\"Getting-a-Token-from-TFS-or-VSTS\"><a href=\"#Getting-a-Token-from-TFS-or-VSTS\" class=\"headerlink\" title=\"Getting a Token from TFS or VSTS\"></a>Getting a Token from TFS or VSTS</h3><p>One of the other things that we are going to need before we really get into the custom PowerShell script that we are going to use to apply all these Iterations to all the teams is a Token. The Token will be the way that the PowerShell tool can authenticate against your version of TFS or your instance of VSTS to perform the work.  From your instance of TFS or VSTS click on your Profile.  This is on the right hand side of the web page and represented by your picture if you have one in your profile or it could just be your initials.  Click on it and select Security.</p>\n<img src=\"/images/OpenProfile.png\" class=\"left\" width=\"600\" height=\"300\" title=\"profile\">\n<p>On the Security page you want to make sure you have Personal Access Token (PAT) selected and then click on the Add button.</p>\n<img src=\"/images/AddNewToken.png\" class=\"left\" width=\"600\" height=\"300\" title=\"pat\">\n<p>Give your token a name, select the length of time that the Token can last (the maximum is a year).  If you are on VSTS the Account will already be filled in for you.  You can accept the default scope which is not to restrict the scope at all, and keep in mind that this Token is based on your Profile so you can’t create a Token that has more permissions than your role provides.  At the bottom of this screen there is a button to Create Token.<br><img src=\"/images/MyNewToken.png\" class=\"left\" width=\"610\" height=\"300\" title=\"profile\"><br>A Guid like string will appear and you want to copy this and store it somewhere because this is the only time that TFS or VSTS will show you this value and this is your Token that we will need in the next step.</p>\n<h3 id=\"Now-We-Put-Together-the-Actual-Script-That-we-Run\"><a href=\"#Now-We-Put-Together-the-Actual-Script-That-we-Run\" class=\"headerlink\" title=\"Now We Put Together the Actual Script That we Run\"></a>Now We Put Together the Actual Script That we Run</h3><p>With all these pieces in hand we begin the process of building our PowerShell Script that will iterate through our Teams and update the Iterations.<br>Create a new PowerShell script, I called mine “UpdateIterations.ps1”.  The first line is just a comment that says that we want to load the module that we are going to use into memory.  The second line will do a Change Directory (cd) to the location on your computer where you have the GitHub project cloned to.  The third line will load the module Releate-VstsIteration.ps1 into memory.  It is important that you have written this line exactly as shown. It is a “period space period backslash Relate-VstsIteration.ps1”  The next part of the script is an assignment of the variable $token with the PAT that you stored in the previous step.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This loads the Releate-VstsIteration scripts and modules into memory</span></span><br><span class=\"line\"><span class=\"built_in\">cd</span> C:\\git\\GitHub\\AIT.VSTS.Scripts\\Iterations\\TeamAssignment\\</span><br><span class=\"line\">. .\\Relate-VstsIteration.ps1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Before we really get started lets setup some variable that are sure to change like the token.</span></span><br><span class=\"line\"><span class=\"variable\">$token</span> = <span class=\"string\">\"&lt;Your Token Goes Here&gt;\"</span></span><br></pre></td></tr></table></figure></p>\n<p>All that is left is a line for each team that you want within your TFS Project to update with the list of iterations that you have created in the Project.  In the sample below you would replace the {account} with your actual VSTS account name and the {TFS Project} with the actual name of the TFS Project.  For TFS you would replace that whole connection to the URL of your TFS instance with a forward slash and the name of the TFS Project.  What you have in the -Username parameter does not matter.  For the -TeamList paramter this is where you put the name of your team.<br><figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relate-VstsIteration -Projecturi <span class=\"string\">\"https://&#123;account&#125;.visualstudio.com/&#123;TFS Project&#125;\"</span> -Username <span class=\"string\">\"user@example.com\"</span> -<span class=\"built_in\">Token</span> $<span class=\"built_in\">token</span> -AuthentificationType <span class=\"string\">\"Token\"</span> -TeamList <span class=\"string\">\"Your First Team\"</span></span><br><span class=\"line\">Relate-VstsIteration -Projecturi <span class=\"string\">\"https://&#123;account&#125;.visualstudio.com/&#123;TFS Project&#125;\"</span> -Username <span class=\"string\">\"user@example.com\"</span> -<span class=\"built_in\">Token</span> $<span class=\"built_in\">token</span> -AuthentificationType <span class=\"string\">\"Token\"</span> -TeamList <span class=\"string\">\"Your Second Team\"</span></span><br></pre></td></tr></table></figure></p>\n<p>Make as many lines of this line of each of your Teams, I just have two listed here but my real set of Teams is really around 12.  With all that in place you just need to run this script which will jump into the location where the open sourced AIT.VSTS.Scripts reside, load the module and run your scripts.  In PowerShell you will see messages about Iterations being updated for each team as it goes through the list.  Once the script has completed go to one of your teams and open the Backlog list and refresh the page, and like magic all the iterations appear.</p>\n","site":{"data":{}},"excerpt":"","more":"<p> I am sure that there are several development teams out there that work similar to me.  I am a big fan of the one TFS Project to rule them all and then using teams to separate the work.  In my case I am working on my own but support several products, so I have a team for each of the products even though I am pretty much the only member on all those teams.  This gives me great visibility at the root where I can see everything that is going on and in many cases I might have several products that I am working on in the same sprint.<br> <img src=\"/images/vstsLogo.png\" class=\"center\" width=\"200\" height=\"200\" title=\"vsts\"><br>You are probably a larger organization than I and you might actually have teams associated with the Teams but similar to me you are all sharing the one set of iterations.  This way everyone in the TFS Project are all on the same sprint which really makes the whole one TFS Project to rule them all really powerful.  We can easily jump around and see how progress is going overall and yet still have the individual teams with their separate backlog list and burn down charts.</p>\n<h3 id=\"Adding-New-Iterations\"><a href=\"#Adding-New-Iterations\" class=\"headerlink\" title=\"Adding New Iterations\"></a>Adding New Iterations</h3><p>The problem appears when ever I add additional iterations to my TFS Project.  This doesn’t happen all that often, maybe once a year or so but once I have the Iterations defined I have to go to each team and select them into their Iteration pool.  Doing this manually can be quite tedious, and I did it this way for a while, “I only do this once a year”, was my justification.  However I would be thinking that there should be an easy way to automate this.  Well today, I thought I would tackle this and either write some scripts to accomplish this or find a good solution out there that maybe someone else has already accomplished.</p>\n<h3 id=\"AIT-VSTS-Scripts\"><a href=\"#AIT-VSTS-Scripts\" class=\"headerlink\" title=\"AIT.VSTS.Scripts\"></a>AIT.VSTS.Scripts</h3><p>Turns out there is an open source project out there that does exactly that.  I just needed to create some scripts to load and then call the functions to go through all my teams and set them up with all the iterations that I have.  The nice thing about this script is that if an iteration already exists for that team it just displays the information without changing it and adds the ones that are missing.  First things first, lets get this open source project which is in GitHub and I would suggest cloning it.<br><figure class=\"highlight crmsh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"keyword\">clone</span> <span class=\"title\">https</span>://github.com/AITGmbH/AIT.VSTS.Scripts.git</span><br></pre></td></tr></table></figure></p>\n<p>With the repository cloned to our local computer we are ready to start to build our own script that we will run using the scripts that are in this open source project to do the actual work.  Okay, so before we start writing this script lets make sure that we have a bunch of Iterations setup and in case you are not sure what I am talking about here or how to do that, lets cover that first.</p>\n<h3 id=\"Setting-up-the-Iterations-at-the-Project-level\"><a href=\"#Setting-up-the-Iterations-at-the-Project-level\" class=\"headerlink\" title=\"Setting up the Iterations at the Project level\"></a>Setting up the Iterations at the Project level</h3><p>First make sure you are at the Project level of the TFS Project you want to add Iterations to.  You should not be on a team within the Project, you want to be at the root of the Project container.  Then click on the Gear icon and in that drop down select Work.<img src=\"/images/GettingToWorkArea.png\" class=\"left\" width=\"600\" height=\"300\" title=\"work menu\"><br>While in the Work menu select the Iteration sub menu and then the next step will depend if you are on the top level or on one of the existing Sprints.  As you can see my selection is on the Root of the Iteration (indicated by the light blue background bar through 3WInc) so I would click on the New Child button as all Iterations fall under this root.  If however I was on Sprint 1 or Sprint 2 I would then click on New which would then give me a new iteration at the same level.  If while on one of these Child Iterations and I clicked on the New Child button, the Iteration would be a Child of that Iteration.<br><img src=\"/images/NewIterationChild.png\" class=\"left\" width=\"600\" height=\"300\" title=\"work menu\"><br>Enter the name of your new Iteration, then the Start and End dates, these dates should all ready be known by TFS and as soon as you click on these boxes the dates show up with what <img src=\"/images/SaveNewIteration.png\" class=\"right\" width=\"300\" height=\"300\" title=\"work menu\"> the next Iteration start should be and then when you click on the end date it should show you what the end date should be as this is based on the pattern you may have started.  If this is your very first Iteration you will need to set this manually and then every Iteration after that TFS will know and understand the pattern.  Click on the Save and Close button.  Add as many Iterations as you need, as you can see by my images I am already making Iterations for next year.</p>\n<h3 id=\"Getting-a-Token-from-TFS-or-VSTS\"><a href=\"#Getting-a-Token-from-TFS-or-VSTS\" class=\"headerlink\" title=\"Getting a Token from TFS or VSTS\"></a>Getting a Token from TFS or VSTS</h3><p>One of the other things that we are going to need before we really get into the custom PowerShell script that we are going to use to apply all these Iterations to all the teams is a Token. The Token will be the way that the PowerShell tool can authenticate against your version of TFS or your instance of VSTS to perform the work.  From your instance of TFS or VSTS click on your Profile.  This is on the right hand side of the web page and represented by your picture if you have one in your profile or it could just be your initials.  Click on it and select Security.</p>\n<img src=\"/images/OpenProfile.png\" class=\"left\" width=\"600\" height=\"300\" title=\"profile\">\n<p>On the Security page you want to make sure you have Personal Access Token (PAT) selected and then click on the Add button.</p>\n<img src=\"/images/AddNewToken.png\" class=\"left\" width=\"600\" height=\"300\" title=\"pat\">\n<p>Give your token a name, select the length of time that the Token can last (the maximum is a year).  If you are on VSTS the Account will already be filled in for you.  You can accept the default scope which is not to restrict the scope at all, and keep in mind that this Token is based on your Profile so you can’t create a Token that has more permissions than your role provides.  At the bottom of this screen there is a button to Create Token.<br><img src=\"/images/MyNewToken.png\" class=\"left\" width=\"610\" height=\"300\" title=\"profile\"><br>A Guid like string will appear and you want to copy this and store it somewhere because this is the only time that TFS or VSTS will show you this value and this is your Token that we will need in the next step.</p>\n<h3 id=\"Now-We-Put-Together-the-Actual-Script-That-we-Run\"><a href=\"#Now-We-Put-Together-the-Actual-Script-That-we-Run\" class=\"headerlink\" title=\"Now We Put Together the Actual Script That we Run\"></a>Now We Put Together the Actual Script That we Run</h3><p>With all these pieces in hand we begin the process of building our PowerShell Script that will iterate through our Teams and update the Iterations.<br>Create a new PowerShell script, I called mine “UpdateIterations.ps1”.  The first line is just a comment that says that we want to load the module that we are going to use into memory.  The second line will do a Change Directory (cd) to the location on your computer where you have the GitHub project cloned to.  The third line will load the module Releate-VstsIteration.ps1 into memory.  It is important that you have written this line exactly as shown. It is a “period space period backslash Relate-VstsIteration.ps1”  The next part of the script is an assignment of the variable $token with the PAT that you stored in the previous step.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># This loads the Releate-VstsIteration scripts and modules into memory</span></span><br><span class=\"line\"><span class=\"built_in\">cd</span> C:\\git\\GitHub\\AIT.VSTS.Scripts\\Iterations\\TeamAssignment\\</span><br><span class=\"line\">. .\\Relate-VstsIteration.ps1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Before we really get started lets setup some variable that are sure to change like the token.</span></span><br><span class=\"line\"><span class=\"variable\">$token</span> = <span class=\"string\">\"&lt;Your Token Goes Here&gt;\"</span></span><br></pre></td></tr></table></figure></p>\n<p>All that is left is a line for each team that you want within your TFS Project to update with the list of iterations that you have created in the Project.  In the sample below you would replace the {account} with your actual VSTS account name and the {TFS Project} with the actual name of the TFS Project.  For TFS you would replace that whole connection to the URL of your TFS instance with a forward slash and the name of the TFS Project.  What you have in the -Username parameter does not matter.  For the -TeamList paramter this is where you put the name of your team.<br><figure class=\"highlight gauss\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relate-VstsIteration -Projecturi <span class=\"string\">\"https://&#123;account&#125;.visualstudio.com/&#123;TFS Project&#125;\"</span> -Username <span class=\"string\">\"user@example.com\"</span> -<span class=\"built_in\">Token</span> $<span class=\"built_in\">token</span> -AuthentificationType <span class=\"string\">\"Token\"</span> -TeamList <span class=\"string\">\"Your First Team\"</span></span><br><span class=\"line\">Relate-VstsIteration -Projecturi <span class=\"string\">\"https://&#123;account&#125;.visualstudio.com/&#123;TFS Project&#125;\"</span> -Username <span class=\"string\">\"user@example.com\"</span> -<span class=\"built_in\">Token</span> $<span class=\"built_in\">token</span> -AuthentificationType <span class=\"string\">\"Token\"</span> -TeamList <span class=\"string\">\"Your Second Team\"</span></span><br></pre></td></tr></table></figure></p>\n<p>Make as many lines of this line of each of your Teams, I just have two listed here but my real set of Teams is really around 12.  With all that in place you just need to run this script which will jump into the location where the open sourced AIT.VSTS.Scripts reside, load the module and run your scripts.  In PowerShell you will see messages about Iterations being updated for each team as it goes through the list.  Once the script has completed go to one of your teams and open the Backlog list and refresh the page, and like magic all the iterations appear.</p>\n"},{"title":"Migrate from TFVC to Git in TFS with Full History","date":"2016-02-02T07:00:44.000Z","_content":"Over the last year or so I have been experimenting and learning about git.  The more I learned about this distributed version control the more I liked it and finally about 6 months ago I moved all my existing code into git repositories.  They are still hosted on TFS which is the best ALM tool on the market by a very, very, very long mile.  Did I mention how much I love TFS and where this product is going?  Anyway, back to my git road map as this road is not as simple as it sounds because many of the concepts are so different and at first I even thought a bit weird.  After getting my head around the concepts and the true power of this tool there was no turning back.  Just to be clear I am not saying that the old centeralized version control known as TFVC is dead, by no means there are somethings that I will continue to use it for and probably always will like my PowerPoint slides, and much of my training material.\n## Starting with Git\nOne thing about git is that there is just an enormous amount of support and its availability on practically every coding IDE for every platform is just remarkable.  What really made things simple for me to do the migration was an open source project on CodePlex called [Git-TF](https://gittf.codeplex.com/). In fact how I originally used this tool was that I made a separate TFS Project with a git repository.  I would work on that new repository and had some CI builds to make sure things kept working and then when I finished a feature I would push this back to the TFVC as a single changeset however because I always link my commits with a work item in the TFVC project it had a side effect that I was not expecting.  If you opened the work item you would see some commits listed in the links section.  Clicking on the commit link would open up the code in compare mode to the previous commit so you could see what changes were made.  Of course this only works if you are looking at work items from web access.\n\nGit-TF also has some other uses and one of those is the ability to take a folder from TFVC and convert that into a git repository with full history.  That is what I am going to cover in this post.  There are some rules to this that I would like to lay down here as best practises as you don't want to just take a whole TFVC repository and turn it into one big git repository as that just is not going to work.  One of the things to get your head around git is that those respoitories need to be small and should be small remember that you are not getting latest when you clone a repository you are getting the whole thing which includes all the history.\n## Install Git-TF\nOne of the easiest ways to install Git-TF on a windows machine is via [Chocolatey](https://chocolatey.org/) since it will automatically wire up the PATH for you.\n```\nchoco install git-tf -y\n```\nNo [Chocolatey](https://chocolatey.org/) or you just don't want to use this package managment tool you can follow the manual instructions on CodePlex [https://gittf.codeplex.com/](https://gittf.codeplex.com/)\n## Clean up your Branches\nIf you have been a client of mine or ever hear me talk about TFS you will certainly have heard me recommending one collection and one TFS Project.  You would also have heard me talk about minimizing the use of branches for when you need them.  If you have branches going all over the place and code that has never found it's way back to main you are going to want to clean this up as we are only going to clone main for one of these solutions into a git repository.  One of the things that is very different about the git enhanced TFS is that a single TFS project can contain many git repositories.  In fact starting from TFS 2015 update 1 you can have a centralized version control TFVC and multiple git repositories in the same TFS project which totally eliminates the need to create a new TFS project just to hold the git repositories.  We could move the code with full history into a git repo of the same project we are pulling from.\n\nIn our examples that we are pulling into the git repository we are doing this from the solution level as that is where most people using Visual Studio have been doing for decades however the git ideal view of this would be to go even smaller to a single project per repository and stitch the depenancies together for all the other projects through package management through tools like NuGet.  Right now that is out of scope for this posting but will delve into this in a future post.\n## Clone\nNow that we have a nice clean branch to create your git repository it is time to run the clone command from the git-tf tool.  So from the command line make a nice clean directory and then be in that directory as this is where the clone will appear.  *Note: if you don't use the **--deep** switch you will just get the latest tip and not the full history*\n```\nmkdir C:\\git\\MySolutionName\ncd c:\\git\\MySolutionName\ngit-tf clone https://myaccount.visualstudio.com/DefaultCollection $/MyBigProject/MyMainBranch --deep\n```\nYou will then be prompted for your credentials (Alt credentials if using visualstudio.com).  Once accepted, the download will begin and could take some time depending on the length of your changeset history or size of your repository.\n## Prep and Cleanup\nNow that you have an exact replica of your team project branch as a local git repository, it's time to clen up some files and add some others to make things a bit more git friendly.\n- Remvoe the TFS source control bindings from the solution.  You could have done this from within Visual Studio, but its just as easy to do it manually.  Simply remove all the `*.vssscc` files and make small a small edit to your .sln file removing the `GlobalSection(TeamFoundationVersionControl) ...`\n`EndGlobalSection` in your [favorite text editor](https://chocolatey.org/packages/VisualStudioCode).\n- Add a `.gitignore` file.  It's likely your Visual Studio project or solution will have some files you won't want in your repository (packages, obj, ect) once your solution is built.  A near complete way to start is by copying everything from the standard [VisualStudio.gitignore](https://github.com/github/gitignore/blob/master/VisualStudio.gitignore) file into your own repository.  This will ensure all the build generated file, packages, and even your resharper cache folder will not be committed into your new repo.  As you can imagine if all you used was Visual Studio to sling your code that would be that.  However with so much of our work now moving into more hibrid models where we might use several different tools for different parts of the application tying to manage this gitignore file could get pretty complicated.  Recently I came across an online tool at <https://www.gitignore.io/> where you pick the OS, IDEs or Programming Language and it will generate the gitignore file for you.\n## Commit and Push\nNow that we have a local git repository, it is time to commit the files, add the remote (back to TFS), and push the new branch (master) back to TFS so the rest of my team can clone this and continue to contribute to the source which will have full history of every check-in that was done before we converted it to git.  From the root, add and commit any new files as there may have been some changes from the previous Prep and Clean step.\n```\ngit add .\ngit commit -a -m \"initial commit after conversion\"\n```\nWe need a git repository on TFS that we want to push this repository to.  So from TFS in the Project that you want this new repository:\n\n{% asset_img TFSNewRepo.png \"Create a new Repository\" %}\n\n1. Click on the Code tab\n1. Click on the repository dropdown\n1. Click on the New Repoisotry big \"+\" sign.\n\n{% asset_img NewRepoDialog.png \"Name your Repository\" %}\n\n1. Make sure the type is Git\n1. Give it a Name\n1. Click on the Create button.\n\n{% asset_img FinishResults.png \"Useful Git Information\" %}\n\nThe result page gives you all the information that you need to finish off your migration process.\n1. This command adds the remote address to your local repository so that it knows where to put it.\n1. This command will push your local repository to the new remote one.\n\n**That's it!** Project published with all history intact.","source":"_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History.md","raw":"title: Migrate from TFVC to Git in TFS with Full History\ndate: 2016-02-01 23:00:44\ntags:\n- ALM\n- git\n- git-tf\n---\nOver the last year or so I have been experimenting and learning about git.  The more I learned about this distributed version control the more I liked it and finally about 6 months ago I moved all my existing code into git repositories.  They are still hosted on TFS which is the best ALM tool on the market by a very, very, very long mile.  Did I mention how much I love TFS and where this product is going?  Anyway, back to my git road map as this road is not as simple as it sounds because many of the concepts are so different and at first I even thought a bit weird.  After getting my head around the concepts and the true power of this tool there was no turning back.  Just to be clear I am not saying that the old centeralized version control known as TFVC is dead, by no means there are somethings that I will continue to use it for and probably always will like my PowerPoint slides, and much of my training material.\n## Starting with Git\nOne thing about git is that there is just an enormous amount of support and its availability on practically every coding IDE for every platform is just remarkable.  What really made things simple for me to do the migration was an open source project on CodePlex called [Git-TF](https://gittf.codeplex.com/). In fact how I originally used this tool was that I made a separate TFS Project with a git repository.  I would work on that new repository and had some CI builds to make sure things kept working and then when I finished a feature I would push this back to the TFVC as a single changeset however because I always link my commits with a work item in the TFVC project it had a side effect that I was not expecting.  If you opened the work item you would see some commits listed in the links section.  Clicking on the commit link would open up the code in compare mode to the previous commit so you could see what changes were made.  Of course this only works if you are looking at work items from web access.\n\nGit-TF also has some other uses and one of those is the ability to take a folder from TFVC and convert that into a git repository with full history.  That is what I am going to cover in this post.  There are some rules to this that I would like to lay down here as best practises as you don't want to just take a whole TFVC repository and turn it into one big git repository as that just is not going to work.  One of the things to get your head around git is that those respoitories need to be small and should be small remember that you are not getting latest when you clone a repository you are getting the whole thing which includes all the history.\n## Install Git-TF\nOne of the easiest ways to install Git-TF on a windows machine is via [Chocolatey](https://chocolatey.org/) since it will automatically wire up the PATH for you.\n```\nchoco install git-tf -y\n```\nNo [Chocolatey](https://chocolatey.org/) or you just don't want to use this package managment tool you can follow the manual instructions on CodePlex [https://gittf.codeplex.com/](https://gittf.codeplex.com/)\n## Clean up your Branches\nIf you have been a client of mine or ever hear me talk about TFS you will certainly have heard me recommending one collection and one TFS Project.  You would also have heard me talk about minimizing the use of branches for when you need them.  If you have branches going all over the place and code that has never found it's way back to main you are going to want to clean this up as we are only going to clone main for one of these solutions into a git repository.  One of the things that is very different about the git enhanced TFS is that a single TFS project can contain many git repositories.  In fact starting from TFS 2015 update 1 you can have a centralized version control TFVC and multiple git repositories in the same TFS project which totally eliminates the need to create a new TFS project just to hold the git repositories.  We could move the code with full history into a git repo of the same project we are pulling from.\n\nIn our examples that we are pulling into the git repository we are doing this from the solution level as that is where most people using Visual Studio have been doing for decades however the git ideal view of this would be to go even smaller to a single project per repository and stitch the depenancies together for all the other projects through package management through tools like NuGet.  Right now that is out of scope for this posting but will delve into this in a future post.\n## Clone\nNow that we have a nice clean branch to create your git repository it is time to run the clone command from the git-tf tool.  So from the command line make a nice clean directory and then be in that directory as this is where the clone will appear.  *Note: if you don't use the **--deep** switch you will just get the latest tip and not the full history*\n```\nmkdir C:\\git\\MySolutionName\ncd c:\\git\\MySolutionName\ngit-tf clone https://myaccount.visualstudio.com/DefaultCollection $/MyBigProject/MyMainBranch --deep\n```\nYou will then be prompted for your credentials (Alt credentials if using visualstudio.com).  Once accepted, the download will begin and could take some time depending on the length of your changeset history or size of your repository.\n## Prep and Cleanup\nNow that you have an exact replica of your team project branch as a local git repository, it's time to clen up some files and add some others to make things a bit more git friendly.\n- Remvoe the TFS source control bindings from the solution.  You could have done this from within Visual Studio, but its just as easy to do it manually.  Simply remove all the `*.vssscc` files and make small a small edit to your .sln file removing the `GlobalSection(TeamFoundationVersionControl) ...`\n`EndGlobalSection` in your [favorite text editor](https://chocolatey.org/packages/VisualStudioCode).\n- Add a `.gitignore` file.  It's likely your Visual Studio project or solution will have some files you won't want in your repository (packages, obj, ect) once your solution is built.  A near complete way to start is by copying everything from the standard [VisualStudio.gitignore](https://github.com/github/gitignore/blob/master/VisualStudio.gitignore) file into your own repository.  This will ensure all the build generated file, packages, and even your resharper cache folder will not be committed into your new repo.  As you can imagine if all you used was Visual Studio to sling your code that would be that.  However with so much of our work now moving into more hibrid models where we might use several different tools for different parts of the application tying to manage this gitignore file could get pretty complicated.  Recently I came across an online tool at <https://www.gitignore.io/> where you pick the OS, IDEs or Programming Language and it will generate the gitignore file for you.\n## Commit and Push\nNow that we have a local git repository, it is time to commit the files, add the remote (back to TFS), and push the new branch (master) back to TFS so the rest of my team can clone this and continue to contribute to the source which will have full history of every check-in that was done before we converted it to git.  From the root, add and commit any new files as there may have been some changes from the previous Prep and Clean step.\n```\ngit add .\ngit commit -a -m \"initial commit after conversion\"\n```\nWe need a git repository on TFS that we want to push this repository to.  So from TFS in the Project that you want this new repository:\n\n{% asset_img TFSNewRepo.png \"Create a new Repository\" %}\n\n1. Click on the Code tab\n1. Click on the repository dropdown\n1. Click on the New Repoisotry big \"+\" sign.\n\n{% asset_img NewRepoDialog.png \"Name your Repository\" %}\n\n1. Make sure the type is Git\n1. Give it a Name\n1. Click on the Create button.\n\n{% asset_img FinishResults.png \"Useful Git Information\" %}\n\nThe result page gives you all the information that you need to finish off your migration process.\n1. This command adds the remote address to your local repository so that it knows where to put it.\n1. This command will push your local repository to the new remote one.\n\n**That's it!** Project published with all history intact.","slug":"Migrate-from-TFVC-to-Git-in-TFS-with-Full-History","published":1,"updated":"2020-01-05T00:36:05.023Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfk000bs4ufh1deno84","content":"<p>Over the last year or so I have been experimenting and learning about git.  The more I learned about this distributed version control the more I liked it and finally about 6 months ago I moved all my existing code into git repositories.  They are still hosted on TFS which is the best ALM tool on the market by a very, very, very long mile.  Did I mention how much I love TFS and where this product is going?  Anyway, back to my git road map as this road is not as simple as it sounds because many of the concepts are so different and at first I even thought a bit weird.  After getting my head around the concepts and the true power of this tool there was no turning back.  Just to be clear I am not saying that the old centeralized version control known as TFVC is dead, by no means there are somethings that I will continue to use it for and probably always will like my PowerPoint slides, and much of my training material.</p>\n<h2 id=\"Starting-with-Git\"><a href=\"#Starting-with-Git\" class=\"headerlink\" title=\"Starting with Git\"></a>Starting with Git</h2><p>One thing about git is that there is just an enormous amount of support and its availability on practically every coding IDE for every platform is just remarkable.  What really made things simple for me to do the migration was an open source project on CodePlex called <a href=\"https://gittf.codeplex.com/\" target=\"_blank\" rel=\"noopener\">Git-TF</a>. In fact how I originally used this tool was that I made a separate TFS Project with a git repository.  I would work on that new repository and had some CI builds to make sure things kept working and then when I finished a feature I would push this back to the TFVC as a single changeset however because I always link my commits with a work item in the TFVC project it had a side effect that I was not expecting.  If you opened the work item you would see some commits listed in the links section.  Clicking on the commit link would open up the code in compare mode to the previous commit so you could see what changes were made.  Of course this only works if you are looking at work items from web access.</p>\n<p>Git-TF also has some other uses and one of those is the ability to take a folder from TFVC and convert that into a git repository with full history.  That is what I am going to cover in this post.  There are some rules to this that I would like to lay down here as best practises as you don’t want to just take a whole TFVC repository and turn it into one big git repository as that just is not going to work.  One of the things to get your head around git is that those respoitories need to be small and should be small remember that you are not getting latest when you clone a repository you are getting the whole thing which includes all the history.</p>\n<h2 id=\"Install-Git-TF\"><a href=\"#Install-Git-TF\" class=\"headerlink\" title=\"Install Git-TF\"></a>Install Git-TF</h2><p>One of the easiest ways to install Git-TF on a windows machine is via <a href=\"https://chocolatey.org/\" target=\"_blank\" rel=\"noopener\">Chocolatey</a> since it will automatically wire up the PATH for you.<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco <span class=\"keyword\">install</span> git-tf -y</span><br></pre></td></tr></table></figure></p>\n<p>No <a href=\"https://chocolatey.org/\" target=\"_blank\" rel=\"noopener\">Chocolatey</a> or you just don’t want to use this package managment tool you can follow the manual instructions on CodePlex <a href=\"https://gittf.codeplex.com/\" target=\"_blank\" rel=\"noopener\">https://gittf.codeplex.com/</a></p>\n<h2 id=\"Clean-up-your-Branches\"><a href=\"#Clean-up-your-Branches\" class=\"headerlink\" title=\"Clean up your Branches\"></a>Clean up your Branches</h2><p>If you have been a client of mine or ever hear me talk about TFS you will certainly have heard me recommending one collection and one TFS Project.  You would also have heard me talk about minimizing the use of branches for when you need them.  If you have branches going all over the place and code that has never found it’s way back to main you are going to want to clean this up as we are only going to clone main for one of these solutions into a git repository.  One of the things that is very different about the git enhanced TFS is that a single TFS project can contain many git repositories.  In fact starting from TFS 2015 update 1 you can have a centralized version control TFVC and multiple git repositories in the same TFS project which totally eliminates the need to create a new TFS project just to hold the git repositories.  We could move the code with full history into a git repo of the same project we are pulling from.</p>\n<p>In our examples that we are pulling into the git repository we are doing this from the solution level as that is where most people using Visual Studio have been doing for decades however the git ideal view of this would be to go even smaller to a single project per repository and stitch the depenancies together for all the other projects through package management through tools like NuGet.  Right now that is out of scope for this posting but will delve into this in a future post.</p>\n<h2 id=\"Clone\"><a href=\"#Clone\" class=\"headerlink\" title=\"Clone\"></a>Clone</h2><p>Now that we have a nice clean branch to create your git repository it is time to run the clone command from the git-tf tool.  So from the command line make a nice clean directory and then be in that directory as this is where the clone will appear.  <em>Note: if you don’t use the <strong>–deep</strong> switch you will just get the latest tip and not the full history</em><br><figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> C:\\git\\MySolutionName</span><br><span class=\"line\"><span class=\"keyword\">cd</span> <span class=\"keyword\">c</span>:\\git\\MySolutionName</span><br><span class=\"line\">git-<span class=\"keyword\">tf</span> clone http<span class=\"variable\">s:</span>//myaccount.visualstudio.<span class=\"keyword\">com</span>/DefaultCollection $/MyBigProject/MyMainBranch --deep</span><br></pre></td></tr></table></figure></p>\n<p>You will then be prompted for your credentials (Alt credentials if using visualstudio.com).  Once accepted, the download will begin and could take some time depending on the length of your changeset history or size of your repository.</p>\n<h2 id=\"Prep-and-Cleanup\"><a href=\"#Prep-and-Cleanup\" class=\"headerlink\" title=\"Prep and Cleanup\"></a>Prep and Cleanup</h2><p>Now that you have an exact replica of your team project branch as a local git repository, it’s time to clen up some files and add some others to make things a bit more git friendly.</p>\n<ul>\n<li>Remvoe the TFS source control bindings from the solution.  You could have done this from within Visual Studio, but its just as easy to do it manually.  Simply remove all the <code>*.vssscc</code> files and make small a small edit to your .sln file removing the <code>GlobalSection(TeamFoundationVersionControl) ...</code><br><code>EndGlobalSection</code> in your <a href=\"https://chocolatey.org/packages/VisualStudioCode\" target=\"_blank\" rel=\"noopener\">favorite text editor</a>.</li>\n<li>Add a <code>.gitignore</code> file.  It’s likely your Visual Studio project or solution will have some files you won’t want in your repository (packages, obj, ect) once your solution is built.  A near complete way to start is by copying everything from the standard <a href=\"https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\" target=\"_blank\" rel=\"noopener\">VisualStudio.gitignore</a> file into your own repository.  This will ensure all the build generated file, packages, and even your resharper cache folder will not be committed into your new repo.  As you can imagine if all you used was Visual Studio to sling your code that would be that.  However with so much of our work now moving into more hibrid models where we might use several different tools for different parts of the application tying to manage this gitignore file could get pretty complicated.  Recently I came across an online tool at <a href=\"https://www.gitignore.io/\" target=\"_blank\" rel=\"noopener\">https://www.gitignore.io/</a> where you pick the OS, IDEs or Programming Language and it will generate the gitignore file for you.<h2 id=\"Commit-and-Push\"><a href=\"#Commit-and-Push\" class=\"headerlink\" title=\"Commit and Push\"></a>Commit and Push</h2>Now that we have a local git repository, it is time to commit the files, add the remote (back to TFS), and push the new branch (master) back to TFS so the rest of my team can clone this and continue to contribute to the source which will have full history of every check-in that was done before we converted it to git.  From the root, add and commit any new files as there may have been some changes from the previous Prep and Clean step.<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"keyword\">add</span><span class=\"bash\"> .</span></span><br><span class=\"line\"><span class=\"bash\">git commit -a -m <span class=\"string\">\"initial commit after conversion\"</span></span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>We need a git repository on TFS that we want to push this repository to.  So from TFS in the Project that you want this new repository:</p>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/TFSNewRepo.png\" title=\"Create a new Repository\">\n<ol>\n<li>Click on the Code tab</li>\n<li>Click on the repository dropdown</li>\n<li>Click on the New Repoisotry big “+” sign.</li>\n</ol>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/NewRepoDialog.png\" title=\"Name your Repository\">\n<ol>\n<li>Make sure the type is Git</li>\n<li>Give it a Name</li>\n<li>Click on the Create button.</li>\n</ol>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/FinishResults.png\" title=\"Useful Git Information\">\n<p>The result page gives you all the information that you need to finish off your migration process.</p>\n<ol>\n<li>This command adds the remote address to your local repository so that it knows where to put it.</li>\n<li>This command will push your local repository to the new remote one.</li>\n</ol>\n<p><strong>That’s it!</strong> Project published with all history intact.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Over the last year or so I have been experimenting and learning about git.  The more I learned about this distributed version control the more I liked it and finally about 6 months ago I moved all my existing code into git repositories.  They are still hosted on TFS which is the best ALM tool on the market by a very, very, very long mile.  Did I mention how much I love TFS and where this product is going?  Anyway, back to my git road map as this road is not as simple as it sounds because many of the concepts are so different and at first I even thought a bit weird.  After getting my head around the concepts and the true power of this tool there was no turning back.  Just to be clear I am not saying that the old centeralized version control known as TFVC is dead, by no means there are somethings that I will continue to use it for and probably always will like my PowerPoint slides, and much of my training material.</p>\n<h2 id=\"Starting-with-Git\"><a href=\"#Starting-with-Git\" class=\"headerlink\" title=\"Starting with Git\"></a>Starting with Git</h2><p>One thing about git is that there is just an enormous amount of support and its availability on practically every coding IDE for every platform is just remarkable.  What really made things simple for me to do the migration was an open source project on CodePlex called <a href=\"https://gittf.codeplex.com/\" target=\"_blank\" rel=\"noopener\">Git-TF</a>. In fact how I originally used this tool was that I made a separate TFS Project with a git repository.  I would work on that new repository and had some CI builds to make sure things kept working and then when I finished a feature I would push this back to the TFVC as a single changeset however because I always link my commits with a work item in the TFVC project it had a side effect that I was not expecting.  If you opened the work item you would see some commits listed in the links section.  Clicking on the commit link would open up the code in compare mode to the previous commit so you could see what changes were made.  Of course this only works if you are looking at work items from web access.</p>\n<p>Git-TF also has some other uses and one of those is the ability to take a folder from TFVC and convert that into a git repository with full history.  That is what I am going to cover in this post.  There are some rules to this that I would like to lay down here as best practises as you don’t want to just take a whole TFVC repository and turn it into one big git repository as that just is not going to work.  One of the things to get your head around git is that those respoitories need to be small and should be small remember that you are not getting latest when you clone a repository you are getting the whole thing which includes all the history.</p>\n<h2 id=\"Install-Git-TF\"><a href=\"#Install-Git-TF\" class=\"headerlink\" title=\"Install Git-TF\"></a>Install Git-TF</h2><p>One of the easiest ways to install Git-TF on a windows machine is via <a href=\"https://chocolatey.org/\" target=\"_blank\" rel=\"noopener\">Chocolatey</a> since it will automatically wire up the PATH for you.<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">choco <span class=\"keyword\">install</span> git-tf -y</span><br></pre></td></tr></table></figure></p>\n<p>No <a href=\"https://chocolatey.org/\" target=\"_blank\" rel=\"noopener\">Chocolatey</a> or you just don’t want to use this package managment tool you can follow the manual instructions on CodePlex <a href=\"https://gittf.codeplex.com/\" target=\"_blank\" rel=\"noopener\">https://gittf.codeplex.com/</a></p>\n<h2 id=\"Clean-up-your-Branches\"><a href=\"#Clean-up-your-Branches\" class=\"headerlink\" title=\"Clean up your Branches\"></a>Clean up your Branches</h2><p>If you have been a client of mine or ever hear me talk about TFS you will certainly have heard me recommending one collection and one TFS Project.  You would also have heard me talk about minimizing the use of branches for when you need them.  If you have branches going all over the place and code that has never found it’s way back to main you are going to want to clean this up as we are only going to clone main for one of these solutions into a git repository.  One of the things that is very different about the git enhanced TFS is that a single TFS project can contain many git repositories.  In fact starting from TFS 2015 update 1 you can have a centralized version control TFVC and multiple git repositories in the same TFS project which totally eliminates the need to create a new TFS project just to hold the git repositories.  We could move the code with full history into a git repo of the same project we are pulling from.</p>\n<p>In our examples that we are pulling into the git repository we are doing this from the solution level as that is where most people using Visual Studio have been doing for decades however the git ideal view of this would be to go even smaller to a single project per repository and stitch the depenancies together for all the other projects through package management through tools like NuGet.  Right now that is out of scope for this posting but will delve into this in a future post.</p>\n<h2 id=\"Clone\"><a href=\"#Clone\" class=\"headerlink\" title=\"Clone\"></a>Clone</h2><p>Now that we have a nice clean branch to create your git repository it is time to run the clone command from the git-tf tool.  So from the command line make a nice clean directory and then be in that directory as this is where the clone will appear.  <em>Note: if you don’t use the <strong>–deep</strong> switch you will just get the latest tip and not the full history</em><br><figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> C:\\git\\MySolutionName</span><br><span class=\"line\"><span class=\"keyword\">cd</span> <span class=\"keyword\">c</span>:\\git\\MySolutionName</span><br><span class=\"line\">git-<span class=\"keyword\">tf</span> clone http<span class=\"variable\">s:</span>//myaccount.visualstudio.<span class=\"keyword\">com</span>/DefaultCollection $/MyBigProject/MyMainBranch --deep</span><br></pre></td></tr></table></figure></p>\n<p>You will then be prompted for your credentials (Alt credentials if using visualstudio.com).  Once accepted, the download will begin and could take some time depending on the length of your changeset history or size of your repository.</p>\n<h2 id=\"Prep-and-Cleanup\"><a href=\"#Prep-and-Cleanup\" class=\"headerlink\" title=\"Prep and Cleanup\"></a>Prep and Cleanup</h2><p>Now that you have an exact replica of your team project branch as a local git repository, it’s time to clen up some files and add some others to make things a bit more git friendly.</p>\n<ul>\n<li>Remvoe the TFS source control bindings from the solution.  You could have done this from within Visual Studio, but its just as easy to do it manually.  Simply remove all the <code>*.vssscc</code> files and make small a small edit to your .sln file removing the <code>GlobalSection(TeamFoundationVersionControl) ...</code><br><code>EndGlobalSection</code> in your <a href=\"https://chocolatey.org/packages/VisualStudioCode\" target=\"_blank\" rel=\"noopener\">favorite text editor</a>.</li>\n<li>Add a <code>.gitignore</code> file.  It’s likely your Visual Studio project or solution will have some files you won’t want in your repository (packages, obj, ect) once your solution is built.  A near complete way to start is by copying everything from the standard <a href=\"https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\" target=\"_blank\" rel=\"noopener\">VisualStudio.gitignore</a> file into your own repository.  This will ensure all the build generated file, packages, and even your resharper cache folder will not be committed into your new repo.  As you can imagine if all you used was Visual Studio to sling your code that would be that.  However with so much of our work now moving into more hibrid models where we might use several different tools for different parts of the application tying to manage this gitignore file could get pretty complicated.  Recently I came across an online tool at <a href=\"https://www.gitignore.io/\" target=\"_blank\" rel=\"noopener\">https://www.gitignore.io/</a> where you pick the OS, IDEs or Programming Language and it will generate the gitignore file for you.<h2 id=\"Commit-and-Push\"><a href=\"#Commit-and-Push\" class=\"headerlink\" title=\"Commit and Push\"></a>Commit and Push</h2>Now that we have a local git repository, it is time to commit the files, add the remote (back to TFS), and push the new branch (master) back to TFS so the rest of my team can clone this and continue to contribute to the source which will have full history of every check-in that was done before we converted it to git.  From the root, add and commit any new files as there may have been some changes from the previous Prep and Clean step.<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"keyword\">add</span><span class=\"bash\"> .</span></span><br><span class=\"line\"><span class=\"bash\">git commit -a -m <span class=\"string\">\"initial commit after conversion\"</span></span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>We need a git repository on TFS that we want to push this repository to.  So from TFS in the Project that you want this new repository:</p>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/TFSNewRepo.png\" title=\"Create a new Repository\">\n<ol>\n<li>Click on the Code tab</li>\n<li>Click on the repository dropdown</li>\n<li>Click on the New Repoisotry big “+” sign.</li>\n</ol>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/NewRepoDialog.png\" title=\"Name your Repository\">\n<ol>\n<li>Make sure the type is Git</li>\n<li>Give it a Name</li>\n<li>Click on the Create button.</li>\n</ol>\n<img src=\"/2016/02/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/FinishResults.png\" title=\"Useful Git Information\">\n<p>The result page gives you all the information that you need to finish off your migration process.</p>\n<ol>\n<li>This command adds the remote address to your local repository so that it knows where to put it.</li>\n<li>This command will push your local repository to the new remote one.</li>\n</ol>\n<p><strong>That’s it!</strong> Project published with all history intact.</p>\n"},{"title":"Living on a Vegan Diet","date":"2016-03-07T19:30:08.000Z","_content":"{% img right /images/go-vegan.png 300 300 \"Keep Calm and Go Vegan\" %}\n\nIn all my blog posts that I have written over the years I have never talked about health or a healthy lifestyle.  This will be a first and you as a technology person might be wondering what has living a Vegan Lifestyle have anything to do with software.  After all the blog title is **\"Donald on Software\"**.\n\nFor years I would go through these decade birthdays and just remark how turning thirty was just like turning twenty except I had all the extra knowledge called life. Going from thirty to forty, same thing but things took a turn when I moved into my fifties.  I have had doctors notice that my blood pressure was a bit elevated.  I took longer to recover from physical activates.  Felt aches I never noticed before and I promised my wife that I would live a long, long time and that wasn't feeling all that convincing.  I didn't have the same get up and go that I had known before.\n\n## A Bit About My Family\nMy wife and step daughter have been vegetarian/vegans for many years.  I was open to other types of food like plant based meals and would eat them on occasion when we were at a vegan restaurant or that was what was being cooked at home.  However, I travel a lot so most of my food would be from a restaurant where I could eat anything I wanted.  This went on for several years, I was taking a mild blood pressure pill every day.  This was keeping my blood pressure under control but there were other things that it appeared to be affecting as well in a negative way.\n{% img right /images/Forks_Over_Knives_movie_poster.png 300 300 \"Keep Calm and Go Vegan\" %}\n## The Turning Point for Me\nDuring Thanksgiving weekend in November 2014, Mary (my wife) and I watched a documentary on Netflix called [**\"Forks over Knives\"**](http://www.bing.com/search?q=netflix+forks+over+knives&qs=AS&pq=netflix+forks+over&sc=1-18&sp=1&cvid=7FB1E1E182E14BAFAC1474507767F07F&FORM=QBLH&ghc=1), and at the end of that I vowed never to eat meat again and start moving towards a Vegan lifestyle.  \nThe documentary is about two doctors one that came from the medical field and one from the science side of things and their adventure to unravelling the truth about how the food that we eat is related to health.  One of the biggest studies that has ever been done is called \"The China Study\" and is a 20 year study that examines the relationship between the consumption of animal products (including dairy) and chronic illnesses such as coronary heart disease, diabetes, breast cancer, prostate cancer and bowel cancer.\n\nNot only reducing these numbers but now that the toxic animal products were out of our system, our bodies would start to repair some of this damage that we have always been told could never be repairable naturally.\n## Getting over the big Lie\nYes there is a very large lie that we have all believed to be the truth because we assumed that it came from the medical field and sanctioned by the government to be the truth.  That being the daily nutritional guide.  This is the guide that told use to eat large amounts of meat and dairy products to give us energy and strong bones but this did not come from any medical study this came from the agriculture and dairy industries to sell more products.  \n\nMost of that animal protein that we take in our body rejects, there is very small amounts that it actually uses.  Now common sense would tell me if my body is rejecting all this animal based protein it is working extra hard and something is going to break down in the form of disease and other difficulties especially as we get older.  Oh wait, they now make a pill for that so we can continue to live the way we always have.  So now we are not only supporting an industry that never had that big of a market before but now we are spending billions of dollars every year to pharmaceutical companies as well in order to correct the mistakes we made with the things we eat.  One thing that I did learn in physics is that one action creates another and opposite reaction so this is not solving anything either just keep making it worse and now health care costs are through the roof with bodies that normally know how to heal themselves. \n## Now for the Good News\nI know I got you all depressed and disappointed as I just dissed your favorite food and called it bad and toxic but there is a happy ending here.  I felt like you are right now for about five minutes and then decided to say \"NO to Meat\".  If you get a chance I would encourage you to look up that documentary [**\"Forks over Knives\"**](http://www.bing.com/search?q=netflix+forks+over+knives&qs=AS&pq=netflix+forks+over&sc=1-18&sp=1&cvid=7FB1E1E182E14BAFAC1474507767F07F&FORM=QBLH&ghc=1) as one other thing that disturbed me was the way they were harvesting these animals and called it ethical or within the approved guidelines.  These animals were under stress and that stress goes into the meat and you wonder why everyone seems so stressed, I know there is a relationship here.\n\nAnyway, the good news is my latest checkup with my doctor.  I am currently on no medication what so ever and my blood pressure numbers are very normal and very impressive for a guy my age.  I did a stress test and was able to reach my ideal heart rate easily and effortlessly and I feel great.  If I had any plaque buildup it is certainly repairing itself as I feel great.  Still can't seem to lose the 15 pounds I have been working on for the last couple of years but I know I will accomplish that soon enough.  I am done with meat and all animal proteins as in milk, eggs, honey and I am going to live a long, long time and feel great.  Won't you join me?","source":"_posts/Living-on-a-Vegan-Diet.md","raw":"title: Living on a Vegan Diet\ndate: 2016-03-07 11:30:08\ntags:\n- Lifestyle\n- Vegan\n- Health\n---\n{% img right /images/go-vegan.png 300 300 \"Keep Calm and Go Vegan\" %}\n\nIn all my blog posts that I have written over the years I have never talked about health or a healthy lifestyle.  This will be a first and you as a technology person might be wondering what has living a Vegan Lifestyle have anything to do with software.  After all the blog title is **\"Donald on Software\"**.\n\nFor years I would go through these decade birthdays and just remark how turning thirty was just like turning twenty except I had all the extra knowledge called life. Going from thirty to forty, same thing but things took a turn when I moved into my fifties.  I have had doctors notice that my blood pressure was a bit elevated.  I took longer to recover from physical activates.  Felt aches I never noticed before and I promised my wife that I would live a long, long time and that wasn't feeling all that convincing.  I didn't have the same get up and go that I had known before.\n\n## A Bit About My Family\nMy wife and step daughter have been vegetarian/vegans for many years.  I was open to other types of food like plant based meals and would eat them on occasion when we were at a vegan restaurant or that was what was being cooked at home.  However, I travel a lot so most of my food would be from a restaurant where I could eat anything I wanted.  This went on for several years, I was taking a mild blood pressure pill every day.  This was keeping my blood pressure under control but there were other things that it appeared to be affecting as well in a negative way.\n{% img right /images/Forks_Over_Knives_movie_poster.png 300 300 \"Keep Calm and Go Vegan\" %}\n## The Turning Point for Me\nDuring Thanksgiving weekend in November 2014, Mary (my wife) and I watched a documentary on Netflix called [**\"Forks over Knives\"**](http://www.bing.com/search?q=netflix+forks+over+knives&qs=AS&pq=netflix+forks+over&sc=1-18&sp=1&cvid=7FB1E1E182E14BAFAC1474507767F07F&FORM=QBLH&ghc=1), and at the end of that I vowed never to eat meat again and start moving towards a Vegan lifestyle.  \nThe documentary is about two doctors one that came from the medical field and one from the science side of things and their adventure to unravelling the truth about how the food that we eat is related to health.  One of the biggest studies that has ever been done is called \"The China Study\" and is a 20 year study that examines the relationship between the consumption of animal products (including dairy) and chronic illnesses such as coronary heart disease, diabetes, breast cancer, prostate cancer and bowel cancer.\n\nNot only reducing these numbers but now that the toxic animal products were out of our system, our bodies would start to repair some of this damage that we have always been told could never be repairable naturally.\n## Getting over the big Lie\nYes there is a very large lie that we have all believed to be the truth because we assumed that it came from the medical field and sanctioned by the government to be the truth.  That being the daily nutritional guide.  This is the guide that told use to eat large amounts of meat and dairy products to give us energy and strong bones but this did not come from any medical study this came from the agriculture and dairy industries to sell more products.  \n\nMost of that animal protein that we take in our body rejects, there is very small amounts that it actually uses.  Now common sense would tell me if my body is rejecting all this animal based protein it is working extra hard and something is going to break down in the form of disease and other difficulties especially as we get older.  Oh wait, they now make a pill for that so we can continue to live the way we always have.  So now we are not only supporting an industry that never had that big of a market before but now we are spending billions of dollars every year to pharmaceutical companies as well in order to correct the mistakes we made with the things we eat.  One thing that I did learn in physics is that one action creates another and opposite reaction so this is not solving anything either just keep making it worse and now health care costs are through the roof with bodies that normally know how to heal themselves. \n## Now for the Good News\nI know I got you all depressed and disappointed as I just dissed your favorite food and called it bad and toxic but there is a happy ending here.  I felt like you are right now for about five minutes and then decided to say \"NO to Meat\".  If you get a chance I would encourage you to look up that documentary [**\"Forks over Knives\"**](http://www.bing.com/search?q=netflix+forks+over+knives&qs=AS&pq=netflix+forks+over&sc=1-18&sp=1&cvid=7FB1E1E182E14BAFAC1474507767F07F&FORM=QBLH&ghc=1) as one other thing that disturbed me was the way they were harvesting these animals and called it ethical or within the approved guidelines.  These animals were under stress and that stress goes into the meat and you wonder why everyone seems so stressed, I know there is a relationship here.\n\nAnyway, the good news is my latest checkup with my doctor.  I am currently on no medication what so ever and my blood pressure numbers are very normal and very impressive for a guy my age.  I did a stress test and was able to reach my ideal heart rate easily and effortlessly and I feel great.  If I had any plaque buildup it is certainly repairing itself as I feel great.  Still can't seem to lose the 15 pounds I have been working on for the last couple of years but I know I will accomplish that soon enough.  I am done with meat and all animal proteins as in milk, eggs, honey and I am going to live a long, long time and feel great.  Won't you join me?","slug":"Living-on-a-Vegan-Diet","published":1,"updated":"2020-01-05T00:36:05.018Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfl000cs4uf3mq3og8i","content":"<img src=\"/images/go-vegan.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Keep Calm and Go Vegan\">\n<p>In all my blog posts that I have written over the years I have never talked about health or a healthy lifestyle.  This will be a first and you as a technology person might be wondering what has living a Vegan Lifestyle have anything to do with software.  After all the blog title is <strong>“Donald on Software”</strong>.</p>\n<p>For years I would go through these decade birthdays and just remark how turning thirty was just like turning twenty except I had all the extra knowledge called life. Going from thirty to forty, same thing but things took a turn when I moved into my fifties.  I have had doctors notice that my blood pressure was a bit elevated.  I took longer to recover from physical activates.  Felt aches I never noticed before and I promised my wife that I would live a long, long time and that wasn’t feeling all that convincing.  I didn’t have the same get up and go that I had known before.</p>\n<h2 id=\"A-Bit-About-My-Family\"><a href=\"#A-Bit-About-My-Family\" class=\"headerlink\" title=\"A Bit About My Family\"></a>A Bit About My Family</h2><p>My wife and step daughter have been vegetarian/vegans for many years.  I was open to other types of food like plant based meals and would eat them on occasion when we were at a vegan restaurant or that was what was being cooked at home.  However, I travel a lot so most of my food would be from a restaurant where I could eat anything I wanted.  This went on for several years, I was taking a mild blood pressure pill every day.  This was keeping my blood pressure under control but there were other things that it appeared to be affecting as well in a negative way.<br><img src=\"/images/Forks_Over_Knives_movie_poster.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Keep Calm and Go Vegan\"></p>\n<h2 id=\"The-Turning-Point-for-Me\"><a href=\"#The-Turning-Point-for-Me\" class=\"headerlink\" title=\"The Turning Point for Me\"></a>The Turning Point for Me</h2><p>During Thanksgiving weekend in November 2014, Mary (my wife) and I watched a documentary on Netflix called <a href=\"http://www.bing.com/search?q=netflix+forks+over+knives&amp;qs=AS&amp;pq=netflix+forks+over&amp;sc=1-18&amp;sp=1&amp;cvid=7FB1E1E182E14BAFAC1474507767F07F&amp;FORM=QBLH&amp;ghc=1\" target=\"_blank\" rel=\"noopener\"><strong>“Forks over Knives”</strong></a>, and at the end of that I vowed never to eat meat again and start moving towards a Vegan lifestyle.<br>The documentary is about two doctors one that came from the medical field and one from the science side of things and their adventure to unravelling the truth about how the food that we eat is related to health.  One of the biggest studies that has ever been done is called “The China Study” and is a 20 year study that examines the relationship between the consumption of animal products (including dairy) and chronic illnesses such as coronary heart disease, diabetes, breast cancer, prostate cancer and bowel cancer.</p>\n<p>Not only reducing these numbers but now that the toxic animal products were out of our system, our bodies would start to repair some of this damage that we have always been told could never be repairable naturally.</p>\n<h2 id=\"Getting-over-the-big-Lie\"><a href=\"#Getting-over-the-big-Lie\" class=\"headerlink\" title=\"Getting over the big Lie\"></a>Getting over the big Lie</h2><p>Yes there is a very large lie that we have all believed to be the truth because we assumed that it came from the medical field and sanctioned by the government to be the truth.  That being the daily nutritional guide.  This is the guide that told use to eat large amounts of meat and dairy products to give us energy and strong bones but this did not come from any medical study this came from the agriculture and dairy industries to sell more products.  </p>\n<p>Most of that animal protein that we take in our body rejects, there is very small amounts that it actually uses.  Now common sense would tell me if my body is rejecting all this animal based protein it is working extra hard and something is going to break down in the form of disease and other difficulties especially as we get older.  Oh wait, they now make a pill for that so we can continue to live the way we always have.  So now we are not only supporting an industry that never had that big of a market before but now we are spending billions of dollars every year to pharmaceutical companies as well in order to correct the mistakes we made with the things we eat.  One thing that I did learn in physics is that one action creates another and opposite reaction so this is not solving anything either just keep making it worse and now health care costs are through the roof with bodies that normally know how to heal themselves. </p>\n<h2 id=\"Now-for-the-Good-News\"><a href=\"#Now-for-the-Good-News\" class=\"headerlink\" title=\"Now for the Good News\"></a>Now for the Good News</h2><p>I know I got you all depressed and disappointed as I just dissed your favorite food and called it bad and toxic but there is a happy ending here.  I felt like you are right now for about five minutes and then decided to say “NO to Meat”.  If you get a chance I would encourage you to look up that documentary <a href=\"http://www.bing.com/search?q=netflix+forks+over+knives&amp;qs=AS&amp;pq=netflix+forks+over&amp;sc=1-18&amp;sp=1&amp;cvid=7FB1E1E182E14BAFAC1474507767F07F&amp;FORM=QBLH&amp;ghc=1\" target=\"_blank\" rel=\"noopener\"><strong>“Forks over Knives”</strong></a> as one other thing that disturbed me was the way they were harvesting these animals and called it ethical or within the approved guidelines.  These animals were under stress and that stress goes into the meat and you wonder why everyone seems so stressed, I know there is a relationship here.</p>\n<p>Anyway, the good news is my latest checkup with my doctor.  I am currently on no medication what so ever and my blood pressure numbers are very normal and very impressive for a guy my age.  I did a stress test and was able to reach my ideal heart rate easily and effortlessly and I feel great.  If I had any plaque buildup it is certainly repairing itself as I feel great.  Still can’t seem to lose the 15 pounds I have been working on for the last couple of years but I know I will accomplish that soon enough.  I am done with meat and all animal proteins as in milk, eggs, honey and I am going to live a long, long time and feel great.  Won’t you join me?</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/go-vegan.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Keep Calm and Go Vegan\">\n<p>In all my blog posts that I have written over the years I have never talked about health or a healthy lifestyle.  This will be a first and you as a technology person might be wondering what has living a Vegan Lifestyle have anything to do with software.  After all the blog title is <strong>“Donald on Software”</strong>.</p>\n<p>For years I would go through these decade birthdays and just remark how turning thirty was just like turning twenty except I had all the extra knowledge called life. Going from thirty to forty, same thing but things took a turn when I moved into my fifties.  I have had doctors notice that my blood pressure was a bit elevated.  I took longer to recover from physical activates.  Felt aches I never noticed before and I promised my wife that I would live a long, long time and that wasn’t feeling all that convincing.  I didn’t have the same get up and go that I had known before.</p>\n<h2 id=\"A-Bit-About-My-Family\"><a href=\"#A-Bit-About-My-Family\" class=\"headerlink\" title=\"A Bit About My Family\"></a>A Bit About My Family</h2><p>My wife and step daughter have been vegetarian/vegans for many years.  I was open to other types of food like plant based meals and would eat them on occasion when we were at a vegan restaurant or that was what was being cooked at home.  However, I travel a lot so most of my food would be from a restaurant where I could eat anything I wanted.  This went on for several years, I was taking a mild blood pressure pill every day.  This was keeping my blood pressure under control but there were other things that it appeared to be affecting as well in a negative way.<br><img src=\"/images/Forks_Over_Knives_movie_poster.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Keep Calm and Go Vegan\"></p>\n<h2 id=\"The-Turning-Point-for-Me\"><a href=\"#The-Turning-Point-for-Me\" class=\"headerlink\" title=\"The Turning Point for Me\"></a>The Turning Point for Me</h2><p>During Thanksgiving weekend in November 2014, Mary (my wife) and I watched a documentary on Netflix called <a href=\"http://www.bing.com/search?q=netflix+forks+over+knives&amp;qs=AS&amp;pq=netflix+forks+over&amp;sc=1-18&amp;sp=1&amp;cvid=7FB1E1E182E14BAFAC1474507767F07F&amp;FORM=QBLH&amp;ghc=1\" target=\"_blank\" rel=\"noopener\"><strong>“Forks over Knives”</strong></a>, and at the end of that I vowed never to eat meat again and start moving towards a Vegan lifestyle.<br>The documentary is about two doctors one that came from the medical field and one from the science side of things and their adventure to unravelling the truth about how the food that we eat is related to health.  One of the biggest studies that has ever been done is called “The China Study” and is a 20 year study that examines the relationship between the consumption of animal products (including dairy) and chronic illnesses such as coronary heart disease, diabetes, breast cancer, prostate cancer and bowel cancer.</p>\n<p>Not only reducing these numbers but now that the toxic animal products were out of our system, our bodies would start to repair some of this damage that we have always been told could never be repairable naturally.</p>\n<h2 id=\"Getting-over-the-big-Lie\"><a href=\"#Getting-over-the-big-Lie\" class=\"headerlink\" title=\"Getting over the big Lie\"></a>Getting over the big Lie</h2><p>Yes there is a very large lie that we have all believed to be the truth because we assumed that it came from the medical field and sanctioned by the government to be the truth.  That being the daily nutritional guide.  This is the guide that told use to eat large amounts of meat and dairy products to give us energy and strong bones but this did not come from any medical study this came from the agriculture and dairy industries to sell more products.  </p>\n<p>Most of that animal protein that we take in our body rejects, there is very small amounts that it actually uses.  Now common sense would tell me if my body is rejecting all this animal based protein it is working extra hard and something is going to break down in the form of disease and other difficulties especially as we get older.  Oh wait, they now make a pill for that so we can continue to live the way we always have.  So now we are not only supporting an industry that never had that big of a market before but now we are spending billions of dollars every year to pharmaceutical companies as well in order to correct the mistakes we made with the things we eat.  One thing that I did learn in physics is that one action creates another and opposite reaction so this is not solving anything either just keep making it worse and now health care costs are through the roof with bodies that normally know how to heal themselves. </p>\n<h2 id=\"Now-for-the-Good-News\"><a href=\"#Now-for-the-Good-News\" class=\"headerlink\" title=\"Now for the Good News\"></a>Now for the Good News</h2><p>I know I got you all depressed and disappointed as I just dissed your favorite food and called it bad and toxic but there is a happy ending here.  I felt like you are right now for about five minutes and then decided to say “NO to Meat”.  If you get a chance I would encourage you to look up that documentary <a href=\"http://www.bing.com/search?q=netflix+forks+over+knives&amp;qs=AS&amp;pq=netflix+forks+over&amp;sc=1-18&amp;sp=1&amp;cvid=7FB1E1E182E14BAFAC1474507767F07F&amp;FORM=QBLH&amp;ghc=1\" target=\"_blank\" rel=\"noopener\"><strong>“Forks over Knives”</strong></a> as one other thing that disturbed me was the way they were harvesting these animals and called it ethical or within the approved guidelines.  These animals were under stress and that stress goes into the meat and you wonder why everyone seems so stressed, I know there is a relationship here.</p>\n<p>Anyway, the good news is my latest checkup with my doctor.  I am currently on no medication what so ever and my blood pressure numbers are very normal and very impressive for a guy my age.  I did a stress test and was able to reach my ideal heart rate easily and effortlessly and I feel great.  If I had any plaque buildup it is certainly repairing itself as I feel great.  Still can’t seem to lose the 15 pounds I have been working on for the last couple of years but I know I will accomplish that soon enough.  I am done with meat and all animal proteins as in milk, eggs, honey and I am going to live a long, long time and feel great.  Won’t you join me?</p>\n"},{"title":"Master Only in Production, an Improvement","date":"2017-07-05T14:15:07.000Z","_content":"{% img right /images/master_branch.png 300 300 \"Deployment from master\" %}\nSome time ago I wrote a blog post about [My New 3 Rules for Releases](/2016/09/My-New-3-Rules-for-Releases/) and one of those rules was to only release into production code that was built from the master branch.  In that solution I wrote a PowerShell script that would run first thing on the deployment to only go forward if the branch from the build came from master otherwise it would fail the deployment.  This gave me a guarantee that builds that did not come from master would never get deployed into Production.\n\nThis solution worked very well and guaranteed builds that did not come from master would ever get into Production, it was my safety net.  It still is and I will probably continue to use it but there has been an improvement in the process to make this even cleaner.  In my solution it was there as a safety net just to make sure that one day when I was clicking on things so fast and maybe doing more than one thing at a time that I did not cause this kind of error.\n### Artifact Condition\nThe new improvement is what is called an Artifact condition and it can be specific to each environment that you are deploying to.  In this case I have selected my Production environments and said to only trigger a deployment in my Production environment when the Dev deployment succeeded and the branch is master.  Of course it still includes all the approval and acceptance gates but the key to note here is if those first two conditions are not met it is not even going to trigger a Deployment to Production.  In the past when a code from a none master branch was successful in Dev or QA I would have to fail it some where along the way to stop the pipeline and in this case the pipeline just nicely ends.  Much, much cleaner.\n### How do you set it up\nThis is kind of tricky because in the VSTS Microsoft has just deployed a new Release Editor that seems to be missing this piece for now, not to worry as the new Release editor is still in preview and you can easily switch back and forth.  When you go to Releases and click on the Edit link and if the screen looks like the following, click on the Edit (Old Editor) link to switch back to the old style Release editor.\n{% asset_img NewReleaseEditor.png \"The New Release Editor is missing this functionality\" %}\n\nNext you select your Production environment and click on the ellipse button and select the Deployment conditions link.\n\n{% asset_img ConditionOption.png \"Selecting the Deployment conditions\" %}\n\n### Finally the Configuration Screen\nNow we are finally on the configuration page where all the real magic happens.  I have listed 5 simple steps that you follow to setup a deployment that will only trigger when the build came from the master branch and the previous build was successful. \n{% asset_img ConfigureScreen.png \"Configure for master branch only\" %}\n1. First make sure that you have the option set to trigger a deployment into production after a successful deployment of the previous environment.\n1. Next click on the new checkbox to check it as this sets some conditions to the new deployment\n1. Click the Add artifact condition big green plus button.\n1. Set the repository to only include the master branch as that condition\n1. Finally click the OK button to save all you adjustments.\n\nNow, you won't even be given the opportunity to promote the build into Production if it was not built from the master branch.","source":"_posts/Master-Only-in-Production-an-Improvement.md","raw":"---\ntitle: 'Master Only in Production, an Improvement'\ndate: 2017-07-05 07:15:07\ntags:\n    - DevOps\n    - ALM\n---\n{% img right /images/master_branch.png 300 300 \"Deployment from master\" %}\nSome time ago I wrote a blog post about [My New 3 Rules for Releases](/2016/09/My-New-3-Rules-for-Releases/) and one of those rules was to only release into production code that was built from the master branch.  In that solution I wrote a PowerShell script that would run first thing on the deployment to only go forward if the branch from the build came from master otherwise it would fail the deployment.  This gave me a guarantee that builds that did not come from master would never get deployed into Production.\n\nThis solution worked very well and guaranteed builds that did not come from master would ever get into Production, it was my safety net.  It still is and I will probably continue to use it but there has been an improvement in the process to make this even cleaner.  In my solution it was there as a safety net just to make sure that one day when I was clicking on things so fast and maybe doing more than one thing at a time that I did not cause this kind of error.\n### Artifact Condition\nThe new improvement is what is called an Artifact condition and it can be specific to each environment that you are deploying to.  In this case I have selected my Production environments and said to only trigger a deployment in my Production environment when the Dev deployment succeeded and the branch is master.  Of course it still includes all the approval and acceptance gates but the key to note here is if those first two conditions are not met it is not even going to trigger a Deployment to Production.  In the past when a code from a none master branch was successful in Dev or QA I would have to fail it some where along the way to stop the pipeline and in this case the pipeline just nicely ends.  Much, much cleaner.\n### How do you set it up\nThis is kind of tricky because in the VSTS Microsoft has just deployed a new Release Editor that seems to be missing this piece for now, not to worry as the new Release editor is still in preview and you can easily switch back and forth.  When you go to Releases and click on the Edit link and if the screen looks like the following, click on the Edit (Old Editor) link to switch back to the old style Release editor.\n{% asset_img NewReleaseEditor.png \"The New Release Editor is missing this functionality\" %}\n\nNext you select your Production environment and click on the ellipse button and select the Deployment conditions link.\n\n{% asset_img ConditionOption.png \"Selecting the Deployment conditions\" %}\n\n### Finally the Configuration Screen\nNow we are finally on the configuration page where all the real magic happens.  I have listed 5 simple steps that you follow to setup a deployment that will only trigger when the build came from the master branch and the previous build was successful. \n{% asset_img ConfigureScreen.png \"Configure for master branch only\" %}\n1. First make sure that you have the option set to trigger a deployment into production after a successful deployment of the previous environment.\n1. Next click on the new checkbox to check it as this sets some conditions to the new deployment\n1. Click the Add artifact condition big green plus button.\n1. Set the repository to only include the master branch as that condition\n1. Finally click the OK button to save all you adjustments.\n\nNow, you won't even be given the opportunity to promote the build into Production if it was not built from the master branch.","slug":"Master-Only-in-Production-an-Improvement","published":1,"updated":"2020-01-05T00:36:05.019Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfm000ds4ufqgbp44zl","content":"<img src=\"/images/master_branch.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Deployment from master\">\n<p>Some time ago I wrote a blog post about <a href=\"/2016/09/My-New-3-Rules-for-Releases/\">My New 3 Rules for Releases</a> and one of those rules was to only release into production code that was built from the master branch.  In that solution I wrote a PowerShell script that would run first thing on the deployment to only go forward if the branch from the build came from master otherwise it would fail the deployment.  This gave me a guarantee that builds that did not come from master would never get deployed into Production.</p>\n<p>This solution worked very well and guaranteed builds that did not come from master would ever get into Production, it was my safety net.  It still is and I will probably continue to use it but there has been an improvement in the process to make this even cleaner.  In my solution it was there as a safety net just to make sure that one day when I was clicking on things so fast and maybe doing more than one thing at a time that I did not cause this kind of error.</p>\n<h3 id=\"Artifact-Condition\"><a href=\"#Artifact-Condition\" class=\"headerlink\" title=\"Artifact Condition\"></a>Artifact Condition</h3><p>The new improvement is what is called an Artifact condition and it can be specific to each environment that you are deploying to.  In this case I have selected my Production environments and said to only trigger a deployment in my Production environment when the Dev deployment succeeded and the branch is master.  Of course it still includes all the approval and acceptance gates but the key to note here is if those first two conditions are not met it is not even going to trigger a Deployment to Production.  In the past when a code from a none master branch was successful in Dev or QA I would have to fail it some where along the way to stop the pipeline and in this case the pipeline just nicely ends.  Much, much cleaner.</p>\n<h3 id=\"How-do-you-set-it-up\"><a href=\"#How-do-you-set-it-up\" class=\"headerlink\" title=\"How do you set it up\"></a>How do you set it up</h3><p>This is kind of tricky because in the VSTS Microsoft has just deployed a new Release Editor that seems to be missing this piece for now, not to worry as the new Release editor is still in preview and you can easily switch back and forth.  When you go to Releases and click on the Edit link and if the screen looks like the following, click on the Edit (Old Editor) link to switch back to the old style Release editor.<br><img src=\"/2017/07/Master-Only-in-Production-an-Improvement/NewReleaseEditor.png\" title=\"The New Release Editor is missing this functionality\"></p>\n<p>Next you select your Production environment and click on the ellipse button and select the Deployment conditions link.</p>\n<img src=\"/2017/07/Master-Only-in-Production-an-Improvement/ConditionOption.png\" title=\"Selecting the Deployment conditions\">\n<h3 id=\"Finally-the-Configuration-Screen\"><a href=\"#Finally-the-Configuration-Screen\" class=\"headerlink\" title=\"Finally the Configuration Screen\"></a>Finally the Configuration Screen</h3><p>Now we are finally on the configuration page where all the real magic happens.  I have listed 5 simple steps that you follow to setup a deployment that will only trigger when the build came from the master branch and the previous build was successful.<br><img src=\"/2017/07/Master-Only-in-Production-an-Improvement/ConfigureScreen.png\" title=\"Configure for master branch only\"></p>\n<ol>\n<li>First make sure that you have the option set to trigger a deployment into production after a successful deployment of the previous environment.</li>\n<li>Next click on the new checkbox to check it as this sets some conditions to the new deployment</li>\n<li>Click the Add artifact condition big green plus button.</li>\n<li>Set the repository to only include the master branch as that condition</li>\n<li>Finally click the OK button to save all you adjustments.</li>\n</ol>\n<p>Now, you won’t even be given the opportunity to promote the build into Production if it was not built from the master branch.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/master_branch.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Deployment from master\">\n<p>Some time ago I wrote a blog post about <a href=\"/2016/09/My-New-3-Rules-for-Releases/\">My New 3 Rules for Releases</a> and one of those rules was to only release into production code that was built from the master branch.  In that solution I wrote a PowerShell script that would run first thing on the deployment to only go forward if the branch from the build came from master otherwise it would fail the deployment.  This gave me a guarantee that builds that did not come from master would never get deployed into Production.</p>\n<p>This solution worked very well and guaranteed builds that did not come from master would ever get into Production, it was my safety net.  It still is and I will probably continue to use it but there has been an improvement in the process to make this even cleaner.  In my solution it was there as a safety net just to make sure that one day when I was clicking on things so fast and maybe doing more than one thing at a time that I did not cause this kind of error.</p>\n<h3 id=\"Artifact-Condition\"><a href=\"#Artifact-Condition\" class=\"headerlink\" title=\"Artifact Condition\"></a>Artifact Condition</h3><p>The new improvement is what is called an Artifact condition and it can be specific to each environment that you are deploying to.  In this case I have selected my Production environments and said to only trigger a deployment in my Production environment when the Dev deployment succeeded and the branch is master.  Of course it still includes all the approval and acceptance gates but the key to note here is if those first two conditions are not met it is not even going to trigger a Deployment to Production.  In the past when a code from a none master branch was successful in Dev or QA I would have to fail it some where along the way to stop the pipeline and in this case the pipeline just nicely ends.  Much, much cleaner.</p>\n<h3 id=\"How-do-you-set-it-up\"><a href=\"#How-do-you-set-it-up\" class=\"headerlink\" title=\"How do you set it up\"></a>How do you set it up</h3><p>This is kind of tricky because in the VSTS Microsoft has just deployed a new Release Editor that seems to be missing this piece for now, not to worry as the new Release editor is still in preview and you can easily switch back and forth.  When you go to Releases and click on the Edit link and if the screen looks like the following, click on the Edit (Old Editor) link to switch back to the old style Release editor.<br><img src=\"/2017/07/Master-Only-in-Production-an-Improvement/NewReleaseEditor.png\" title=\"The New Release Editor is missing this functionality\"></p>\n<p>Next you select your Production environment and click on the ellipse button and select the Deployment conditions link.</p>\n<img src=\"/2017/07/Master-Only-in-Production-an-Improvement/ConditionOption.png\" title=\"Selecting the Deployment conditions\">\n<h3 id=\"Finally-the-Configuration-Screen\"><a href=\"#Finally-the-Configuration-Screen\" class=\"headerlink\" title=\"Finally the Configuration Screen\"></a>Finally the Configuration Screen</h3><p>Now we are finally on the configuration page where all the real magic happens.  I have listed 5 simple steps that you follow to setup a deployment that will only trigger when the build came from the master branch and the previous build was successful.<br><img src=\"/2017/07/Master-Only-in-Production-an-Improvement/ConfigureScreen.png\" title=\"Configure for master branch only\"></p>\n<ol>\n<li>First make sure that you have the option set to trigger a deployment into production after a successful deployment of the previous environment.</li>\n<li>Next click on the new checkbox to check it as this sets some conditions to the new deployment</li>\n<li>Click the Add artifact condition big green plus button.</li>\n<li>Set the repository to only include the master branch as that condition</li>\n<li>Finally click the OK button to save all you adjustments.</li>\n</ol>\n<p>Now, you won’t even be given the opportunity to promote the build into Production if it was not built from the master branch.</p>\n"},{"title":"My Experience with Git Sub-modules","date":"2016-09-13T03:43:37.000Z","_content":"{% img left /images/Lumina-950XL.png 200 200 \"Lumina 950 XL\" %}\nI just replaced my phone with a new Microsoft Lumina 950 XL which is a great phone.  In my usual fashion of checking out the new features of my phone I wanted to see how my web sites looked.\n\nThe operating system of this phone is the Mobil version of Windows 10 and of course is using the new browser called Edge.  Well it seems that my blog did not look good at all on this new platform and was in fact not even close to being workable.  Even though I had the fonts set to the smallest setting, what was displayed were hugh letters so hardly any words fit on a line and was just crazy looking.  However, I noticed that other web sites looked just fine especially the ones that I recognized and truely being built around the bootstrapper framework.  \n\nI was also surprised as to how many other web sites look badly in this browser with the same problems that I had.  Anyway I may address some of that in a later post but right now, what I wanted to find out is if I changed the syle of this blog would it solve my problem.  If I just changed the theme or something could it be possible that my site would look great again.  This was all very surprising to me as I had tested the responsiveness of this site and it always looked good, just don't know why my new phone made it look so bad.\n\n## New Theme, based on Bootstrapper\nLooking for different themes for Hexo was not a problem, there are many of them and most of them are even free.  I am really loving the work that I have done working with the Bootstrapper Framework so when I found a Hexo theme that was built around the Bootsrapper Framework, you know I just had to try it.  Well this theme looked great a lot simpler looking theme than what I was using which was really the default theme with a few customizations.  The new theme was also open source and in another git hub repository.  The instructions said to use some sub-module mumbo jumbo to pull the source into the build.  Well now I was curious as there was something that I saw on the build definition when working with git repositories, a simple check box that says include sub-modules.  Looks like it is time to find out was git sub-modules is all about.\n\nWelcome to another git light bulb moment.\n{% img right /images/git-logo.jpg 200 200 \"Git\" %}\n## What is a git sub module.\nThe concept of a git sub module is a whole new concept for me as a developer that has been using for the most part, a centralized version control system of one sort or another for most of my career.  I then looked up the help files for these git sub modules and read a few blog posts, and it can get quite complicated but rather then going through all that it can do let me explain how this worked for me to quickly update the theme for my blog.  In short, a git sub module is another git repository that may be used to prove source for certain parts for yet another git repository without being a part of that repository.  \nIn other words, instead of having to add all that source from this other git repository and adding it to my existing Blog git respoitory it instead has a reference to that repository and will pull down that code so that I can use it during my build both locally and on the build machine.  And the crazy thing is it makes it really easy for me to keep up with the latest changes because I don't have to manage that it is pulling the latest from this other repository through this sub module.\n\nI started from my local git repository and because I wanted this library in my themes folder I navigated to that folder as this is where hexo is going to expect to see themes.  Then using git-posh (PowerShell module for working with git)  I entered the following command.\n```\ngit submodule add https://github.com/cgmartin/hexo-theme-bootstrap-blog.git\n```\nThis created the folder hexo-theme-bootstrap-blog and downloaded all the git repository into my local workspace and added a file called .gitmodules at the root of my Blog bit repository.  Looking\ninside the file, it contains the following contents:\n```\n[submodule \"themes/bootstrap-blog\"]\n\tpath = themes/bootstrap-blog\n\turl = https://github.com/cgmartin/hexo-theme-bootstrap-blog.git\n```\nWhen I added these changes to my staging area by using the add command:\n\n*```\ngit add .\n```*\n\nIt only added the .gitmodules file and of course the push only added that file as well to my remote git repository in TFS.  Looking at the code of this Blog repository in TFS there is no evidence that this theme has been added to the repository, because it has not.  Instead there is this file that tells the build machine and any other local git repositories where to find this theme and to get it.  The only thing left was to change my _config.yml file to tell it to use the bootstrap-blog theme and run my builds.  Everything works like a charm. \n\nI really don't think that there is any way that you can do something like this using centralized version control.  Humm, makes me wonder, where else can I use git sub modules?","source":"_posts/My-Experience-with-Git-Sub-modules.md","raw":"title: My Experience with Git Sub-modules\ndate: 2016-09-12 20:43:37\ntags:\n- git\n- Blogs\n---\n{% img left /images/Lumina-950XL.png 200 200 \"Lumina 950 XL\" %}\nI just replaced my phone with a new Microsoft Lumina 950 XL which is a great phone.  In my usual fashion of checking out the new features of my phone I wanted to see how my web sites looked.\n\nThe operating system of this phone is the Mobil version of Windows 10 and of course is using the new browser called Edge.  Well it seems that my blog did not look good at all on this new platform and was in fact not even close to being workable.  Even though I had the fonts set to the smallest setting, what was displayed were hugh letters so hardly any words fit on a line and was just crazy looking.  However, I noticed that other web sites looked just fine especially the ones that I recognized and truely being built around the bootstrapper framework.  \n\nI was also surprised as to how many other web sites look badly in this browser with the same problems that I had.  Anyway I may address some of that in a later post but right now, what I wanted to find out is if I changed the syle of this blog would it solve my problem.  If I just changed the theme or something could it be possible that my site would look great again.  This was all very surprising to me as I had tested the responsiveness of this site and it always looked good, just don't know why my new phone made it look so bad.\n\n## New Theme, based on Bootstrapper\nLooking for different themes for Hexo was not a problem, there are many of them and most of them are even free.  I am really loving the work that I have done working with the Bootstrapper Framework so when I found a Hexo theme that was built around the Bootsrapper Framework, you know I just had to try it.  Well this theme looked great a lot simpler looking theme than what I was using which was really the default theme with a few customizations.  The new theme was also open source and in another git hub repository.  The instructions said to use some sub-module mumbo jumbo to pull the source into the build.  Well now I was curious as there was something that I saw on the build definition when working with git repositories, a simple check box that says include sub-modules.  Looks like it is time to find out was git sub-modules is all about.\n\nWelcome to another git light bulb moment.\n{% img right /images/git-logo.jpg 200 200 \"Git\" %}\n## What is a git sub module.\nThe concept of a git sub module is a whole new concept for me as a developer that has been using for the most part, a centralized version control system of one sort or another for most of my career.  I then looked up the help files for these git sub modules and read a few blog posts, and it can get quite complicated but rather then going through all that it can do let me explain how this worked for me to quickly update the theme for my blog.  In short, a git sub module is another git repository that may be used to prove source for certain parts for yet another git repository without being a part of that repository.  \nIn other words, instead of having to add all that source from this other git repository and adding it to my existing Blog git respoitory it instead has a reference to that repository and will pull down that code so that I can use it during my build both locally and on the build machine.  And the crazy thing is it makes it really easy for me to keep up with the latest changes because I don't have to manage that it is pulling the latest from this other repository through this sub module.\n\nI started from my local git repository and because I wanted this library in my themes folder I navigated to that folder as this is where hexo is going to expect to see themes.  Then using git-posh (PowerShell module for working with git)  I entered the following command.\n```\ngit submodule add https://github.com/cgmartin/hexo-theme-bootstrap-blog.git\n```\nThis created the folder hexo-theme-bootstrap-blog and downloaded all the git repository into my local workspace and added a file called .gitmodules at the root of my Blog bit repository.  Looking\ninside the file, it contains the following contents:\n```\n[submodule \"themes/bootstrap-blog\"]\n\tpath = themes/bootstrap-blog\n\turl = https://github.com/cgmartin/hexo-theme-bootstrap-blog.git\n```\nWhen I added these changes to my staging area by using the add command:\n\n*```\ngit add .\n```*\n\nIt only added the .gitmodules file and of course the push only added that file as well to my remote git repository in TFS.  Looking at the code of this Blog repository in TFS there is no evidence that this theme has been added to the repository, because it has not.  Instead there is this file that tells the build machine and any other local git repositories where to find this theme and to get it.  The only thing left was to change my _config.yml file to tell it to use the bootstrap-blog theme and run my builds.  Everything works like a charm. \n\nI really don't think that there is any way that you can do something like this using centralized version control.  Humm, makes me wonder, where else can I use git sub modules?","slug":"My-Experience-with-Git-Sub-modules","published":1,"updated":"2020-01-05T00:36:05.027Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfo000es4ufuenn9q28","content":"<img src=\"/images/Lumina-950XL.png\" class=\"left\" width=\"200\" height=\"200\" title=\"Lumina 950 XL\">\n<p>I just replaced my phone with a new Microsoft Lumina 950 XL which is a great phone.  In my usual fashion of checking out the new features of my phone I wanted to see how my web sites looked.</p>\n<p>The operating system of this phone is the Mobil version of Windows 10 and of course is using the new browser called Edge.  Well it seems that my blog did not look good at all on this new platform and was in fact not even close to being workable.  Even though I had the fonts set to the smallest setting, what was displayed were hugh letters so hardly any words fit on a line and was just crazy looking.  However, I noticed that other web sites looked just fine especially the ones that I recognized and truely being built around the bootstrapper framework.  </p>\n<p>I was also surprised as to how many other web sites look badly in this browser with the same problems that I had.  Anyway I may address some of that in a later post but right now, what I wanted to find out is if I changed the syle of this blog would it solve my problem.  If I just changed the theme or something could it be possible that my site would look great again.  This was all very surprising to me as I had tested the responsiveness of this site and it always looked good, just don’t know why my new phone made it look so bad.</p>\n<h2 id=\"New-Theme-based-on-Bootstrapper\"><a href=\"#New-Theme-based-on-Bootstrapper\" class=\"headerlink\" title=\"New Theme, based on Bootstrapper\"></a>New Theme, based on Bootstrapper</h2><p>Looking for different themes for Hexo was not a problem, there are many of them and most of them are even free.  I am really loving the work that I have done working with the Bootstrapper Framework so when I found a Hexo theme that was built around the Bootsrapper Framework, you know I just had to try it.  Well this theme looked great a lot simpler looking theme than what I was using which was really the default theme with a few customizations.  The new theme was also open source and in another git hub repository.  The instructions said to use some sub-module mumbo jumbo to pull the source into the build.  Well now I was curious as there was something that I saw on the build definition when working with git repositories, a simple check box that says include sub-modules.  Looks like it is time to find out was git sub-modules is all about.</p>\n<p>Welcome to another git light bulb moment.<br><img src=\"/images/git-logo.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Git\"></p>\n<h2 id=\"What-is-a-git-sub-module\"><a href=\"#What-is-a-git-sub-module\" class=\"headerlink\" title=\"What is a git sub module.\"></a>What is a git sub module.</h2><p>The concept of a git sub module is a whole new concept for me as a developer that has been using for the most part, a centralized version control system of one sort or another for most of my career.  I then looked up the help files for these git sub modules and read a few blog posts, and it can get quite complicated but rather then going through all that it can do let me explain how this worked for me to quickly update the theme for my blog.  In short, a git sub module is another git repository that may be used to prove source for certain parts for yet another git repository without being a part of that repository.<br>In other words, instead of having to add all that source from this other git repository and adding it to my existing Blog git respoitory it instead has a reference to that repository and will pull down that code so that I can use it during my build both locally and on the build machine.  And the crazy thing is it makes it really easy for me to keep up with the latest changes because I don’t have to manage that it is pulling the latest from this other repository through this sub module.</p>\n<p>I started from my local git repository and because I wanted this library in my themes folder I navigated to that folder as this is where hexo is going to expect to see themes.  Then using git-posh (PowerShell module for working with git)  I entered the following command.<br><figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git submodule <span class=\"built_in\">add</span> http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/cgmartin/hexo-theme-bootstrap-blog.git</span><br></pre></td></tr></table></figure></p>\n<p>This created the folder hexo-theme-bootstrap-blog and downloaded all the git repository into my local workspace and added a file called .gitmodules at the root of my Blog bit repository.  Looking<br>inside the file, it contains the following contents:<br><figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"keyword\">submodule </span><span class=\"string\">\"themes/bootstrap-blog\"</span>]</span><br><span class=\"line\">\tpath = themes/<span class=\"keyword\">bootstrap-blog</span></span><br><span class=\"line\"><span class=\"keyword\">\t</span>url = https://github.com/cgmartin/hexo-theme-<span class=\"keyword\">bootstrap-blog.git</span></span><br></pre></td></tr></table></figure></p>\n<p>When I added these changes to my staging area by using the add command:</p>\n<p><em><code>git add .</code></em></p>\n<p>It only added the .gitmodules file and of course the push only added that file as well to my remote git repository in TFS.  Looking at the code of this Blog repository in TFS there is no evidence that this theme has been added to the repository, because it has not.  Instead there is this file that tells the build machine and any other local git repositories where to find this theme and to get it.  The only thing left was to change my _config.yml file to tell it to use the bootstrap-blog theme and run my builds.  Everything works like a charm. </p>\n<p>I really don’t think that there is any way that you can do something like this using centralized version control.  Humm, makes me wonder, where else can I use git sub modules?</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/Lumina-950XL.png\" class=\"left\" width=\"200\" height=\"200\" title=\"Lumina 950 XL\">\n<p>I just replaced my phone with a new Microsoft Lumina 950 XL which is a great phone.  In my usual fashion of checking out the new features of my phone I wanted to see how my web sites looked.</p>\n<p>The operating system of this phone is the Mobil version of Windows 10 and of course is using the new browser called Edge.  Well it seems that my blog did not look good at all on this new platform and was in fact not even close to being workable.  Even though I had the fonts set to the smallest setting, what was displayed were hugh letters so hardly any words fit on a line and was just crazy looking.  However, I noticed that other web sites looked just fine especially the ones that I recognized and truely being built around the bootstrapper framework.  </p>\n<p>I was also surprised as to how many other web sites look badly in this browser with the same problems that I had.  Anyway I may address some of that in a later post but right now, what I wanted to find out is if I changed the syle of this blog would it solve my problem.  If I just changed the theme or something could it be possible that my site would look great again.  This was all very surprising to me as I had tested the responsiveness of this site and it always looked good, just don’t know why my new phone made it look so bad.</p>\n<h2 id=\"New-Theme-based-on-Bootstrapper\"><a href=\"#New-Theme-based-on-Bootstrapper\" class=\"headerlink\" title=\"New Theme, based on Bootstrapper\"></a>New Theme, based on Bootstrapper</h2><p>Looking for different themes for Hexo was not a problem, there are many of them and most of them are even free.  I am really loving the work that I have done working with the Bootstrapper Framework so when I found a Hexo theme that was built around the Bootsrapper Framework, you know I just had to try it.  Well this theme looked great a lot simpler looking theme than what I was using which was really the default theme with a few customizations.  The new theme was also open source and in another git hub repository.  The instructions said to use some sub-module mumbo jumbo to pull the source into the build.  Well now I was curious as there was something that I saw on the build definition when working with git repositories, a simple check box that says include sub-modules.  Looks like it is time to find out was git sub-modules is all about.</p>\n<p>Welcome to another git light bulb moment.<br><img src=\"/images/git-logo.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Git\"></p>\n<h2 id=\"What-is-a-git-sub-module\"><a href=\"#What-is-a-git-sub-module\" class=\"headerlink\" title=\"What is a git sub module.\"></a>What is a git sub module.</h2><p>The concept of a git sub module is a whole new concept for me as a developer that has been using for the most part, a centralized version control system of one sort or another for most of my career.  I then looked up the help files for these git sub modules and read a few blog posts, and it can get quite complicated but rather then going through all that it can do let me explain how this worked for me to quickly update the theme for my blog.  In short, a git sub module is another git repository that may be used to prove source for certain parts for yet another git repository without being a part of that repository.<br>In other words, instead of having to add all that source from this other git repository and adding it to my existing Blog git respoitory it instead has a reference to that repository and will pull down that code so that I can use it during my build both locally and on the build machine.  And the crazy thing is it makes it really easy for me to keep up with the latest changes because I don’t have to manage that it is pulling the latest from this other repository through this sub module.</p>\n<p>I started from my local git repository and because I wanted this library in my themes folder I navigated to that folder as this is where hexo is going to expect to see themes.  Then using git-posh (PowerShell module for working with git)  I entered the following command.<br><figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git submodule <span class=\"built_in\">add</span> http<span class=\"variable\">s:</span>//github.<span class=\"keyword\">com</span>/cgmartin/hexo-theme-bootstrap-blog.git</span><br></pre></td></tr></table></figure></p>\n<p>This created the folder hexo-theme-bootstrap-blog and downloaded all the git repository into my local workspace and added a file called .gitmodules at the root of my Blog bit repository.  Looking<br>inside the file, it contains the following contents:<br><figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"keyword\">submodule </span><span class=\"string\">\"themes/bootstrap-blog\"</span>]</span><br><span class=\"line\">\tpath = themes/<span class=\"keyword\">bootstrap-blog</span></span><br><span class=\"line\"><span class=\"keyword\">\t</span>url = https://github.com/cgmartin/hexo-theme-<span class=\"keyword\">bootstrap-blog.git</span></span><br></pre></td></tr></table></figure></p>\n<p>When I added these changes to my staging area by using the add command:</p>\n<p><em><code>git add .</code></em></p>\n<p>It only added the .gitmodules file and of course the push only added that file as well to my remote git repository in TFS.  Looking at the code of this Blog repository in TFS there is no evidence that this theme has been added to the repository, because it has not.  Instead there is this file that tells the build machine and any other local git repositories where to find this theme and to get it.  The only thing left was to change my _config.yml file to tell it to use the bootstrap-blog theme and run my builds.  Everything works like a charm. </p>\n<p>I really don’t think that there is any way that you can do something like this using centralized version control.  Humm, makes me wonder, where else can I use git sub modules?</p>\n"},{"title":"My New 3 Rules for Releases","date":"2016-09-24T00:08:25.000Z","_content":"{% img left /images/ALM.png 250 200 \"Application Lifecycle\" %}\nEveryone of my products have an automated build and a properly managed release pipeline.  At the time I just thought business as usual as I was always on my way to having a well performing DevOps operation in my personal development efforts.  Well something happened in the way that I started approaching things which you don't really plan, things will just start to happen when you get into a situation where everything is automated or at least they should and that is what this post is about.\n\n## I don't have to wait\nOne of the first things that I did notice was that I didn't have the feeling like I needed to wait until this big plan of mine to do a release.  In the past I was using the Epic work item to plan out the finished features the I would need to complete to get the next release out.  I even noticed before I had all these steps automated that plans would change quite often.  The priorities and the well-meaning releases would take a turn to become something different like finding a critical bug that could affect the current customers.  I would want to release that bug or feature as quickly as possible.\n\nBefore everything was automated, these things bothered me but there wasn't an easy way to just get the release out there as there were still enough manual steps that you want to limit these.  However, now there is no reason to get a build that has a complete bug fix or feature and push it down the pipeline and get it released into production.    However, if this rush to production is now suddenly available to me isn't there the possibility that something that wasn't quite ready get into production by accident?  That is why I came up with these 3 new rules that I set for myself that need to be followed before the build can be pushed into production.\n\n## My New 3 Rules for Releases\n{% img right /images/3simplerules.jpg 200 200 \"Three Rules\" %}\n1. Don't allow any builds that came from any branch other than Master (git) or Main (tfvc) into production.  If it is not Master then it should just be rejected in the deployment steps.\n1. A build that is released with success into Production, will be locked indefinitely with regards to the retention policy.\n1. The build number must incremented any time that we successfully released into production.\n\nWhat follows are the ways that I automated these 3 rules and made them part of my operation.  Now there is never a fear that something might get deployed into production that really should not.  I can push things into production when it is important to do so and sometimes I might delay a release because there is no big benefit and saves the customers from having to download and install a release that could be packaged up with a later release.  The point being that a release can be made any time it needs to and no more of this long range planning which never happens the way you expected anyway.\n\n## No Builds into Production that did not come from Master\nAs you may have gathered from some of my earlier posts, my personal projects have pretty much all landed up in git repositories that are remotely hosted in Visual Studio Team Services which is Microsoft's cloud implementation of TFS.  With that I am following a very typical git like workflow.  Every Product Backlog Item or Bug starts with a new feature or bug branch.  This is really nice as it gives me a nice level of isolation and knowing that my work will not affect the working code.  It also gives me the flexibility to fix an important bug or PBI that changed in priority and know that the code I tore apart will not affect the outcome.\n\nThis also gives me the opportunity to test this code, confirm that it is complete and give it one more look through as the only way code from a branch can get into master is through a pull request.  The pull request has a number of options with it as well such as squashing all the commits into a single commit (so I get a very clean and clear way of answering the question, how did you add this feature.) and deleting the branch after the merge.\n\nMaster is always the branch that represents production or ready for production.  I wanted the code only to come from master because this is where all the branches come back to.  Having a rule like this makes sure that the merge will always happen and that nothing gets left out.  I have seen some very complicated branching structures when working with clients and something that I have seen quite often is that branches did not always get merged back to where they should.  There would be these complicated discussions about where the code that goes to production should really come from.  Here I have eliminated all the complexity by having a rule that says you can't push a build that did not come from master into Production.\n\nNow, how do you enforce this automatically?  Well I could not find a task that would help me with this but I did know how I could do this with a simple PowerShell script.\n```\n    $branch = \"$Env:BUILD_SOURCEBRANCHNAME\"\n\n    if ($branch -ne \"master\") {\n        Write-Host \"Cannot deploy builds from the $branch branch into Production\" \n        Write-Error (\"Can only deploy builds from the master branch into Production\")\n        exit 1\n    }\n    else {\n        Write-Host \"Carry on, this build qualifies for a deployment into Production\"\n    }\n\n``` \n\n{% asset_img MasterBranchOnly.png \"Implementing the Master Branch Only Rule\" %}\n\nUsing a PowerShell task at the top of a release for the Production environment as an inline script to implement this rule.  If for some reason I pushed a build that came from some other branch this task will fail and not go any farther.  In my world I typically have one build definition that is by default pointing to the master branch but I override that when I am working on one of my feature branches to get feedback on how the code is building and deploying.  Which I really like because I am using the very same build and deployment scripts that I would use when going into production.  So you can see how a build from one of these branches could accidentally get into production if I did not have this very elegant rule enforcement.\n\n## Locking A Released Build\nDuring the process of development, there are several builds and deployments are happening all the time.  However, most of these I don't really care about as their only real value is to give feedback that the application was still able to build and deploy as it always has.  So one thing I never want to do is to lock down a build that came from anything other than the master branch.  I used to have a task on the build definition that would lock down any build that was created from the master branch.  However this is not always a good rule to live by either as there have been times when the deployment of a master branch did fail while going through the release pipeline and other times it might not have failed but there was a conscious decision to hold off on a release but was merged into master to be added with a few more features.\n\n{% img left /images/BuildTasks.png 300 300 \"Locking a Release\" %}\n\nWhat I needed was a task that would update the build with an infinite lock on the build when ever it was successfully deployed into Production.  For that task I did find one in the Microsoft Market Place that did exactly that.  This task is part of a small collection of BuildTasks written by Richard Fennell who is a long time ALM MVP.  In the Market Place it is called \"Build Updating Tasks\" and if you search for that, \"Richard Fennel\" or \"Black Marble\" I am sure you will find it.\n\nI have this task near the end of my Prod deployment and set the Build selection mode to \"On primary build artifact\" and done.  Works like a charm, when I deploy to production and it was successful it will find that build and set its retention to keep forever.  I no longer have to think about making sure I don't lose those builds that are in Production.\n\n## Increment the Build number\nThis rule has really allowed me to move freely into my new DevOps approach and no longer have this dependancy of the long planned release which I explained earlier did not ever get released the way I thought that it would.  Things and priorities change, that is life.  In my build definition I have a set of variables. One called the MajorMinorNumber and the other is the BuildNumber.  These combined with the TFS revision number on the end gives me the version number of my release.  So in the build definition under the general sub tab my Build number format looks similar to:\n\n    Product-v$(MajorMinorNumber).$(BuildNumber)$(rev:.r)\n\nNow lets break this down a little.  The MajorMinorNumber change rarely as they would represent big changes in the application.  This follows something close to [semantic versioning](http://semver.org/) in that if there is going to be a breaking change I would update the Major Number, if there was going to be a big change but would remain backwards compatible then the minor number would be incremented.  In the case where I am just adding some new features that are additive to the application or fixing some bugs then the build number would be incremented.  The 4th number which is the revision is left for TFS to make guarantee that we always have a unqiue build number.\n\nIn the past I have been known for using a date like version number for applications that I didn't think would really matter.  However, I have even noticed with them that there is some very important information that gets lost.  If I had a daily build going on and so the day part of the version number would increment everyday even though I might still be working on the same PBI or Bug.  Instead I want to have a new build number after I have a successful deployment into Production.  This means that I have customers out there who may have upgraded to a newer version and with that I can even produce some release notes as to what was part of that release.  But I did not want to go and increment the build number in the build everytime this happened, I wanted this to be automatic as well.\n\n{% img right /images/BuildTasks.png 300 300 \"Locking a Release\" %}\n\nThe solution for this is using the another special task that is part of the last extension that we installled.  There is a task called \"Update Build Variable\" and I have this as the very last task for the deployment into my Prod Environment.  Very simple to setup, the Build selection mode is: \"Only primary build artifact\" the Variable to update:  \"BuildNumber\" and the Update mode is \"Autoincrement\".\n\nNow after a successful deployment into Production and my build number is incremented and ready to go for either my next long planned set of feature or getting out that really quick important fix or special feature that I just needed to get out there.   ","source":"_posts/My-New-3-Rules-for-Releases.md","raw":"title: My New 3 Rules for Releases\ndate: 2016-09-23 17:08:25\ntags:\n- DevOps\n- ALM\n- PowerShell\n---\n{% img left /images/ALM.png 250 200 \"Application Lifecycle\" %}\nEveryone of my products have an automated build and a properly managed release pipeline.  At the time I just thought business as usual as I was always on my way to having a well performing DevOps operation in my personal development efforts.  Well something happened in the way that I started approaching things which you don't really plan, things will just start to happen when you get into a situation where everything is automated or at least they should and that is what this post is about.\n\n## I don't have to wait\nOne of the first things that I did notice was that I didn't have the feeling like I needed to wait until this big plan of mine to do a release.  In the past I was using the Epic work item to plan out the finished features the I would need to complete to get the next release out.  I even noticed before I had all these steps automated that plans would change quite often.  The priorities and the well-meaning releases would take a turn to become something different like finding a critical bug that could affect the current customers.  I would want to release that bug or feature as quickly as possible.\n\nBefore everything was automated, these things bothered me but there wasn't an easy way to just get the release out there as there were still enough manual steps that you want to limit these.  However, now there is no reason to get a build that has a complete bug fix or feature and push it down the pipeline and get it released into production.    However, if this rush to production is now suddenly available to me isn't there the possibility that something that wasn't quite ready get into production by accident?  That is why I came up with these 3 new rules that I set for myself that need to be followed before the build can be pushed into production.\n\n## My New 3 Rules for Releases\n{% img right /images/3simplerules.jpg 200 200 \"Three Rules\" %}\n1. Don't allow any builds that came from any branch other than Master (git) or Main (tfvc) into production.  If it is not Master then it should just be rejected in the deployment steps.\n1. A build that is released with success into Production, will be locked indefinitely with regards to the retention policy.\n1. The build number must incremented any time that we successfully released into production.\n\nWhat follows are the ways that I automated these 3 rules and made them part of my operation.  Now there is never a fear that something might get deployed into production that really should not.  I can push things into production when it is important to do so and sometimes I might delay a release because there is no big benefit and saves the customers from having to download and install a release that could be packaged up with a later release.  The point being that a release can be made any time it needs to and no more of this long range planning which never happens the way you expected anyway.\n\n## No Builds into Production that did not come from Master\nAs you may have gathered from some of my earlier posts, my personal projects have pretty much all landed up in git repositories that are remotely hosted in Visual Studio Team Services which is Microsoft's cloud implementation of TFS.  With that I am following a very typical git like workflow.  Every Product Backlog Item or Bug starts with a new feature or bug branch.  This is really nice as it gives me a nice level of isolation and knowing that my work will not affect the working code.  It also gives me the flexibility to fix an important bug or PBI that changed in priority and know that the code I tore apart will not affect the outcome.\n\nThis also gives me the opportunity to test this code, confirm that it is complete and give it one more look through as the only way code from a branch can get into master is through a pull request.  The pull request has a number of options with it as well such as squashing all the commits into a single commit (so I get a very clean and clear way of answering the question, how did you add this feature.) and deleting the branch after the merge.\n\nMaster is always the branch that represents production or ready for production.  I wanted the code only to come from master because this is where all the branches come back to.  Having a rule like this makes sure that the merge will always happen and that nothing gets left out.  I have seen some very complicated branching structures when working with clients and something that I have seen quite often is that branches did not always get merged back to where they should.  There would be these complicated discussions about where the code that goes to production should really come from.  Here I have eliminated all the complexity by having a rule that says you can't push a build that did not come from master into Production.\n\nNow, how do you enforce this automatically?  Well I could not find a task that would help me with this but I did know how I could do this with a simple PowerShell script.\n```\n    $branch = \"$Env:BUILD_SOURCEBRANCHNAME\"\n\n    if ($branch -ne \"master\") {\n        Write-Host \"Cannot deploy builds from the $branch branch into Production\" \n        Write-Error (\"Can only deploy builds from the master branch into Production\")\n        exit 1\n    }\n    else {\n        Write-Host \"Carry on, this build qualifies for a deployment into Production\"\n    }\n\n``` \n\n{% asset_img MasterBranchOnly.png \"Implementing the Master Branch Only Rule\" %}\n\nUsing a PowerShell task at the top of a release for the Production environment as an inline script to implement this rule.  If for some reason I pushed a build that came from some other branch this task will fail and not go any farther.  In my world I typically have one build definition that is by default pointing to the master branch but I override that when I am working on one of my feature branches to get feedback on how the code is building and deploying.  Which I really like because I am using the very same build and deployment scripts that I would use when going into production.  So you can see how a build from one of these branches could accidentally get into production if I did not have this very elegant rule enforcement.\n\n## Locking A Released Build\nDuring the process of development, there are several builds and deployments are happening all the time.  However, most of these I don't really care about as their only real value is to give feedback that the application was still able to build and deploy as it always has.  So one thing I never want to do is to lock down a build that came from anything other than the master branch.  I used to have a task on the build definition that would lock down any build that was created from the master branch.  However this is not always a good rule to live by either as there have been times when the deployment of a master branch did fail while going through the release pipeline and other times it might not have failed but there was a conscious decision to hold off on a release but was merged into master to be added with a few more features.\n\n{% img left /images/BuildTasks.png 300 300 \"Locking a Release\" %}\n\nWhat I needed was a task that would update the build with an infinite lock on the build when ever it was successfully deployed into Production.  For that task I did find one in the Microsoft Market Place that did exactly that.  This task is part of a small collection of BuildTasks written by Richard Fennell who is a long time ALM MVP.  In the Market Place it is called \"Build Updating Tasks\" and if you search for that, \"Richard Fennel\" or \"Black Marble\" I am sure you will find it.\n\nI have this task near the end of my Prod deployment and set the Build selection mode to \"On primary build artifact\" and done.  Works like a charm, when I deploy to production and it was successful it will find that build and set its retention to keep forever.  I no longer have to think about making sure I don't lose those builds that are in Production.\n\n## Increment the Build number\nThis rule has really allowed me to move freely into my new DevOps approach and no longer have this dependancy of the long planned release which I explained earlier did not ever get released the way I thought that it would.  Things and priorities change, that is life.  In my build definition I have a set of variables. One called the MajorMinorNumber and the other is the BuildNumber.  These combined with the TFS revision number on the end gives me the version number of my release.  So in the build definition under the general sub tab my Build number format looks similar to:\n\n    Product-v$(MajorMinorNumber).$(BuildNumber)$(rev:.r)\n\nNow lets break this down a little.  The MajorMinorNumber change rarely as they would represent big changes in the application.  This follows something close to [semantic versioning](http://semver.org/) in that if there is going to be a breaking change I would update the Major Number, if there was going to be a big change but would remain backwards compatible then the minor number would be incremented.  In the case where I am just adding some new features that are additive to the application or fixing some bugs then the build number would be incremented.  The 4th number which is the revision is left for TFS to make guarantee that we always have a unqiue build number.\n\nIn the past I have been known for using a date like version number for applications that I didn't think would really matter.  However, I have even noticed with them that there is some very important information that gets lost.  If I had a daily build going on and so the day part of the version number would increment everyday even though I might still be working on the same PBI or Bug.  Instead I want to have a new build number after I have a successful deployment into Production.  This means that I have customers out there who may have upgraded to a newer version and with that I can even produce some release notes as to what was part of that release.  But I did not want to go and increment the build number in the build everytime this happened, I wanted this to be automatic as well.\n\n{% img right /images/BuildTasks.png 300 300 \"Locking a Release\" %}\n\nThe solution for this is using the another special task that is part of the last extension that we installled.  There is a task called \"Update Build Variable\" and I have this as the very last task for the deployment into my Prod Environment.  Very simple to setup, the Build selection mode is: \"Only primary build artifact\" the Variable to update:  \"BuildNumber\" and the Update mode is \"Autoincrement\".\n\nNow after a successful deployment into Production and my build number is incremented and ready to go for either my next long planned set of feature or getting out that really quick important fix or special feature that I just needed to get out there.   ","slug":"My-New-3-Rules-for-Releases","published":1,"updated":"2020-01-05T00:36:05.028Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfp000fs4uf9onpilok","content":"<img src=\"/images/ALM.png\" class=\"left\" width=\"250\" height=\"200\" title=\"Application Lifecycle\">\n<p>Everyone of my products have an automated build and a properly managed release pipeline.  At the time I just thought business as usual as I was always on my way to having a well performing DevOps operation in my personal development efforts.  Well something happened in the way that I started approaching things which you don’t really plan, things will just start to happen when you get into a situation where everything is automated or at least they should and that is what this post is about.</p>\n<h2 id=\"I-don’t-have-to-wait\"><a href=\"#I-don’t-have-to-wait\" class=\"headerlink\" title=\"I don’t have to wait\"></a>I don’t have to wait</h2><p>One of the first things that I did notice was that I didn’t have the feeling like I needed to wait until this big plan of mine to do a release.  In the past I was using the Epic work item to plan out the finished features the I would need to complete to get the next release out.  I even noticed before I had all these steps automated that plans would change quite often.  The priorities and the well-meaning releases would take a turn to become something different like finding a critical bug that could affect the current customers.  I would want to release that bug or feature as quickly as possible.</p>\n<p>Before everything was automated, these things bothered me but there wasn’t an easy way to just get the release out there as there were still enough manual steps that you want to limit these.  However, now there is no reason to get a build that has a complete bug fix or feature and push it down the pipeline and get it released into production.    However, if this rush to production is now suddenly available to me isn’t there the possibility that something that wasn’t quite ready get into production by accident?  That is why I came up with these 3 new rules that I set for myself that need to be followed before the build can be pushed into production.</p>\n<h2 id=\"My-New-3-Rules-for-Releases\"><a href=\"#My-New-3-Rules-for-Releases\" class=\"headerlink\" title=\"My New 3 Rules for Releases\"></a>My New 3 Rules for Releases</h2><img src=\"/images/3simplerules.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Three Rules\">\n<ol>\n<li>Don’t allow any builds that came from any branch other than Master (git) or Main (tfvc) into production.  If it is not Master then it should just be rejected in the deployment steps.</li>\n<li>A build that is released with success into Production, will be locked indefinitely with regards to the retention policy.</li>\n<li>The build number must incremented any time that we successfully released into production.</li>\n</ol>\n<p>What follows are the ways that I automated these 3 rules and made them part of my operation.  Now there is never a fear that something might get deployed into production that really should not.  I can push things into production when it is important to do so and sometimes I might delay a release because there is no big benefit and saves the customers from having to download and install a release that could be packaged up with a later release.  The point being that a release can be made any time it needs to and no more of this long range planning which never happens the way you expected anyway.</p>\n<h2 id=\"No-Builds-into-Production-that-did-not-come-from-Master\"><a href=\"#No-Builds-into-Production-that-did-not-come-from-Master\" class=\"headerlink\" title=\"No Builds into Production that did not come from Master\"></a>No Builds into Production that did not come from Master</h2><p>As you may have gathered from some of my earlier posts, my personal projects have pretty much all landed up in git repositories that are remotely hosted in Visual Studio Team Services which is Microsoft’s cloud implementation of TFS.  With that I am following a very typical git like workflow.  Every Product Backlog Item or Bug starts with a new feature or bug branch.  This is really nice as it gives me a nice level of isolation and knowing that my work will not affect the working code.  It also gives me the flexibility to fix an important bug or PBI that changed in priority and know that the code I tore apart will not affect the outcome.</p>\n<p>This also gives me the opportunity to test this code, confirm that it is complete and give it one more look through as the only way code from a branch can get into master is through a pull request.  The pull request has a number of options with it as well such as squashing all the commits into a single commit (so I get a very clean and clear way of answering the question, how did you add this feature.) and deleting the branch after the merge.</p>\n<p>Master is always the branch that represents production or ready for production.  I wanted the code only to come from master because this is where all the branches come back to.  Having a rule like this makes sure that the merge will always happen and that nothing gets left out.  I have seen some very complicated branching structures when working with clients and something that I have seen quite often is that branches did not always get merged back to where they should.  There would be these complicated discussions about where the code that goes to production should really come from.  Here I have eliminated all the complexity by having a rule that says you can’t push a build that did not come from master into Production.</p>\n<p>Now, how do you enforce this automatically?  Well I could not find a task that would help me with this but I did know how I could do this with a simple PowerShell script.</p>\n<pre><code>$branch = &quot;$Env:BUILD_SOURCEBRANCHNAME&quot;\n\nif ($branch -ne &quot;master&quot;) {\n    Write-Host &quot;Cannot deploy builds from the $branch branch into Production&quot; \n    Write-Error (&quot;Can only deploy builds from the master branch into Production&quot;)\n    exit 1\n}\nelse {\n    Write-Host &quot;Carry on, this build qualifies for a deployment into Production&quot;\n}\n\n</code></pre><img src=\"/2016/09/My-New-3-Rules-for-Releases/MasterBranchOnly.png\" title=\"Implementing the Master Branch Only Rule\">\n<p>Using a PowerShell task at the top of a release for the Production environment as an inline script to implement this rule.  If for some reason I pushed a build that came from some other branch this task will fail and not go any farther.  In my world I typically have one build definition that is by default pointing to the master branch but I override that when I am working on one of my feature branches to get feedback on how the code is building and deploying.  Which I really like because I am using the very same build and deployment scripts that I would use when going into production.  So you can see how a build from one of these branches could accidentally get into production if I did not have this very elegant rule enforcement.</p>\n<h2 id=\"Locking-A-Released-Build\"><a href=\"#Locking-A-Released-Build\" class=\"headerlink\" title=\"Locking A Released Build\"></a>Locking A Released Build</h2><p>During the process of development, there are several builds and deployments are happening all the time.  However, most of these I don’t really care about as their only real value is to give feedback that the application was still able to build and deploy as it always has.  So one thing I never want to do is to lock down a build that came from anything other than the master branch.  I used to have a task on the build definition that would lock down any build that was created from the master branch.  However this is not always a good rule to live by either as there have been times when the deployment of a master branch did fail while going through the release pipeline and other times it might not have failed but there was a conscious decision to hold off on a release but was merged into master to be added with a few more features.</p>\n<img src=\"/images/BuildTasks.png\" class=\"left\" width=\"300\" height=\"300\" title=\"Locking a Release\">\n<p>What I needed was a task that would update the build with an infinite lock on the build when ever it was successfully deployed into Production.  For that task I did find one in the Microsoft Market Place that did exactly that.  This task is part of a small collection of BuildTasks written by Richard Fennell who is a long time ALM MVP.  In the Market Place it is called “Build Updating Tasks” and if you search for that, “Richard Fennel” or “Black Marble” I am sure you will find it.</p>\n<p>I have this task near the end of my Prod deployment and set the Build selection mode to “On primary build artifact” and done.  Works like a charm, when I deploy to production and it was successful it will find that build and set its retention to keep forever.  I no longer have to think about making sure I don’t lose those builds that are in Production.</p>\n<h2 id=\"Increment-the-Build-number\"><a href=\"#Increment-the-Build-number\" class=\"headerlink\" title=\"Increment the Build number\"></a>Increment the Build number</h2><p>This rule has really allowed me to move freely into my new DevOps approach and no longer have this dependancy of the long planned release which I explained earlier did not ever get released the way I thought that it would.  Things and priorities change, that is life.  In my build definition I have a set of variables. One called the MajorMinorNumber and the other is the BuildNumber.  These combined with the TFS revision number on the end gives me the version number of my release.  So in the build definition under the general sub tab my Build number format looks similar to:</p>\n<pre><code>Product-v$(MajorMinorNumber).$(BuildNumber)$(rev:.r)\n</code></pre><p>Now lets break this down a little.  The MajorMinorNumber change rarely as they would represent big changes in the application.  This follows something close to <a href=\"http://semver.org/\" target=\"_blank\" rel=\"noopener\">semantic versioning</a> in that if there is going to be a breaking change I would update the Major Number, if there was going to be a big change but would remain backwards compatible then the minor number would be incremented.  In the case where I am just adding some new features that are additive to the application or fixing some bugs then the build number would be incremented.  The 4th number which is the revision is left for TFS to make guarantee that we always have a unqiue build number.</p>\n<p>In the past I have been known for using a date like version number for applications that I didn’t think would really matter.  However, I have even noticed with them that there is some very important information that gets lost.  If I had a daily build going on and so the day part of the version number would increment everyday even though I might still be working on the same PBI or Bug.  Instead I want to have a new build number after I have a successful deployment into Production.  This means that I have customers out there who may have upgraded to a newer version and with that I can even produce some release notes as to what was part of that release.  But I did not want to go and increment the build number in the build everytime this happened, I wanted this to be automatic as well.</p>\n<img src=\"/images/BuildTasks.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Locking a Release\">\n<p>The solution for this is using the another special task that is part of the last extension that we installled.  There is a task called “Update Build Variable” and I have this as the very last task for the deployment into my Prod Environment.  Very simple to setup, the Build selection mode is: “Only primary build artifact” the Variable to update:  “BuildNumber” and the Update mode is “Autoincrement”.</p>\n<p>Now after a successful deployment into Production and my build number is incremented and ready to go for either my next long planned set of feature or getting out that really quick important fix or special feature that I just needed to get out there.   </p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/ALM.png\" class=\"left\" width=\"250\" height=\"200\" title=\"Application Lifecycle\">\n<p>Everyone of my products have an automated build and a properly managed release pipeline.  At the time I just thought business as usual as I was always on my way to having a well performing DevOps operation in my personal development efforts.  Well something happened in the way that I started approaching things which you don’t really plan, things will just start to happen when you get into a situation where everything is automated or at least they should and that is what this post is about.</p>\n<h2 id=\"I-don’t-have-to-wait\"><a href=\"#I-don’t-have-to-wait\" class=\"headerlink\" title=\"I don’t have to wait\"></a>I don’t have to wait</h2><p>One of the first things that I did notice was that I didn’t have the feeling like I needed to wait until this big plan of mine to do a release.  In the past I was using the Epic work item to plan out the finished features the I would need to complete to get the next release out.  I even noticed before I had all these steps automated that plans would change quite often.  The priorities and the well-meaning releases would take a turn to become something different like finding a critical bug that could affect the current customers.  I would want to release that bug or feature as quickly as possible.</p>\n<p>Before everything was automated, these things bothered me but there wasn’t an easy way to just get the release out there as there were still enough manual steps that you want to limit these.  However, now there is no reason to get a build that has a complete bug fix or feature and push it down the pipeline and get it released into production.    However, if this rush to production is now suddenly available to me isn’t there the possibility that something that wasn’t quite ready get into production by accident?  That is why I came up with these 3 new rules that I set for myself that need to be followed before the build can be pushed into production.</p>\n<h2 id=\"My-New-3-Rules-for-Releases\"><a href=\"#My-New-3-Rules-for-Releases\" class=\"headerlink\" title=\"My New 3 Rules for Releases\"></a>My New 3 Rules for Releases</h2><img src=\"/images/3simplerules.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Three Rules\">\n<ol>\n<li>Don’t allow any builds that came from any branch other than Master (git) or Main (tfvc) into production.  If it is not Master then it should just be rejected in the deployment steps.</li>\n<li>A build that is released with success into Production, will be locked indefinitely with regards to the retention policy.</li>\n<li>The build number must incremented any time that we successfully released into production.</li>\n</ol>\n<p>What follows are the ways that I automated these 3 rules and made them part of my operation.  Now there is never a fear that something might get deployed into production that really should not.  I can push things into production when it is important to do so and sometimes I might delay a release because there is no big benefit and saves the customers from having to download and install a release that could be packaged up with a later release.  The point being that a release can be made any time it needs to and no more of this long range planning which never happens the way you expected anyway.</p>\n<h2 id=\"No-Builds-into-Production-that-did-not-come-from-Master\"><a href=\"#No-Builds-into-Production-that-did-not-come-from-Master\" class=\"headerlink\" title=\"No Builds into Production that did not come from Master\"></a>No Builds into Production that did not come from Master</h2><p>As you may have gathered from some of my earlier posts, my personal projects have pretty much all landed up in git repositories that are remotely hosted in Visual Studio Team Services which is Microsoft’s cloud implementation of TFS.  With that I am following a very typical git like workflow.  Every Product Backlog Item or Bug starts with a new feature or bug branch.  This is really nice as it gives me a nice level of isolation and knowing that my work will not affect the working code.  It also gives me the flexibility to fix an important bug or PBI that changed in priority and know that the code I tore apart will not affect the outcome.</p>\n<p>This also gives me the opportunity to test this code, confirm that it is complete and give it one more look through as the only way code from a branch can get into master is through a pull request.  The pull request has a number of options with it as well such as squashing all the commits into a single commit (so I get a very clean and clear way of answering the question, how did you add this feature.) and deleting the branch after the merge.</p>\n<p>Master is always the branch that represents production or ready for production.  I wanted the code only to come from master because this is where all the branches come back to.  Having a rule like this makes sure that the merge will always happen and that nothing gets left out.  I have seen some very complicated branching structures when working with clients and something that I have seen quite often is that branches did not always get merged back to where they should.  There would be these complicated discussions about where the code that goes to production should really come from.  Here I have eliminated all the complexity by having a rule that says you can’t push a build that did not come from master into Production.</p>\n<p>Now, how do you enforce this automatically?  Well I could not find a task that would help me with this but I did know how I could do this with a simple PowerShell script.</p>\n<pre><code>$branch = &quot;$Env:BUILD_SOURCEBRANCHNAME&quot;\n\nif ($branch -ne &quot;master&quot;) {\n    Write-Host &quot;Cannot deploy builds from the $branch branch into Production&quot; \n    Write-Error (&quot;Can only deploy builds from the master branch into Production&quot;)\n    exit 1\n}\nelse {\n    Write-Host &quot;Carry on, this build qualifies for a deployment into Production&quot;\n}\n\n</code></pre><img src=\"/2016/09/My-New-3-Rules-for-Releases/MasterBranchOnly.png\" title=\"Implementing the Master Branch Only Rule\">\n<p>Using a PowerShell task at the top of a release for the Production environment as an inline script to implement this rule.  If for some reason I pushed a build that came from some other branch this task will fail and not go any farther.  In my world I typically have one build definition that is by default pointing to the master branch but I override that when I am working on one of my feature branches to get feedback on how the code is building and deploying.  Which I really like because I am using the very same build and deployment scripts that I would use when going into production.  So you can see how a build from one of these branches could accidentally get into production if I did not have this very elegant rule enforcement.</p>\n<h2 id=\"Locking-A-Released-Build\"><a href=\"#Locking-A-Released-Build\" class=\"headerlink\" title=\"Locking A Released Build\"></a>Locking A Released Build</h2><p>During the process of development, there are several builds and deployments are happening all the time.  However, most of these I don’t really care about as their only real value is to give feedback that the application was still able to build and deploy as it always has.  So one thing I never want to do is to lock down a build that came from anything other than the master branch.  I used to have a task on the build definition that would lock down any build that was created from the master branch.  However this is not always a good rule to live by either as there have been times when the deployment of a master branch did fail while going through the release pipeline and other times it might not have failed but there was a conscious decision to hold off on a release but was merged into master to be added with a few more features.</p>\n<img src=\"/images/BuildTasks.png\" class=\"left\" width=\"300\" height=\"300\" title=\"Locking a Release\">\n<p>What I needed was a task that would update the build with an infinite lock on the build when ever it was successfully deployed into Production.  For that task I did find one in the Microsoft Market Place that did exactly that.  This task is part of a small collection of BuildTasks written by Richard Fennell who is a long time ALM MVP.  In the Market Place it is called “Build Updating Tasks” and if you search for that, “Richard Fennel” or “Black Marble” I am sure you will find it.</p>\n<p>I have this task near the end of my Prod deployment and set the Build selection mode to “On primary build artifact” and done.  Works like a charm, when I deploy to production and it was successful it will find that build and set its retention to keep forever.  I no longer have to think about making sure I don’t lose those builds that are in Production.</p>\n<h2 id=\"Increment-the-Build-number\"><a href=\"#Increment-the-Build-number\" class=\"headerlink\" title=\"Increment the Build number\"></a>Increment the Build number</h2><p>This rule has really allowed me to move freely into my new DevOps approach and no longer have this dependancy of the long planned release which I explained earlier did not ever get released the way I thought that it would.  Things and priorities change, that is life.  In my build definition I have a set of variables. One called the MajorMinorNumber and the other is the BuildNumber.  These combined with the TFS revision number on the end gives me the version number of my release.  So in the build definition under the general sub tab my Build number format looks similar to:</p>\n<pre><code>Product-v$(MajorMinorNumber).$(BuildNumber)$(rev:.r)\n</code></pre><p>Now lets break this down a little.  The MajorMinorNumber change rarely as they would represent big changes in the application.  This follows something close to <a href=\"http://semver.org/\" target=\"_blank\" rel=\"noopener\">semantic versioning</a> in that if there is going to be a breaking change I would update the Major Number, if there was going to be a big change but would remain backwards compatible then the minor number would be incremented.  In the case where I am just adding some new features that are additive to the application or fixing some bugs then the build number would be incremented.  The 4th number which is the revision is left for TFS to make guarantee that we always have a unqiue build number.</p>\n<p>In the past I have been known for using a date like version number for applications that I didn’t think would really matter.  However, I have even noticed with them that there is some very important information that gets lost.  If I had a daily build going on and so the day part of the version number would increment everyday even though I might still be working on the same PBI or Bug.  Instead I want to have a new build number after I have a successful deployment into Production.  This means that I have customers out there who may have upgraded to a newer version and with that I can even produce some release notes as to what was part of that release.  But I did not want to go and increment the build number in the build everytime this happened, I wanted this to be automatic as well.</p>\n<img src=\"/images/BuildTasks.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Locking a Release\">\n<p>The solution for this is using the another special task that is part of the last extension that we installled.  There is a task called “Update Build Variable” and I have this as the very last task for the deployment into my Prod Environment.  Very simple to setup, the Build selection mode is: “Only primary build artifact” the Variable to update:  “BuildNumber” and the Update mode is “Autoincrement”.</p>\n<p>Now after a successful deployment into Production and my build number is incremented and ready to go for either my next long planned set of feature or getting out that really quick important fix or special feature that I just needed to get out there.   </p>\n"},{"title":"Security Configuration for Teams","date":"2016-11-02T19:51:13.000Z","_content":"Typically if it does not matter if team member’s can view the work of other teams or maybe they even work across teams which is usually the case, then having Contributor access at the TFS Project is all that is needed and desired.  However, there may be those situations where you find that you need to guard data from each team so that the other teams cannot see the source or the work items of the other team and yet be within the same TFS Project so that we can get good cross team reporting that makes sense.\n\nThis post will take you through the steps that you will need to take in order to put that level of security in place.\n\n## Creating Teams\nYou create the teams and administrate the security from the TFS admin page.  You would need to be a Project Administrator in order to create teams and you would have to be a Collection Administrator to create TFS Projects.  Assuming that you have the appropriate permission we start from the normal TFS web access page and click on the gear icon on the very far right of the page.\n\n{% asset_img 1.png \"~\" %}\n\nThen just click on the New team button to create a new Team.\n\n{% asset_img 2.png \"~\" %}\n\nWhen creating a team it is important not to put them into any of the built in TFS Security groups that exist.  These groups are setup from the TFS Project level and their rights and permissions filter all the way down to include all the teams.  The end result is that you add a member into one team and they would still see the work and source from all the other teams because they got their permissions from the TFS Project level.\n\n{% asset_img 3.png \"~\" %}\n\nWhen you create the team make sure that you set the permissions to (Do not add to a security group) and although it is not saying this what happens is that this team also gets its own TFS Security Group with that name.  This means that anyone we add to this team (provided they did not get higher permissions by being a member of some others team that does have a higher elevated security group) they would only have access to the things that we have given permission to for this team.\n\nBefore we move on to set the actual security we will have to set up the security for this team from the perspective of the TFS Project.  There are a few things that we would have to set here otherwise the team members would not be able to even see their team.  You do this by starting from the root team (this would match the name of the TFS Project) in the admin page.  While still in the page where you created the team click on the Security tab.\n\n{% asset_img 4.png \"~\" %}\n\nHere you want to select your new team and then allow the permissions at the TFS Project level.  You might be tempted to not set the View project-level information but doing that would not allow them to even see the project let alone get to their team.  Things you defiantly don’t want to allow is the ability to Delete team project or edit that project-level information that sort of thing should be reserved for someone like the Project Administorators.\n\n{% asset_img 5.png \"~\" %}\n\n## Area Path\n\nThe next thing that we need to tackle is the area path.  In TFS starting from TFS 2012, the area path is what represents the team.  Work items in the area path of the team is what we are able to use to keep the work items only visible to the appropriate team.\n\n{% asset_img 6.png \"~\" %}\n\nWhen this security screen first pops up you can see all the security groups that are from the Project level and it is important to note that if you want to restrict any users you want to make sure that they do not fall into any of these groups otherwise it will leave you wondering why they are able to access things that you did not give them permission.\n\n{% asset_img 7.png \"~\" %}\n\nThe first thing you will want to do is to add the team security group to the area.\n\n{% asset_img 8.png \"~\" %}\n\nFind your team security group (it will existing from the creation of the team) and click the Save changes button.\n\n{% asset_img 9.png \"~\" %}\n\nWith the new TFS group selected you will see on the right that nothing is set by default.  Click on all the permissions that you want to grant to the users of this group and then click on the Save changes.\n\n{% asset_img 10.png \"~\" %}\n\n## Version Control Security\n\nVersion control security works in a similar way that we had going with the areas.  To start, the security is placed at a folder and then the permissions would be set on each of the folders for the team that has permission to access that folder and down (recursive).\n\n{% asset_img 11.png \"~\" %}\n\nThe first step is to right click on the folder where you want to apply the security then go down to Advanced from the context menu that pops up and finally click on the Security tab.\n\n{% asset_img 12.png \"~\" %}\n\nWhen this folder opens up for the first time the group for the team will not be in the list of roles that have permissions.  First thing you will need to do on this screen is to click on the Add button and choose the menu option of “Add TFS group”.\n\n{% asset_img 13.png \"~\" %}\n\nNext you will need to select the team group and add the permissions that you want this new group to have and finally click on the Save changes.\n\n{% asset_img 14.png \"~\" %}\n\nThat is really all it takes to setup security at the team level.  The thing to keep in mind is that the members should not be members of any of the default roles, as you can see from the image above all these roles have some sort of permission to at a minimum read (Readers role).  If you follow this pattern where the members are only members of their team, then they would only see source that their team group can see.  It would be like the other source would not even be there.\n\n## Shared Folders Security\n\nFor each of the teams to be able to show query tiles on their home page, those queries must exist in the Share Queries area.  Because each team will have different needs and reporting on items that are different from other teams they should have their own folder area that only their team can see.  One of the ways we can manage this is to create a query folder for each of the teams under the Share Query folder and then add security specific to each team.\n\nStart in the Shared Queries folder, you can do this in either Web Access or with Visual Studio.  Web Access is shown here as everyone will have access to this tool but the steps are very similar to this to do this in Visual Studio.  Here we start from the home page and click on the View queries link\n\n{% asset_img 15.png \"~\" %}\n\nExpand the Shared Queries folder to expose all the folders and out of the box queries.  Then right click onto the Shared Queries folder and select “New query folder”.\n\n{% asset_img 16.png \"~\" %}\n\nEnter the name of the team for this query folder.  After it has been created right click while on the Team Folder and select Security…\n\n{% asset_img 17.png \"~\" %}\n\nClick on the Add dropdown control and the “Add TFS group” selection.  This will open another dialog box so that we can add the Donald Team group to this folder.\n\n{% asset_img 18.png \"~\" %}\n\nFind or enter the name of the Team and then click on the Save changes button.\n\n{% asset_img 19.png \"~\" %}\n\nWith the team security group selected you can select the permissions that they are allowed to have.  Typically this would be the Contribute and the Read permissions.  Then click on the Save changes button.\n\n{% asset_img 20.png \"~\" %}\n\nNow going back to that Shared Query view, you want to look at what this looks like from the view that a member who is only a member of this team would see.  They can only see their team folder under Shared Queries, even the defaults are not visable.\n\n{% asset_img 21.png \"~\" %}\n\n## Active Directory Groups\n\nOne final discussion in this area of Security and that is showing how the Active Directory Groups play into this whole thing.  The TFS Groups are used to manage the permissions but instead of adding any individuals to the Group you add the AD Group instead.\n\nIt pretty much has to be done this way because TFS automatically makes a TFS Group at the time that the new team is created.  Another way that this could have been done was by using a TFS Group and give it the permissions directly but the way that TFS works, this is the cleaner way to go because the TFS Group is going to be created regardless.\n\nStart from the home page of the Team and make sure that you are in the team that you want to add the active directory groups.  Next click on the Manage all members link which will open up a new window.\n\n{% asset_img 22.png \"~\" %}\n\nIn this window click on the Add… dropdown and choose “Add Windows user or group”.  This is where you would add the Active Directory (AD) group to be used to manage the actual users.  From this point on as you add or remove people from the AD Groups they would get or loose the rights that were assigned to the appropriate team.\n\n{% asset_img 23.png \"~\" %}\n\n","source":"_posts/Security-Configuration-for-Teams.md","raw":"title: Security Configuration for Teams\ndate: 2016-11-02 12:51:13\ntags:\n- ALM\n- Security\n---\nTypically if it does not matter if team member’s can view the work of other teams or maybe they even work across teams which is usually the case, then having Contributor access at the TFS Project is all that is needed and desired.  However, there may be those situations where you find that you need to guard data from each team so that the other teams cannot see the source or the work items of the other team and yet be within the same TFS Project so that we can get good cross team reporting that makes sense.\n\nThis post will take you through the steps that you will need to take in order to put that level of security in place.\n\n## Creating Teams\nYou create the teams and administrate the security from the TFS admin page.  You would need to be a Project Administrator in order to create teams and you would have to be a Collection Administrator to create TFS Projects.  Assuming that you have the appropriate permission we start from the normal TFS web access page and click on the gear icon on the very far right of the page.\n\n{% asset_img 1.png \"~\" %}\n\nThen just click on the New team button to create a new Team.\n\n{% asset_img 2.png \"~\" %}\n\nWhen creating a team it is important not to put them into any of the built in TFS Security groups that exist.  These groups are setup from the TFS Project level and their rights and permissions filter all the way down to include all the teams.  The end result is that you add a member into one team and they would still see the work and source from all the other teams because they got their permissions from the TFS Project level.\n\n{% asset_img 3.png \"~\" %}\n\nWhen you create the team make sure that you set the permissions to (Do not add to a security group) and although it is not saying this what happens is that this team also gets its own TFS Security Group with that name.  This means that anyone we add to this team (provided they did not get higher permissions by being a member of some others team that does have a higher elevated security group) they would only have access to the things that we have given permission to for this team.\n\nBefore we move on to set the actual security we will have to set up the security for this team from the perspective of the TFS Project.  There are a few things that we would have to set here otherwise the team members would not be able to even see their team.  You do this by starting from the root team (this would match the name of the TFS Project) in the admin page.  While still in the page where you created the team click on the Security tab.\n\n{% asset_img 4.png \"~\" %}\n\nHere you want to select your new team and then allow the permissions at the TFS Project level.  You might be tempted to not set the View project-level information but doing that would not allow them to even see the project let alone get to their team.  Things you defiantly don’t want to allow is the ability to Delete team project or edit that project-level information that sort of thing should be reserved for someone like the Project Administorators.\n\n{% asset_img 5.png \"~\" %}\n\n## Area Path\n\nThe next thing that we need to tackle is the area path.  In TFS starting from TFS 2012, the area path is what represents the team.  Work items in the area path of the team is what we are able to use to keep the work items only visible to the appropriate team.\n\n{% asset_img 6.png \"~\" %}\n\nWhen this security screen first pops up you can see all the security groups that are from the Project level and it is important to note that if you want to restrict any users you want to make sure that they do not fall into any of these groups otherwise it will leave you wondering why they are able to access things that you did not give them permission.\n\n{% asset_img 7.png \"~\" %}\n\nThe first thing you will want to do is to add the team security group to the area.\n\n{% asset_img 8.png \"~\" %}\n\nFind your team security group (it will existing from the creation of the team) and click the Save changes button.\n\n{% asset_img 9.png \"~\" %}\n\nWith the new TFS group selected you will see on the right that nothing is set by default.  Click on all the permissions that you want to grant to the users of this group and then click on the Save changes.\n\n{% asset_img 10.png \"~\" %}\n\n## Version Control Security\n\nVersion control security works in a similar way that we had going with the areas.  To start, the security is placed at a folder and then the permissions would be set on each of the folders for the team that has permission to access that folder and down (recursive).\n\n{% asset_img 11.png \"~\" %}\n\nThe first step is to right click on the folder where you want to apply the security then go down to Advanced from the context menu that pops up and finally click on the Security tab.\n\n{% asset_img 12.png \"~\" %}\n\nWhen this folder opens up for the first time the group for the team will not be in the list of roles that have permissions.  First thing you will need to do on this screen is to click on the Add button and choose the menu option of “Add TFS group”.\n\n{% asset_img 13.png \"~\" %}\n\nNext you will need to select the team group and add the permissions that you want this new group to have and finally click on the Save changes.\n\n{% asset_img 14.png \"~\" %}\n\nThat is really all it takes to setup security at the team level.  The thing to keep in mind is that the members should not be members of any of the default roles, as you can see from the image above all these roles have some sort of permission to at a minimum read (Readers role).  If you follow this pattern where the members are only members of their team, then they would only see source that their team group can see.  It would be like the other source would not even be there.\n\n## Shared Folders Security\n\nFor each of the teams to be able to show query tiles on their home page, those queries must exist in the Share Queries area.  Because each team will have different needs and reporting on items that are different from other teams they should have their own folder area that only their team can see.  One of the ways we can manage this is to create a query folder for each of the teams under the Share Query folder and then add security specific to each team.\n\nStart in the Shared Queries folder, you can do this in either Web Access or with Visual Studio.  Web Access is shown here as everyone will have access to this tool but the steps are very similar to this to do this in Visual Studio.  Here we start from the home page and click on the View queries link\n\n{% asset_img 15.png \"~\" %}\n\nExpand the Shared Queries folder to expose all the folders and out of the box queries.  Then right click onto the Shared Queries folder and select “New query folder”.\n\n{% asset_img 16.png \"~\" %}\n\nEnter the name of the team for this query folder.  After it has been created right click while on the Team Folder and select Security…\n\n{% asset_img 17.png \"~\" %}\n\nClick on the Add dropdown control and the “Add TFS group” selection.  This will open another dialog box so that we can add the Donald Team group to this folder.\n\n{% asset_img 18.png \"~\" %}\n\nFind or enter the name of the Team and then click on the Save changes button.\n\n{% asset_img 19.png \"~\" %}\n\nWith the team security group selected you can select the permissions that they are allowed to have.  Typically this would be the Contribute and the Read permissions.  Then click on the Save changes button.\n\n{% asset_img 20.png \"~\" %}\n\nNow going back to that Shared Query view, you want to look at what this looks like from the view that a member who is only a member of this team would see.  They can only see their team folder under Shared Queries, even the defaults are not visable.\n\n{% asset_img 21.png \"~\" %}\n\n## Active Directory Groups\n\nOne final discussion in this area of Security and that is showing how the Active Directory Groups play into this whole thing.  The TFS Groups are used to manage the permissions but instead of adding any individuals to the Group you add the AD Group instead.\n\nIt pretty much has to be done this way because TFS automatically makes a TFS Group at the time that the new team is created.  Another way that this could have been done was by using a TFS Group and give it the permissions directly but the way that TFS works, this is the cleaner way to go because the TFS Group is going to be created regardless.\n\nStart from the home page of the Team and make sure that you are in the team that you want to add the active directory groups.  Next click on the Manage all members link which will open up a new window.\n\n{% asset_img 22.png \"~\" %}\n\nIn this window click on the Add… dropdown and choose “Add Windows user or group”.  This is where you would add the Active Directory (AD) group to be used to manage the actual users.  From this point on as you add or remove people from the AD Groups they would get or loose the rights that were assigned to the appropriate team.\n\n{% asset_img 23.png \"~\" %}\n\n","slug":"Security-Configuration-for-Teams","published":1,"updated":"2020-01-05T00:36:05.038Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgfr000gs4uf0uzqjis4","content":"<p>Typically if it does not matter if team member’s can view the work of other teams or maybe they even work across teams which is usually the case, then having Contributor access at the TFS Project is all that is needed and desired.  However, there may be those situations where you find that you need to guard data from each team so that the other teams cannot see the source or the work items of the other team and yet be within the same TFS Project so that we can get good cross team reporting that makes sense.</p>\n<p>This post will take you through the steps that you will need to take in order to put that level of security in place.</p>\n<h2 id=\"Creating-Teams\"><a href=\"#Creating-Teams\" class=\"headerlink\" title=\"Creating Teams\"></a>Creating Teams</h2><p>You create the teams and administrate the security from the TFS admin page.  You would need to be a Project Administrator in order to create teams and you would have to be a Collection Administrator to create TFS Projects.  Assuming that you have the appropriate permission we start from the normal TFS web access page and click on the gear icon on the very far right of the page.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/1.png\" title=\"~\">\n<p>Then just click on the New team button to create a new Team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/2.png\" title=\"~\">\n<p>When creating a team it is important not to put them into any of the built in TFS Security groups that exist.  These groups are setup from the TFS Project level and their rights and permissions filter all the way down to include all the teams.  The end result is that you add a member into one team and they would still see the work and source from all the other teams because they got their permissions from the TFS Project level.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/3.png\" title=\"~\">\n<p>When you create the team make sure that you set the permissions to (Do not add to a security group) and although it is not saying this what happens is that this team also gets its own TFS Security Group with that name.  This means that anyone we add to this team (provided they did not get higher permissions by being a member of some others team that does have a higher elevated security group) they would only have access to the things that we have given permission to for this team.</p>\n<p>Before we move on to set the actual security we will have to set up the security for this team from the perspective of the TFS Project.  There are a few things that we would have to set here otherwise the team members would not be able to even see their team.  You do this by starting from the root team (this would match the name of the TFS Project) in the admin page.  While still in the page where you created the team click on the Security tab.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/4.png\" title=\"~\">\n<p>Here you want to select your new team and then allow the permissions at the TFS Project level.  You might be tempted to not set the View project-level information but doing that would not allow them to even see the project let alone get to their team.  Things you defiantly don’t want to allow is the ability to Delete team project or edit that project-level information that sort of thing should be reserved for someone like the Project Administorators.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/5.png\" title=\"~\">\n<h2 id=\"Area-Path\"><a href=\"#Area-Path\" class=\"headerlink\" title=\"Area Path\"></a>Area Path</h2><p>The next thing that we need to tackle is the area path.  In TFS starting from TFS 2012, the area path is what represents the team.  Work items in the area path of the team is what we are able to use to keep the work items only visible to the appropriate team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/6.png\" title=\"~\">\n<p>When this security screen first pops up you can see all the security groups that are from the Project level and it is important to note that if you want to restrict any users you want to make sure that they do not fall into any of these groups otherwise it will leave you wondering why they are able to access things that you did not give them permission.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/7.png\" title=\"~\">\n<p>The first thing you will want to do is to add the team security group to the area.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/8.png\" title=\"~\">\n<p>Find your team security group (it will existing from the creation of the team) and click the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/9.png\" title=\"~\">\n<p>With the new TFS group selected you will see on the right that nothing is set by default.  Click on all the permissions that you want to grant to the users of this group and then click on the Save changes.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/10.png\" title=\"~\">\n<h2 id=\"Version-Control-Security\"><a href=\"#Version-Control-Security\" class=\"headerlink\" title=\"Version Control Security\"></a>Version Control Security</h2><p>Version control security works in a similar way that we had going with the areas.  To start, the security is placed at a folder and then the permissions would be set on each of the folders for the team that has permission to access that folder and down (recursive).</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/11.png\" title=\"~\">\n<p>The first step is to right click on the folder where you want to apply the security then go down to Advanced from the context menu that pops up and finally click on the Security tab.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/12.png\" title=\"~\">\n<p>When this folder opens up for the first time the group for the team will not be in the list of roles that have permissions.  First thing you will need to do on this screen is to click on the Add button and choose the menu option of “Add TFS group”.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/13.png\" title=\"~\">\n<p>Next you will need to select the team group and add the permissions that you want this new group to have and finally click on the Save changes.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/14.png\" title=\"~\">\n<p>That is really all it takes to setup security at the team level.  The thing to keep in mind is that the members should not be members of any of the default roles, as you can see from the image above all these roles have some sort of permission to at a minimum read (Readers role).  If you follow this pattern where the members are only members of their team, then they would only see source that their team group can see.  It would be like the other source would not even be there.</p>\n<h2 id=\"Shared-Folders-Security\"><a href=\"#Shared-Folders-Security\" class=\"headerlink\" title=\"Shared Folders Security\"></a>Shared Folders Security</h2><p>For each of the teams to be able to show query tiles on their home page, those queries must exist in the Share Queries area.  Because each team will have different needs and reporting on items that are different from other teams they should have their own folder area that only their team can see.  One of the ways we can manage this is to create a query folder for each of the teams under the Share Query folder and then add security specific to each team.</p>\n<p>Start in the Shared Queries folder, you can do this in either Web Access or with Visual Studio.  Web Access is shown here as everyone will have access to this tool but the steps are very similar to this to do this in Visual Studio.  Here we start from the home page and click on the View queries link</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/15.png\" title=\"~\">\n<p>Expand the Shared Queries folder to expose all the folders and out of the box queries.  Then right click onto the Shared Queries folder and select “New query folder”.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/16.png\" title=\"~\">\n<p>Enter the name of the team for this query folder.  After it has been created right click while on the Team Folder and select Security…</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/17.png\" title=\"~\">\n<p>Click on the Add dropdown control and the “Add TFS group” selection.  This will open another dialog box so that we can add the Donald Team group to this folder.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/18.png\" title=\"~\">\n<p>Find or enter the name of the Team and then click on the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/19.png\" title=\"~\">\n<p>With the team security group selected you can select the permissions that they are allowed to have.  Typically this would be the Contribute and the Read permissions.  Then click on the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/20.png\" title=\"~\">\n<p>Now going back to that Shared Query view, you want to look at what this looks like from the view that a member who is only a member of this team would see.  They can only see their team folder under Shared Queries, even the defaults are not visable.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/21.png\" title=\"~\">\n<h2 id=\"Active-Directory-Groups\"><a href=\"#Active-Directory-Groups\" class=\"headerlink\" title=\"Active Directory Groups\"></a>Active Directory Groups</h2><p>One final discussion in this area of Security and that is showing how the Active Directory Groups play into this whole thing.  The TFS Groups are used to manage the permissions but instead of adding any individuals to the Group you add the AD Group instead.</p>\n<p>It pretty much has to be done this way because TFS automatically makes a TFS Group at the time that the new team is created.  Another way that this could have been done was by using a TFS Group and give it the permissions directly but the way that TFS works, this is the cleaner way to go because the TFS Group is going to be created regardless.</p>\n<p>Start from the home page of the Team and make sure that you are in the team that you want to add the active directory groups.  Next click on the Manage all members link which will open up a new window.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/22.png\" title=\"~\">\n<p>In this window click on the Add… dropdown and choose “Add Windows user or group”.  This is where you would add the Active Directory (AD) group to be used to manage the actual users.  From this point on as you add or remove people from the AD Groups they would get or loose the rights that were assigned to the appropriate team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/23.png\" title=\"~\">\n","site":{"data":{}},"excerpt":"","more":"<p>Typically if it does not matter if team member’s can view the work of other teams or maybe they even work across teams which is usually the case, then having Contributor access at the TFS Project is all that is needed and desired.  However, there may be those situations where you find that you need to guard data from each team so that the other teams cannot see the source or the work items of the other team and yet be within the same TFS Project so that we can get good cross team reporting that makes sense.</p>\n<p>This post will take you through the steps that you will need to take in order to put that level of security in place.</p>\n<h2 id=\"Creating-Teams\"><a href=\"#Creating-Teams\" class=\"headerlink\" title=\"Creating Teams\"></a>Creating Teams</h2><p>You create the teams and administrate the security from the TFS admin page.  You would need to be a Project Administrator in order to create teams and you would have to be a Collection Administrator to create TFS Projects.  Assuming that you have the appropriate permission we start from the normal TFS web access page and click on the gear icon on the very far right of the page.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/1.png\" title=\"~\">\n<p>Then just click on the New team button to create a new Team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/2.png\" title=\"~\">\n<p>When creating a team it is important not to put them into any of the built in TFS Security groups that exist.  These groups are setup from the TFS Project level and their rights and permissions filter all the way down to include all the teams.  The end result is that you add a member into one team and they would still see the work and source from all the other teams because they got their permissions from the TFS Project level.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/3.png\" title=\"~\">\n<p>When you create the team make sure that you set the permissions to (Do not add to a security group) and although it is not saying this what happens is that this team also gets its own TFS Security Group with that name.  This means that anyone we add to this team (provided they did not get higher permissions by being a member of some others team that does have a higher elevated security group) they would only have access to the things that we have given permission to for this team.</p>\n<p>Before we move on to set the actual security we will have to set up the security for this team from the perspective of the TFS Project.  There are a few things that we would have to set here otherwise the team members would not be able to even see their team.  You do this by starting from the root team (this would match the name of the TFS Project) in the admin page.  While still in the page where you created the team click on the Security tab.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/4.png\" title=\"~\">\n<p>Here you want to select your new team and then allow the permissions at the TFS Project level.  You might be tempted to not set the View project-level information but doing that would not allow them to even see the project let alone get to their team.  Things you defiantly don’t want to allow is the ability to Delete team project or edit that project-level information that sort of thing should be reserved for someone like the Project Administorators.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/5.png\" title=\"~\">\n<h2 id=\"Area-Path\"><a href=\"#Area-Path\" class=\"headerlink\" title=\"Area Path\"></a>Area Path</h2><p>The next thing that we need to tackle is the area path.  In TFS starting from TFS 2012, the area path is what represents the team.  Work items in the area path of the team is what we are able to use to keep the work items only visible to the appropriate team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/6.png\" title=\"~\">\n<p>When this security screen first pops up you can see all the security groups that are from the Project level and it is important to note that if you want to restrict any users you want to make sure that they do not fall into any of these groups otherwise it will leave you wondering why they are able to access things that you did not give them permission.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/7.png\" title=\"~\">\n<p>The first thing you will want to do is to add the team security group to the area.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/8.png\" title=\"~\">\n<p>Find your team security group (it will existing from the creation of the team) and click the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/9.png\" title=\"~\">\n<p>With the new TFS group selected you will see on the right that nothing is set by default.  Click on all the permissions that you want to grant to the users of this group and then click on the Save changes.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/10.png\" title=\"~\">\n<h2 id=\"Version-Control-Security\"><a href=\"#Version-Control-Security\" class=\"headerlink\" title=\"Version Control Security\"></a>Version Control Security</h2><p>Version control security works in a similar way that we had going with the areas.  To start, the security is placed at a folder and then the permissions would be set on each of the folders for the team that has permission to access that folder and down (recursive).</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/11.png\" title=\"~\">\n<p>The first step is to right click on the folder where you want to apply the security then go down to Advanced from the context menu that pops up and finally click on the Security tab.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/12.png\" title=\"~\">\n<p>When this folder opens up for the first time the group for the team will not be in the list of roles that have permissions.  First thing you will need to do on this screen is to click on the Add button and choose the menu option of “Add TFS group”.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/13.png\" title=\"~\">\n<p>Next you will need to select the team group and add the permissions that you want this new group to have and finally click on the Save changes.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/14.png\" title=\"~\">\n<p>That is really all it takes to setup security at the team level.  The thing to keep in mind is that the members should not be members of any of the default roles, as you can see from the image above all these roles have some sort of permission to at a minimum read (Readers role).  If you follow this pattern where the members are only members of their team, then they would only see source that their team group can see.  It would be like the other source would not even be there.</p>\n<h2 id=\"Shared-Folders-Security\"><a href=\"#Shared-Folders-Security\" class=\"headerlink\" title=\"Shared Folders Security\"></a>Shared Folders Security</h2><p>For each of the teams to be able to show query tiles on their home page, those queries must exist in the Share Queries area.  Because each team will have different needs and reporting on items that are different from other teams they should have their own folder area that only their team can see.  One of the ways we can manage this is to create a query folder for each of the teams under the Share Query folder and then add security specific to each team.</p>\n<p>Start in the Shared Queries folder, you can do this in either Web Access or with Visual Studio.  Web Access is shown here as everyone will have access to this tool but the steps are very similar to this to do this in Visual Studio.  Here we start from the home page and click on the View queries link</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/15.png\" title=\"~\">\n<p>Expand the Shared Queries folder to expose all the folders and out of the box queries.  Then right click onto the Shared Queries folder and select “New query folder”.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/16.png\" title=\"~\">\n<p>Enter the name of the team for this query folder.  After it has been created right click while on the Team Folder and select Security…</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/17.png\" title=\"~\">\n<p>Click on the Add dropdown control and the “Add TFS group” selection.  This will open another dialog box so that we can add the Donald Team group to this folder.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/18.png\" title=\"~\">\n<p>Find or enter the name of the Team and then click on the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/19.png\" title=\"~\">\n<p>With the team security group selected you can select the permissions that they are allowed to have.  Typically this would be the Contribute and the Read permissions.  Then click on the Save changes button.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/20.png\" title=\"~\">\n<p>Now going back to that Shared Query view, you want to look at what this looks like from the view that a member who is only a member of this team would see.  They can only see their team folder under Shared Queries, even the defaults are not visable.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/21.png\" title=\"~\">\n<h2 id=\"Active-Directory-Groups\"><a href=\"#Active-Directory-Groups\" class=\"headerlink\" title=\"Active Directory Groups\"></a>Active Directory Groups</h2><p>One final discussion in this area of Security and that is showing how the Active Directory Groups play into this whole thing.  The TFS Groups are used to manage the permissions but instead of adding any individuals to the Group you add the AD Group instead.</p>\n<p>It pretty much has to be done this way because TFS automatically makes a TFS Group at the time that the new team is created.  Another way that this could have been done was by using a TFS Group and give it the permissions directly but the way that TFS works, this is the cleaner way to go because the TFS Group is going to be created regardless.</p>\n<p>Start from the home page of the Team and make sure that you are in the team that you want to add the active directory groups.  Next click on the Manage all members link which will open up a new window.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/22.png\" title=\"~\">\n<p>In this window click on the Add… dropdown and choose “Add Windows user or group”.  This is where you would add the Active Directory (AD) group to be used to manage the actual users.  From this point on as you add or remove people from the AD Groups they would get or loose the rights that were assigned to the appropriate team.</p>\n<img src=\"/2016/11/Security-Configuration-for-Teams/23.png\" title=\"~\">\n"},{"title":"No no he's not dead, he's, he's restin'!","date":"2008-06-21T07:00:00.000Z","_content":"{% asset_img blog_grim_reaper.gif \"The blog grim reaper\"%}\nJust in case you did not get the Monty Python reference here is a cartoon courtesy of *Blaugh* which gets right to the point.  I have been away from writing anything for my web site for a very, very, long time.  Where have I been?  Where do I begin?  I have been quite busy developing software for a number of clients that I cannot name because of none disclosure clauses in my contracts.  I never did understand how disclosing who the actual client is in a public forum would be such a big deal but I can only descibe what I have been doing over the last four years as having worked in the hospitality, mortgage, back to hospitality and now property cost industry.  While that has been keeping me busy with all the work that these projects generate, Mary and I have continued to develop and support our [AGP Maker](http://www.agpmaker.com) program.\n\nWhat has brought my sudden attention back to this site and providing more articles and input on what I have been working on?  I guess because of the change in where and how I am hosting the site and a change in the content program to update the site.  This has been the third time that I have changed the content management system for this site.  I started out using City Desk because of an article that I came across. I don't rember which magazine but the article was about content managment systems. The article quoted a couple of paragraphs  from Joel Spolsky and he was talking about City Desk.  That article took me to Joel's web site [Joel on Software](http://www.joelonsoftware.com) which was the original inspiration for starting my site.  I liked Joel's style and how he looked at things.  This was the very first blog that I followed faithfully.  Even today, when Joel writes something, I just want to find the time to sit down and read it.  I guess part of it is that Joel does not write every day or even every week.  When he has something he wants to say and share he does and that has always been my goal.  Speak when I have something to say, not just to generate content.\n\nNext I switched the content management system over to Microsoft Content Management System (MCMS).  This was a great learning experience and I was able to leverage my dot net skills.  It provided me with the ability to edit the pages from where ever I was at home or on the road, which was a problem that I had with City Desk as I had to make changes within the City Desk program and then push out all the files to their final location.  The future of MCMS is uncertain as Microsoft is moving that technology into the latest release of Microsoft Office SharePoint Server (MOSS).  That was not the reason I am leaving this platform though, as this is a really great product, it was just impossible to host these sites anywhere but on my public exposing web server.  I really want to move all the public web sites to be hosted outside our office so that they can be expanded and extended and provide a much more stable environment.  Our office is not setup for hosting and right now our hosting needs are not all that great, but things may very well change over time.\n\nThis brings us to the third content mangement system that I am switching to.  I am moving all our content over to [DotNetNuke](http://www.dnnsoftware.com).  Once again that leverages my skills as a dot net developer added with the extra benifit that GoDaddy supports this in their free hosting program natively.  This continues to give me the flexability to update the pages were ever I am, give me a better opportunity to get my pages indexed by the search engines and allow readers to link to direct pages and articles.  When I had this site hosted in my office you could not link directly to an article unless you knew the name of the page which was all hidden from view.  This may even lead to some articles that I might do about working with DotNetNuke.\n\nOver the next couple of weeks and months I want to take on some technical issues like authentication and how I have taken advantage of windows authentication but used it in the way that forms authentication provides some greater flexability.  The way that in house internal programs are built and consumed in other companies that I have worked, just bugs me to death.  There is no reason why I need to log onto every single tool that I use if I have logged onto the computer that I am using.  There is no need for this and I have developed some techinques that I will share on how I use this to work the way it should. \n\nI would also like to cover some topics that I have never covered before.  These would just be opion pieces so take them with a grain of salt, but I do want to cover some political and economic issues that have been bugging me.  If nothing else they will make you think cause I am sure my views are going to be a little different then what you might have been expecting.  I do at times have a unique view on the way I see things working and how I think that they should be working.  Keep in mind these are opionions not necessarily based on a lot of facts. \n\nI would like to talk about my conversion and on going conversion of all my web sites going the way of DotNetNuke.  This is a great content management tool that gives me a lot of flexabiltiy as the skins are easy to create, now if only I was better at graphics I could really do something with this tool, but over all the experience is quite plesant.  Modules that are not provided by what is in the DotNetNuke installation package I can create quite easily, I am after all a software developer.  Plus the fact that GoDaddy which has been my domain name registar for years is now providing some free hosting (for the price of a domain registration) and they fully support DotNetNuke as a hosting package. \n  ","source":"_posts/No-no-he-s-not-dead-he-s-he-s-restin.md","raw":"title: \"No no he's not dead, he's, he's restin'!\"\ndate: 2008-06-21\ntags: \n- Blogs\n---\n{% asset_img blog_grim_reaper.gif \"The blog grim reaper\"%}\nJust in case you did not get the Monty Python reference here is a cartoon courtesy of *Blaugh* which gets right to the point.  I have been away from writing anything for my web site for a very, very, long time.  Where have I been?  Where do I begin?  I have been quite busy developing software for a number of clients that I cannot name because of none disclosure clauses in my contracts.  I never did understand how disclosing who the actual client is in a public forum would be such a big deal but I can only descibe what I have been doing over the last four years as having worked in the hospitality, mortgage, back to hospitality and now property cost industry.  While that has been keeping me busy with all the work that these projects generate, Mary and I have continued to develop and support our [AGP Maker](http://www.agpmaker.com) program.\n\nWhat has brought my sudden attention back to this site and providing more articles and input on what I have been working on?  I guess because of the change in where and how I am hosting the site and a change in the content program to update the site.  This has been the third time that I have changed the content management system for this site.  I started out using City Desk because of an article that I came across. I don't rember which magazine but the article was about content managment systems. The article quoted a couple of paragraphs  from Joel Spolsky and he was talking about City Desk.  That article took me to Joel's web site [Joel on Software](http://www.joelonsoftware.com) which was the original inspiration for starting my site.  I liked Joel's style and how he looked at things.  This was the very first blog that I followed faithfully.  Even today, when Joel writes something, I just want to find the time to sit down and read it.  I guess part of it is that Joel does not write every day or even every week.  When he has something he wants to say and share he does and that has always been my goal.  Speak when I have something to say, not just to generate content.\n\nNext I switched the content management system over to Microsoft Content Management System (MCMS).  This was a great learning experience and I was able to leverage my dot net skills.  It provided me with the ability to edit the pages from where ever I was at home or on the road, which was a problem that I had with City Desk as I had to make changes within the City Desk program and then push out all the files to their final location.  The future of MCMS is uncertain as Microsoft is moving that technology into the latest release of Microsoft Office SharePoint Server (MOSS).  That was not the reason I am leaving this platform though, as this is a really great product, it was just impossible to host these sites anywhere but on my public exposing web server.  I really want to move all the public web sites to be hosted outside our office so that they can be expanded and extended and provide a much more stable environment.  Our office is not setup for hosting and right now our hosting needs are not all that great, but things may very well change over time.\n\nThis brings us to the third content mangement system that I am switching to.  I am moving all our content over to [DotNetNuke](http://www.dnnsoftware.com).  Once again that leverages my skills as a dot net developer added with the extra benifit that GoDaddy supports this in their free hosting program natively.  This continues to give me the flexability to update the pages were ever I am, give me a better opportunity to get my pages indexed by the search engines and allow readers to link to direct pages and articles.  When I had this site hosted in my office you could not link directly to an article unless you knew the name of the page which was all hidden from view.  This may even lead to some articles that I might do about working with DotNetNuke.\n\nOver the next couple of weeks and months I want to take on some technical issues like authentication and how I have taken advantage of windows authentication but used it in the way that forms authentication provides some greater flexability.  The way that in house internal programs are built and consumed in other companies that I have worked, just bugs me to death.  There is no reason why I need to log onto every single tool that I use if I have logged onto the computer that I am using.  There is no need for this and I have developed some techinques that I will share on how I use this to work the way it should. \n\nI would also like to cover some topics that I have never covered before.  These would just be opion pieces so take them with a grain of salt, but I do want to cover some political and economic issues that have been bugging me.  If nothing else they will make you think cause I am sure my views are going to be a little different then what you might have been expecting.  I do at times have a unique view on the way I see things working and how I think that they should be working.  Keep in mind these are opionions not necessarily based on a lot of facts. \n\nI would like to talk about my conversion and on going conversion of all my web sites going the way of DotNetNuke.  This is a great content management tool that gives me a lot of flexabiltiy as the skins are easy to create, now if only I was better at graphics I could really do something with this tool, but over all the experience is quite plesant.  Modules that are not provided by what is in the DotNetNuke installation package I can create quite easily, I am after all a software developer.  Plus the fact that GoDaddy which has been my domain name registar for years is now providing some free hosting (for the price of a domain registration) and they fully support DotNetNuke as a hosting package. \n  ","slug":"No-no-he-s-not-dead-he-s-he-s-restin","published":1,"updated":"2020-01-05T00:36:05.030Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg0000hs4uf33488baz","content":"<img src=\"/2008/06/No-no-he-s-not-dead-he-s-he-s-restin/blog_grim_reaper.gif\" title=\"The blog grim reaper\">\n<p>Just in case you did not get the Monty Python reference here is a cartoon courtesy of <em>Blaugh</em> which gets right to the point.  I have been away from writing anything for my web site for a very, very, long time.  Where have I been?  Where do I begin?  I have been quite busy developing software for a number of clients that I cannot name because of none disclosure clauses in my contracts.  I never did understand how disclosing who the actual client is in a public forum would be such a big deal but I can only descibe what I have been doing over the last four years as having worked in the hospitality, mortgage, back to hospitality and now property cost industry.  While that has been keeping me busy with all the work that these projects generate, Mary and I have continued to develop and support our <a href=\"http://www.agpmaker.com\" target=\"_blank\" rel=\"noopener\">AGP Maker</a> program.</p>\n<p>What has brought my sudden attention back to this site and providing more articles and input on what I have been working on?  I guess because of the change in where and how I am hosting the site and a change in the content program to update the site.  This has been the third time that I have changed the content management system for this site.  I started out using City Desk because of an article that I came across. I don’t rember which magazine but the article was about content managment systems. The article quoted a couple of paragraphs  from Joel Spolsky and he was talking about City Desk.  That article took me to Joel’s web site <a href=\"http://www.joelonsoftware.com\" target=\"_blank\" rel=\"noopener\">Joel on Software</a> which was the original inspiration for starting my site.  I liked Joel’s style and how he looked at things.  This was the very first blog that I followed faithfully.  Even today, when Joel writes something, I just want to find the time to sit down and read it.  I guess part of it is that Joel does not write every day or even every week.  When he has something he wants to say and share he does and that has always been my goal.  Speak when I have something to say, not just to generate content.</p>\n<p>Next I switched the content management system over to Microsoft Content Management System (MCMS).  This was a great learning experience and I was able to leverage my dot net skills.  It provided me with the ability to edit the pages from where ever I was at home or on the road, which was a problem that I had with City Desk as I had to make changes within the City Desk program and then push out all the files to their final location.  The future of MCMS is uncertain as Microsoft is moving that technology into the latest release of Microsoft Office SharePoint Server (MOSS).  That was not the reason I am leaving this platform though, as this is a really great product, it was just impossible to host these sites anywhere but on my public exposing web server.  I really want to move all the public web sites to be hosted outside our office so that they can be expanded and extended and provide a much more stable environment.  Our office is not setup for hosting and right now our hosting needs are not all that great, but things may very well change over time.</p>\n<p>This brings us to the third content mangement system that I am switching to.  I am moving all our content over to <a href=\"http://www.dnnsoftware.com\" target=\"_blank\" rel=\"noopener\">DotNetNuke</a>.  Once again that leverages my skills as a dot net developer added with the extra benifit that GoDaddy supports this in their free hosting program natively.  This continues to give me the flexability to update the pages were ever I am, give me a better opportunity to get my pages indexed by the search engines and allow readers to link to direct pages and articles.  When I had this site hosted in my office you could not link directly to an article unless you knew the name of the page which was all hidden from view.  This may even lead to some articles that I might do about working with DotNetNuke.</p>\n<p>Over the next couple of weeks and months I want to take on some technical issues like authentication and how I have taken advantage of windows authentication but used it in the way that forms authentication provides some greater flexability.  The way that in house internal programs are built and consumed in other companies that I have worked, just bugs me to death.  There is no reason why I need to log onto every single tool that I use if I have logged onto the computer that I am using.  There is no need for this and I have developed some techinques that I will share on how I use this to work the way it should. </p>\n<p>I would also like to cover some topics that I have never covered before.  These would just be opion pieces so take them with a grain of salt, but I do want to cover some political and economic issues that have been bugging me.  If nothing else they will make you think cause I am sure my views are going to be a little different then what you might have been expecting.  I do at times have a unique view on the way I see things working and how I think that they should be working.  Keep in mind these are opionions not necessarily based on a lot of facts. </p>\n<p>I would like to talk about my conversion and on going conversion of all my web sites going the way of DotNetNuke.  This is a great content management tool that gives me a lot of flexabiltiy as the skins are easy to create, now if only I was better at graphics I could really do something with this tool, but over all the experience is quite plesant.  Modules that are not provided by what is in the DotNetNuke installation package I can create quite easily, I am after all a software developer.  Plus the fact that GoDaddy which has been my domain name registar for years is now providing some free hosting (for the price of a domain registration) and they fully support DotNetNuke as a hosting package. </p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/2008/06/No-no-he-s-not-dead-he-s-he-s-restin/blog_grim_reaper.gif\" title=\"The blog grim reaper\">\n<p>Just in case you did not get the Monty Python reference here is a cartoon courtesy of <em>Blaugh</em> which gets right to the point.  I have been away from writing anything for my web site for a very, very, long time.  Where have I been?  Where do I begin?  I have been quite busy developing software for a number of clients that I cannot name because of none disclosure clauses in my contracts.  I never did understand how disclosing who the actual client is in a public forum would be such a big deal but I can only descibe what I have been doing over the last four years as having worked in the hospitality, mortgage, back to hospitality and now property cost industry.  While that has been keeping me busy with all the work that these projects generate, Mary and I have continued to develop and support our <a href=\"http://www.agpmaker.com\" target=\"_blank\" rel=\"noopener\">AGP Maker</a> program.</p>\n<p>What has brought my sudden attention back to this site and providing more articles and input on what I have been working on?  I guess because of the change in where and how I am hosting the site and a change in the content program to update the site.  This has been the third time that I have changed the content management system for this site.  I started out using City Desk because of an article that I came across. I don’t rember which magazine but the article was about content managment systems. The article quoted a couple of paragraphs  from Joel Spolsky and he was talking about City Desk.  That article took me to Joel’s web site <a href=\"http://www.joelonsoftware.com\" target=\"_blank\" rel=\"noopener\">Joel on Software</a> which was the original inspiration for starting my site.  I liked Joel’s style and how he looked at things.  This was the very first blog that I followed faithfully.  Even today, when Joel writes something, I just want to find the time to sit down and read it.  I guess part of it is that Joel does not write every day or even every week.  When he has something he wants to say and share he does and that has always been my goal.  Speak when I have something to say, not just to generate content.</p>\n<p>Next I switched the content management system over to Microsoft Content Management System (MCMS).  This was a great learning experience and I was able to leverage my dot net skills.  It provided me with the ability to edit the pages from where ever I was at home or on the road, which was a problem that I had with City Desk as I had to make changes within the City Desk program and then push out all the files to their final location.  The future of MCMS is uncertain as Microsoft is moving that technology into the latest release of Microsoft Office SharePoint Server (MOSS).  That was not the reason I am leaving this platform though, as this is a really great product, it was just impossible to host these sites anywhere but on my public exposing web server.  I really want to move all the public web sites to be hosted outside our office so that they can be expanded and extended and provide a much more stable environment.  Our office is not setup for hosting and right now our hosting needs are not all that great, but things may very well change over time.</p>\n<p>This brings us to the third content mangement system that I am switching to.  I am moving all our content over to <a href=\"http://www.dnnsoftware.com\" target=\"_blank\" rel=\"noopener\">DotNetNuke</a>.  Once again that leverages my skills as a dot net developer added with the extra benifit that GoDaddy supports this in their free hosting program natively.  This continues to give me the flexability to update the pages were ever I am, give me a better opportunity to get my pages indexed by the search engines and allow readers to link to direct pages and articles.  When I had this site hosted in my office you could not link directly to an article unless you knew the name of the page which was all hidden from view.  This may even lead to some articles that I might do about working with DotNetNuke.</p>\n<p>Over the next couple of weeks and months I want to take on some technical issues like authentication and how I have taken advantage of windows authentication but used it in the way that forms authentication provides some greater flexability.  The way that in house internal programs are built and consumed in other companies that I have worked, just bugs me to death.  There is no reason why I need to log onto every single tool that I use if I have logged onto the computer that I am using.  There is no need for this and I have developed some techinques that I will share on how I use this to work the way it should. </p>\n<p>I would also like to cover some topics that I have never covered before.  These would just be opion pieces so take them with a grain of salt, but I do want to cover some political and economic issues that have been bugging me.  If nothing else they will make you think cause I am sure my views are going to be a little different then what you might have been expecting.  I do at times have a unique view on the way I see things working and how I think that they should be working.  Keep in mind these are opionions not necessarily based on a lot of facts. </p>\n<p>I would like to talk about my conversion and on going conversion of all my web sites going the way of DotNetNuke.  This is a great content management tool that gives me a lot of flexabiltiy as the skins are easy to create, now if only I was better at graphics I could really do something with this tool, but over all the experience is quite plesant.  Modules that are not provided by what is in the DotNetNuke installation package I can create quite easily, I am after all a software developer.  Plus the fact that GoDaddy which has been my domain name registar for years is now providing some free hosting (for the price of a domain registration) and they fully support DotNetNuke as a hosting package. </p>\n"},{"title":"One Build Definition to Support Multiple Branches","date":"2017-06-01T17:23:26.000Z","_content":"Before I moved to git, I had the same situation that many of you have had when it comes to managing build definitions.  I had a build definition for each branch and for a single product this could have been several all doing the same thing.  Yea, sure they were clones of each other and all I really needed to do was to change the path to the source in each case.  Then in order to keep track of what each of these builds was for and what might have triggered it I would develop some sort of naming convention so that I could sort of tell without having to open it up.  This really felt dirty and raised a red flag for me because once again we were introducing something into our environment that was not the same, but sort of the same.  Wouldn't it be better to actually have one build definition that we can use for all these various types of builds and different branches?\n\n### Builds with Git\n{% img right /images/git-logo.jpg 200 200 \"Git\" %}\nWhen you really look at git, you learn that a branch is nothing more than a pointer to a changeset.  When you compare this to any of the centralized source control systems out there including VSTF the branch is pointing to a copy of the source control in a different location.  With that said, then I should be able to create one build definition and with a wild card be able to even trigger a Continuous Integration (CI) build by checking in code and it would use the appropriate branch.  That is absolutely true, and for the remainder of this post we will go over the simple steps to make that happen.\n\n### Same Build Definition for All Branches\nI will assume that you have a build that is working and your source code for this build is a git repository on TFS or VSTS.  Because it is a Git repo, you can specify path filters to reduce the set of files that you want to trigger a build.  According to the documentation, if you don't set path filters, the the root folder of the repo is implicitly included by default.  When you add an explicit path filter, the implicit include of the root folder is removed.\n\nThis is exactly what we want to do but we want to include a couple of different paths.  So lets start by going to the Build Definition and clicking on the Triggers sub-menu.  Make sure that the Continuous Integration switch is turned on and next pay our attention to the Branch Filters.  In my branching schema I use three (3) kinds of paths.  Master of course, as this is where all the finished and releasable code lands up.  I also use features for any new items I am implementing and I usually include the Work Item Number in my branch as well as a short description.  So an example of a feature branch for me would look something like:\n```\nfeature/3660_NewFunctionality\n```\nWith that said I have a similar path for bugs which are things that have an incorrect behavior or something that needs to be fixed.  In my branch Filters I would include 3 paths and the feature and bug would include the wild card to have everything included that is part of a feature or bug branch.\n{% asset_img CIBranchFilters.png \"CI Branch Filters\" %}\nWith this in place my commit pushed to the remote repository will kick off a new build for any new features and bugs that I have been working on.  Even better, the very same build definition kicks off when ever I complete a pull requests into Master.  Not a clone or a copy but exactly the same build.  There is never a question about what happened to the build, but rather what code change or merge did we introduce that caused this problem.\n\nBefore I discovered this I was happily flipping the branch name between my features and bugs, the definition defaulted to master.  Because of that I wasn't even bothering with CI for the development branch and the trick was to always remember to build from the correct branch.  Now I don't even have to think about that because the branch that triggered the build is the branch that is being built.  Just another thing that I could have easily screwed up is out of the picture.  I don't even have to think about kicking off a build and deployment as this just happens every time I commit my code and push those commits up to the remote Git.","source":"_posts/One-Build-Definition-to-Support-Multiple-Branches.md","raw":"---\ntitle: One Build Definition to Support Multiple Branches\ndate: 2017-06-01 10:23:26\ntags:\n- ALM\n- git\n- DevOps\n---\nBefore I moved to git, I had the same situation that many of you have had when it comes to managing build definitions.  I had a build definition for each branch and for a single product this could have been several all doing the same thing.  Yea, sure they were clones of each other and all I really needed to do was to change the path to the source in each case.  Then in order to keep track of what each of these builds was for and what might have triggered it I would develop some sort of naming convention so that I could sort of tell without having to open it up.  This really felt dirty and raised a red flag for me because once again we were introducing something into our environment that was not the same, but sort of the same.  Wouldn't it be better to actually have one build definition that we can use for all these various types of builds and different branches?\n\n### Builds with Git\n{% img right /images/git-logo.jpg 200 200 \"Git\" %}\nWhen you really look at git, you learn that a branch is nothing more than a pointer to a changeset.  When you compare this to any of the centralized source control systems out there including VSTF the branch is pointing to a copy of the source control in a different location.  With that said, then I should be able to create one build definition and with a wild card be able to even trigger a Continuous Integration (CI) build by checking in code and it would use the appropriate branch.  That is absolutely true, and for the remainder of this post we will go over the simple steps to make that happen.\n\n### Same Build Definition for All Branches\nI will assume that you have a build that is working and your source code for this build is a git repository on TFS or VSTS.  Because it is a Git repo, you can specify path filters to reduce the set of files that you want to trigger a build.  According to the documentation, if you don't set path filters, the the root folder of the repo is implicitly included by default.  When you add an explicit path filter, the implicit include of the root folder is removed.\n\nThis is exactly what we want to do but we want to include a couple of different paths.  So lets start by going to the Build Definition and clicking on the Triggers sub-menu.  Make sure that the Continuous Integration switch is turned on and next pay our attention to the Branch Filters.  In my branching schema I use three (3) kinds of paths.  Master of course, as this is where all the finished and releasable code lands up.  I also use features for any new items I am implementing and I usually include the Work Item Number in my branch as well as a short description.  So an example of a feature branch for me would look something like:\n```\nfeature/3660_NewFunctionality\n```\nWith that said I have a similar path for bugs which are things that have an incorrect behavior or something that needs to be fixed.  In my branch Filters I would include 3 paths and the feature and bug would include the wild card to have everything included that is part of a feature or bug branch.\n{% asset_img CIBranchFilters.png \"CI Branch Filters\" %}\nWith this in place my commit pushed to the remote repository will kick off a new build for any new features and bugs that I have been working on.  Even better, the very same build definition kicks off when ever I complete a pull requests into Master.  Not a clone or a copy but exactly the same build.  There is never a question about what happened to the build, but rather what code change or merge did we introduce that caused this problem.\n\nBefore I discovered this I was happily flipping the branch name between my features and bugs, the definition defaulted to master.  Because of that I wasn't even bothering with CI for the development branch and the trick was to always remember to build from the correct branch.  Now I don't even have to think about that because the branch that triggered the build is the branch that is being built.  Just another thing that I could have easily screwed up is out of the picture.  I don't even have to think about kicking off a build and deployment as this just happens every time I commit my code and push those commits up to the remote Git.","slug":"One-Build-Definition-to-Support-Multiple-Branches","published":1,"updated":"2020-01-05T00:36:05.031Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg4000is4ufvbcnl4is","content":"<p>Before I moved to git, I had the same situation that many of you have had when it comes to managing build definitions.  I had a build definition for each branch and for a single product this could have been several all doing the same thing.  Yea, sure they were clones of each other and all I really needed to do was to change the path to the source in each case.  Then in order to keep track of what each of these builds was for and what might have triggered it I would develop some sort of naming convention so that I could sort of tell without having to open it up.  This really felt dirty and raised a red flag for me because once again we were introducing something into our environment that was not the same, but sort of the same.  Wouldn’t it be better to actually have one build definition that we can use for all these various types of builds and different branches?</p>\n<h3 id=\"Builds-with-Git\"><a href=\"#Builds-with-Git\" class=\"headerlink\" title=\"Builds with Git\"></a>Builds with Git</h3><img src=\"/images/git-logo.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Git\">\n<p>When you really look at git, you learn that a branch is nothing more than a pointer to a changeset.  When you compare this to any of the centralized source control systems out there including VSTF the branch is pointing to a copy of the source control in a different location.  With that said, then I should be able to create one build definition and with a wild card be able to even trigger a Continuous Integration (CI) build by checking in code and it would use the appropriate branch.  That is absolutely true, and for the remainder of this post we will go over the simple steps to make that happen.</p>\n<h3 id=\"Same-Build-Definition-for-All-Branches\"><a href=\"#Same-Build-Definition-for-All-Branches\" class=\"headerlink\" title=\"Same Build Definition for All Branches\"></a>Same Build Definition for All Branches</h3><p>I will assume that you have a build that is working and your source code for this build is a git repository on TFS or VSTS.  Because it is a Git repo, you can specify path filters to reduce the set of files that you want to trigger a build.  According to the documentation, if you don’t set path filters, the the root folder of the repo is implicitly included by default.  When you add an explicit path filter, the implicit include of the root folder is removed.</p>\n<p>This is exactly what we want to do but we want to include a couple of different paths.  So lets start by going to the Build Definition and clicking on the Triggers sub-menu.  Make sure that the Continuous Integration switch is turned on and next pay our attention to the Branch Filters.  In my branching schema I use three (3) kinds of paths.  Master of course, as this is where all the finished and releasable code lands up.  I also use features for any new items I am implementing and I usually include the Work Item Number in my branch as well as a short description.  So an example of a feature branch for me would look something like:<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">feature/<span class=\"number\">3660</span>_NewFunctionality</span><br></pre></td></tr></table></figure></p>\n<p>With that said I have a similar path for bugs which are things that have an incorrect behavior or something that needs to be fixed.  In my branch Filters I would include 3 paths and the feature and bug would include the wild card to have everything included that is part of a feature or bug branch.<br><img src=\"/2017/06/One-Build-Definition-to-Support-Multiple-Branches/CIBranchFilters.png\" title=\"CI Branch Filters\"><br>With this in place my commit pushed to the remote repository will kick off a new build for any new features and bugs that I have been working on.  Even better, the very same build definition kicks off when ever I complete a pull requests into Master.  Not a clone or a copy but exactly the same build.  There is never a question about what happened to the build, but rather what code change or merge did we introduce that caused this problem.</p>\n<p>Before I discovered this I was happily flipping the branch name between my features and bugs, the definition defaulted to master.  Because of that I wasn’t even bothering with CI for the development branch and the trick was to always remember to build from the correct branch.  Now I don’t even have to think about that because the branch that triggered the build is the branch that is being built.  Just another thing that I could have easily screwed up is out of the picture.  I don’t even have to think about kicking off a build and deployment as this just happens every time I commit my code and push those commits up to the remote Git.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Before I moved to git, I had the same situation that many of you have had when it comes to managing build definitions.  I had a build definition for each branch and for a single product this could have been several all doing the same thing.  Yea, sure they were clones of each other and all I really needed to do was to change the path to the source in each case.  Then in order to keep track of what each of these builds was for and what might have triggered it I would develop some sort of naming convention so that I could sort of tell without having to open it up.  This really felt dirty and raised a red flag for me because once again we were introducing something into our environment that was not the same, but sort of the same.  Wouldn’t it be better to actually have one build definition that we can use for all these various types of builds and different branches?</p>\n<h3 id=\"Builds-with-Git\"><a href=\"#Builds-with-Git\" class=\"headerlink\" title=\"Builds with Git\"></a>Builds with Git</h3><img src=\"/images/git-logo.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Git\">\n<p>When you really look at git, you learn that a branch is nothing more than a pointer to a changeset.  When you compare this to any of the centralized source control systems out there including VSTF the branch is pointing to a copy of the source control in a different location.  With that said, then I should be able to create one build definition and with a wild card be able to even trigger a Continuous Integration (CI) build by checking in code and it would use the appropriate branch.  That is absolutely true, and for the remainder of this post we will go over the simple steps to make that happen.</p>\n<h3 id=\"Same-Build-Definition-for-All-Branches\"><a href=\"#Same-Build-Definition-for-All-Branches\" class=\"headerlink\" title=\"Same Build Definition for All Branches\"></a>Same Build Definition for All Branches</h3><p>I will assume that you have a build that is working and your source code for this build is a git repository on TFS or VSTS.  Because it is a Git repo, you can specify path filters to reduce the set of files that you want to trigger a build.  According to the documentation, if you don’t set path filters, the the root folder of the repo is implicitly included by default.  When you add an explicit path filter, the implicit include of the root folder is removed.</p>\n<p>This is exactly what we want to do but we want to include a couple of different paths.  So lets start by going to the Build Definition and clicking on the Triggers sub-menu.  Make sure that the Continuous Integration switch is turned on and next pay our attention to the Branch Filters.  In my branching schema I use three (3) kinds of paths.  Master of course, as this is where all the finished and releasable code lands up.  I also use features for any new items I am implementing and I usually include the Work Item Number in my branch as well as a short description.  So an example of a feature branch for me would look something like:<br><figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">feature/<span class=\"number\">3660</span>_NewFunctionality</span><br></pre></td></tr></table></figure></p>\n<p>With that said I have a similar path for bugs which are things that have an incorrect behavior or something that needs to be fixed.  In my branch Filters I would include 3 paths and the feature and bug would include the wild card to have everything included that is part of a feature or bug branch.<br><img src=\"/2017/06/One-Build-Definition-to-Support-Multiple-Branches/CIBranchFilters.png\" title=\"CI Branch Filters\"><br>With this in place my commit pushed to the remote repository will kick off a new build for any new features and bugs that I have been working on.  Even better, the very same build definition kicks off when ever I complete a pull requests into Master.  Not a clone or a copy but exactly the same build.  There is never a question about what happened to the build, but rather what code change or merge did we introduce that caused this problem.</p>\n<p>Before I discovered this I was happily flipping the branch name between my features and bugs, the definition defaulted to master.  Because of that I wasn’t even bothering with CI for the development branch and the trick was to always remember to build from the correct branch.  Now I don’t even have to think about that because the branch that triggered the build is the branch that is being built.  Just another thing that I could have easily screwed up is out of the picture.  I don’t even have to think about kicking off a build and deployment as this just happens every time I commit my code and push those commits up to the remote Git.</p>\n"},{"title":"Red Gate tools vs SQL Server Data Tools","date":"2018-05-16T17:28:44.000Z","_content":"{% img left /images/redgate.jpg 100 100 \"Redgate\" %}\nRecently I have been tasked with showing a development team how to version their SQL databases using the Red Gate tools.  Normally I mentor and give guidance in these kinds of projects using the SQL Server Data Tools but because their databases were so large they found that the SSDT approach would just not work for them.  They did find in their own experiments that the Red Gate tools did not impose these limitations and worked quite well dispite their database size.\n\nI was faced with the task of learning about a technology that I was not that familiar with.  I would provide them with guidance from an ALM and DevOps perspective.  In working with this tool I thought it would be a good exercise to blog my experience and show how these two tools are similar in what they do and yet have slightly different approaches.\n## Installing Red Gate Source Control\n{% img right /images/RedGateInstalled.png 300 300 \"Toolbar\" %}\nTo start things off you will need to install the Red Gate Source Control for SQL Server Management Studio (SSMS).  This is probably the big difference between the two approaches were the SSDT approach all centers around Visual Studio to manage the code and the source control, Red Gate does this in SSMS. The following link: [https://www.red-gate.com/products/sql-development/sql-source-control/](https://www.red-gate.com/products/sql-development/sql-source-control/) will get you to the Red Gate page where you can download a 28-day free trial or buy a license.  Installation is straight forward, just run the executable that you downloaded which will add a couple of items in the toolbar of SSMS.\n## Adding a Database to Source Control with Red Gate\nIn this walk through I am using git as my repository and in fact my remote repository is going to be in Visual Studio Team Services (VSTS).  We need to create a git repository before we can grab the code from our database.  We will start with creating a local repository and push this up to VSTS afterwards.  I use a variety of tools when it comes to git and for this first step the easiest way to get an empty git repository created is with the command prompt and my favorite way of doing that is in PowerShell with the Posh Git Extension module. [https://github.com/dahlbyk/posh-git](https://github.com/dahlbyk/posh-git)\n```\n    mkdir C:\\git\\Widget\\dbScript\n    cd C:\\git\\Widget\n    git init\n```\n  {% img left /images/RedGateInstalled.png 200 200 \"Toolbar\" %}\n  For the Red Gate solution we don't use Visual Studio at all but instead fire up SSMS.  If the SQL Source Control tab is not visible when the application finishes loading, click on the SQL Source Control button found in the tool bar, which will load up the screen.  The next part is really simple, you can select the database from the left side of the IDE and do a right click and select \"Link database to source control\" from the context sensitive menu.  Or if the SQL Source Control tab's sub menu is set to Setup, as you click on Databases that are not in source control will give you the option to Link to my source control system.  Either way it will get you to the same page.  Select that first option (Link to my source control system) and click on the Next button.\n\nThe screen should now show you a selection of repository types that it supports.  Even though we are going to VSTS (TFS in the cloud) we don't want to use that Source control system type but instead select Git.  For the folder point it to the location of the dbScript folder. (C:\\git\\Widget\\dbScript) and click on the Link button.  Click on the OK button to close the confirmation screen and you should see that your database icon in SSMS is now green indicating that it is linked to Source Control.  There is no source in this container quite yet.\n\nClick on the Commit sub tab of the SQL Source Control and there you will see a list of SQL files that just linking to the source control has found out of the database.  These files however, are not in your git repository workspace yet.  You will need to enter a commit \"Initial Commit\" is a good one for the first commit and press the Commit button.  The files are now in the folder you designated and checked into your local git repository.  If we had the remote setup there might have been a visible option to push these changes up to the remote.  Instead we will do this manually now.\n{% asset_img NewWidgitRepo.png \"New Git Repository\" %}\nGo to your account on VSTS and create a new git repository call it Widget just to keep things consistent.  Don't add any extras to this like the README or the .gitignore options that are available to you.  We want a clean empty repository that we can push to easily.  Click on the Create button.  After the repository is created it will take you to the Widgit is empty page.  \n\n{% asset_img pushRepoCopyButton.png \"Push Repo to VSTS\" %}\nClick on the copy button for the \"or push an existing repository from command line\" section of this page.  Go to the PowerShell command window that you used to create the local git repository and paste it in and run the two lines.  This will push your local repository back up to the remote server and now you have your database versioned in Source Control.  We will come back to this in a minute, but first lets see what we need to do to get a SQL Database into Source Control using the SSDT.\n\n## Adding a Database to Source Control with SSDT\nFor this exercise we start in Visual Studio and create a new Project.  For the Project type select \"SQL Server Database Project\" which you will find under the Other Languages in side the SQL Server item of the New Project type tree.  We could use the very same database that we used in the previous exercise \"Widget\" and now that we have a git repository started for our experiment we can push it there as well.  So to make sure we have a nice clean separation of these two approaches lets make a new folder in our git repository called SSDT and make this the same level as the dbScript folder that we used in the Red Gate experiment.\n\nCall this Project Widget and the location should be C:\\git\\Widget\\SSDT if you have been following along.  Uncheck both the Create directory for solution and Create new Git repository and click on the OK button.  At this point all we have done is created a project in Visual Studio and set the location but there is no Database source.  We are missing a step so lets go and do that now.\n{% asset_img ImportDb.png \"Import the Database\" %}\nIn Visual Studio right click on the Widget project and from the context menu that appears select Import and Database.  This opens a dialog box where you can select a connection to the database of the schema we want to import.  From there you can find the same Widget database that we used for the Red Gate solution.  Once a database has been selected the Start button becomes enabled so click it.  This will startup a wizard that will go through the database and pull all the things it find about the database.  This is similar to what the Red Gate tool did when we link a database to Source Control.  When it completes click on the Finish button.  You will now see a bunch of folders with files, which are source code from the database.  It will be structured a little different than the Red Gate tool but the same principal.  I also had about 120 errors and that was because the Widget database that I am using is a Red Gate sample database and it has a number of unit tests that it uses to demonstrate the unit testing capabilities of the Red Gate Unit tester and it is not compatible to what I am pulling into Visual Studio there are a number of references that are outside the pure SQL approach.  I just deleted the folder called tSQLt and all those errors went away with it.\n\nIf you go to the Team Explorer tab and click on the Changes tab you will see all the files were you just need to enter a comment like Initial Commit for SSDT and click on the Commit all and Sync as we are in the same git repository that we setup for the Red Gate solution so it will push these changes up to that same VSTS repository.\n\nAll done, now you have the Database in source control from two different solutions but overall the same similar results, the tools that you use are different.  For SSDT we used Visual Studio and for the Red Gate solution we used SSMS.  Lets go back to our Red Gate Solution and see how we can get an Artifact built in the build system that we can use to deploy.\n\n## Creating the Artifact from Red Gate Source\nOkay so now we have our database source in Source Control lets make sure our target database is up to date.  To do that we will start with an artifact which will be a NuGet package that represents the snapshot of the schema from the latest set of changes.  We are going to do this on the VSTS build system.  If you are not familiar with how builds work in VSTS there are several blogs and articles out there that cover this topic in depth for now I am just going to cover the steps we need to take to produce our artifact, the build.\n\nWe start by creating a new empty build as there is no template (yet) for the Red Gate Project.  When we open this new Build Definition the first thing we will need to do is set the settings in the Process section which is right at the top of the build definition.  Here we can name our Build Definition, I have called mine \"WidgetShop DLM\".  Then you need to set the Agent queue, here I have selected the Hosted VS2017 but if you have a private agent queue you certainly could use that.  The only thing you will need to do is to make sure that the Red Gate tool, DMAutomation (which is part of the SQL Toolbelt) is installed on that machine.\n\nThe second thing to do is to select our git repository where the source code for our Widget database lives.  So a few things I have set here is that the source is VSTS Git, my project which I am calling Demo and my repository called WidgetShop.  The default branch is set to master and that is fine for now.  Leave the rest of the settings as they are.\n\nYou should have an Agent Phase section (probably called Phase 1), we won't change anything about the settings of this section, but I will mention that in a single Build Definition you could have several different Phase's and each Phase could use a different Agent queue if you desired, the default is to inherit from the definition (the one we set in the Process area).  What we will do in this Phase is to add the Redgate DLM Automation 2:Build task and you do that by clicking on the green \"+\" symbol.  The easiest way to find the task we are looking for is to enter the letters \"DLM\" in the search box.  If this set of tasks has never been installed in your instance of VSTS you will be able to do that from this page, otherwise just select the \"Redgate DML Automation 2:Build\" by hovering over the item and clicking the Add button.\n\n{% asset_img DLMBuildTask.png \"DLM Build Task\" %}\n\nThere are just a few settings that we need to change here.\n- Operation: \"Build a database package from Redgate SQL Source Control\"\n- Database folder: \"Database scripts folder is a sub-folder of the VCS root\"  If you followed my steps and placed the source in the dbScripts folder then this would be the choice.  The other choice is if you did not put the source in a folder then you would make the other choice of it being in the root.\n- Subfolder Path: \"dbScripts\" Actually you can use the ellipse button on the right of this text box and navigate to the actual folder in the git repository.\n- Output NuGet package ID: \"WidgetShop\"  This will be the name of the NuGet package that gets created.\n- Temporary server type: \"SQL LocalDB (recommended)\"  Since we are using a hosted agent and it has full versions of Visual Studio installed I know that this option is available to me and simple to do.  If you have a large database that is using the full feature of SQL Server then you might want to point to an actual SQL Server and when you change to that option it will prompt your for the Server Name, Database, Authentication method and if you use SQL authentication a place for you to provide the user name and password.\n- Show advanced options and confirm that the Publish Artifact checkbox is checked.\n\nSave you definition and Queue a build.  After the build completes you can click on the link of the specific build where displayed in the sub menu is an item named Artifacts.  As you drill though that using the explorer you will see that you now have a WidgtShop.1.0.nupkg which is the package that we will be using to deploy these changes to our environments but first lets see how you would do these same process with and SSDT project.\n\n## Creating the Artifact from the SSDT Source\nWe start this process by creating a new build definition and the first thing that the wizard is going to ask you is the repository that this project is in.  Again we are using the same WidgetShop git repository in our Demo project this is exactly the same step that you did with the Red Gate Project.  For the SSDT project we take a slightly different approach for the selection of the build template.  Here we select the .NET Desktop and click on the Apply button.  This populates the buid definition with 7 tasks that are ready to go.  Right out of the gate, the Agent queue is probably pointing to \"Hosted VS2017\" and that is what we want for this demonstation but if you have a private build server you certainly are welcome to that.\n\nAt this point we can just run the build as this template will know how to find the project and build it and publish our artifact for us.  One thing you might notice here is that there is no mention of a database so it is using MSBuild to create this package directly from the source in source control.\n\nIn the end an artifact is created so if we click on the build number of the build after it has completed and click on the Artifacts sub menu, using the Artifacts explorer we walk down the tree and at the end of that rainbow we will see probably about 4 files but the one we are really interested in is the Widget.dacpac file.  This is the file that represents a snapshot in time of the database schema at the time of this build and this is the file we will use to do our deployments which we will do next.\n\n## Deploying the Red Gate Artifact\nSimilar to the build steps that we used to create the artifact, Red Gate provides another task that we use in the release definition called \"Redgate DLM Automation 2: Release\".  Lets go over the steps on how to implement that into an environment.  In my environment I am using Deployment Groups to deploy to a virtual machine that I have setup in Azure but this is exactly the same process that you would use to deploy to a private machine in your network.  I am not going to go into details about setting up Deployment Groups but for more information follow this link: [https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts](https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts)\n\nWe start this part of the exercise by clicking on the Create Release Definition button and for the template select the Empty process, which is right at the top of the list of available templates.  Then you can give your environment a proper name other than \"Environment 1\" like Dev, Stage, or Test to signify what this environment represents.  Next you will want to add the Artifacts from the build, so click on the + Add link beside the Artifacts header.  Here you will select the Project (in my case this is Demo) then select the build definition.  If you have been following the names I have been giving things then this should be the \"WidgetShop DLM\".  The Default version will default to the Latest and that is what we want and accept the Source alias that is provided.  Click on the Add button.\n\nNow we will start to add the tasks to the release in our environment so either click on the Tasks dropdown and select your task or click on the link in the environments where it probably says 1 phase, 0 task.  Either way this will bring you to the same place where you will see your environment block (another chance to rename your environment), and probably an Agent phase (which is the default).  We don't want the Agent phase so click on it to select it and then on the right hand side of the screen click on the Remove link.\n\nThis will take you back to the Environment block with a red message that your Environment should have at least one phase.  Click on the ellipse button to the right of this block and select \"Add deployment group phase\" and in the Deployment group select the Deployment group that you setup.  That is all you need to do for this part.  Next we need to add some tasks into this Deployment Group and you do that by clicking on the \"+\" button on the Deployment group phase block.  Enter DLM in the search box and if you have the \"Redgate DLM Automation 2: Release\" installed select it and click on the Add button.  If you have not installed it you can select it in the Marketplace area and click on the Install button first then add the task.\n\nWith the task added to the Deployment group phase, we configure it with the following settings.\n- Operation: choose \"Deploy database changes from a package\" from the dropdown control.\n- Package path:  The best way to select this required file is to use the ellipse button at the end of the text box and navigate to the file probably called \"WidgetShop.1.0.nupkg\" \n- Target SQL Server instance:  I have used \".\\SQLExpress\" because that is what I have installed on my machine you should but the instance name or \".\" if this is the Default instance on this Server.\n- Tarket database name:  For this I named mine \"WidgetCI\".  One other thing to note is that this database needs to exist before this tool can work.  It can be an empty database, that is fine but it must exist or else this task will fail.\n- Authentication method:  I choose \"SQL Server authentication\" from the drop down as I am not in a domain so windows authentication would have been harder to pull off.  When you choose SQL Server authentication it will prompt you for Username and Password.\n- Usernme: Enter the username for your SQL Server\n- Password: Enter the variable name for the password $(SQLPassword) and in the Variables section create a new variable SQLPassword and in the value put in the actual password and then click on the lock button to hide the actual password.\n\nWith that you can kick off a new release and the deployment to this environment will begin.\n\n## Deploying the SSDT Artifact \nTo deploy the SSDT dacpac file we follow the same steps to get an environment and get to the Deployment Group phase.  Then click on the \"+\" button on the deployment group and in the search window I just typed SQL and up at the top appeared the task I was looking for \"SQL Server Database Deploy\"\n\nWe just need to configure a few things on this task to complete this:\n- Deploy SQL Using: \"Sql Dacpac\" select this from the dropdown list.\n- DACPAC File: Click on the ellipse button and walk down the linked artifacts tree until you get down to the Dacpac file you are going to deploy.\n- Specify SQL Using: \"Server\" select this from the dropdown.\n- Server Name:  could leave this as localhost if it is the default server instance on this machine.  I used \".\\SQLExpress\" as that is the server and instance name.\n- Database Name: I named this database \"WidgetSSDT\" just to make this deployment won't override my Redgate one.  This database does not need to exist as it will be created if not found.\n- Authentication method:  I choose \"SQL Server authentication\" from the drop down.\n- SQL User name: Enter the user name for your SQL Server\n- SQL Password: Because I used the same user name and password I just entered the $(SQLPassword) variable that I setup with working with the Redgate tool task.\n\nWith that you can kick off a new release and the deployment to this environment will begin.\n\n## Conclusion\nAs you can see the two tools have a very similar model just a different approach from a different starting point.  With SSDT we start in Visual Studio and with the Redgate tools we are starting in SQL Server Management Studio.  I am sure for some DBA's that have lived most of their career in SSMS they will likely lean towards the Redgate solution.  That is okay, it does provide a good ALM/DevOps solution but you must be careful as well.  The version control is not a true git version control but does more of a sync operation.  For instance in a true git client I should be able to switch from one tool to another and because the tools should be reading directly from the repository you should be able to start in one tool and finish in another.  However the Redgate source control does not work this way.  It builds other workspaces outside of the git repo to manage some of these changes.  what may happen is if you make a change outside of SSMS, the source control in SSMS may not see your changes and things could fall out of sync.  This is also a reason why they don't have good support for branching.  You can still use branching if you create the branch first and check it out before you start to work on your changes.\n\n","source":"_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools.md","raw":"---\ntitle: Red Gate tools vs SQL Server Data Tools\ndate: 2018-05-16 10:28:44\ntags: \n- DevOps\n- ALM\n---\n{% img left /images/redgate.jpg 100 100 \"Redgate\" %}\nRecently I have been tasked with showing a development team how to version their SQL databases using the Red Gate tools.  Normally I mentor and give guidance in these kinds of projects using the SQL Server Data Tools but because their databases were so large they found that the SSDT approach would just not work for them.  They did find in their own experiments that the Red Gate tools did not impose these limitations and worked quite well dispite their database size.\n\nI was faced with the task of learning about a technology that I was not that familiar with.  I would provide them with guidance from an ALM and DevOps perspective.  In working with this tool I thought it would be a good exercise to blog my experience and show how these two tools are similar in what they do and yet have slightly different approaches.\n## Installing Red Gate Source Control\n{% img right /images/RedGateInstalled.png 300 300 \"Toolbar\" %}\nTo start things off you will need to install the Red Gate Source Control for SQL Server Management Studio (SSMS).  This is probably the big difference between the two approaches were the SSDT approach all centers around Visual Studio to manage the code and the source control, Red Gate does this in SSMS. The following link: [https://www.red-gate.com/products/sql-development/sql-source-control/](https://www.red-gate.com/products/sql-development/sql-source-control/) will get you to the Red Gate page where you can download a 28-day free trial or buy a license.  Installation is straight forward, just run the executable that you downloaded which will add a couple of items in the toolbar of SSMS.\n## Adding a Database to Source Control with Red Gate\nIn this walk through I am using git as my repository and in fact my remote repository is going to be in Visual Studio Team Services (VSTS).  We need to create a git repository before we can grab the code from our database.  We will start with creating a local repository and push this up to VSTS afterwards.  I use a variety of tools when it comes to git and for this first step the easiest way to get an empty git repository created is with the command prompt and my favorite way of doing that is in PowerShell with the Posh Git Extension module. [https://github.com/dahlbyk/posh-git](https://github.com/dahlbyk/posh-git)\n```\n    mkdir C:\\git\\Widget\\dbScript\n    cd C:\\git\\Widget\n    git init\n```\n  {% img left /images/RedGateInstalled.png 200 200 \"Toolbar\" %}\n  For the Red Gate solution we don't use Visual Studio at all but instead fire up SSMS.  If the SQL Source Control tab is not visible when the application finishes loading, click on the SQL Source Control button found in the tool bar, which will load up the screen.  The next part is really simple, you can select the database from the left side of the IDE and do a right click and select \"Link database to source control\" from the context sensitive menu.  Or if the SQL Source Control tab's sub menu is set to Setup, as you click on Databases that are not in source control will give you the option to Link to my source control system.  Either way it will get you to the same page.  Select that first option (Link to my source control system) and click on the Next button.\n\nThe screen should now show you a selection of repository types that it supports.  Even though we are going to VSTS (TFS in the cloud) we don't want to use that Source control system type but instead select Git.  For the folder point it to the location of the dbScript folder. (C:\\git\\Widget\\dbScript) and click on the Link button.  Click on the OK button to close the confirmation screen and you should see that your database icon in SSMS is now green indicating that it is linked to Source Control.  There is no source in this container quite yet.\n\nClick on the Commit sub tab of the SQL Source Control and there you will see a list of SQL files that just linking to the source control has found out of the database.  These files however, are not in your git repository workspace yet.  You will need to enter a commit \"Initial Commit\" is a good one for the first commit and press the Commit button.  The files are now in the folder you designated and checked into your local git repository.  If we had the remote setup there might have been a visible option to push these changes up to the remote.  Instead we will do this manually now.\n{% asset_img NewWidgitRepo.png \"New Git Repository\" %}\nGo to your account on VSTS and create a new git repository call it Widget just to keep things consistent.  Don't add any extras to this like the README or the .gitignore options that are available to you.  We want a clean empty repository that we can push to easily.  Click on the Create button.  After the repository is created it will take you to the Widgit is empty page.  \n\n{% asset_img pushRepoCopyButton.png \"Push Repo to VSTS\" %}\nClick on the copy button for the \"or push an existing repository from command line\" section of this page.  Go to the PowerShell command window that you used to create the local git repository and paste it in and run the two lines.  This will push your local repository back up to the remote server and now you have your database versioned in Source Control.  We will come back to this in a minute, but first lets see what we need to do to get a SQL Database into Source Control using the SSDT.\n\n## Adding a Database to Source Control with SSDT\nFor this exercise we start in Visual Studio and create a new Project.  For the Project type select \"SQL Server Database Project\" which you will find under the Other Languages in side the SQL Server item of the New Project type tree.  We could use the very same database that we used in the previous exercise \"Widget\" and now that we have a git repository started for our experiment we can push it there as well.  So to make sure we have a nice clean separation of these two approaches lets make a new folder in our git repository called SSDT and make this the same level as the dbScript folder that we used in the Red Gate experiment.\n\nCall this Project Widget and the location should be C:\\git\\Widget\\SSDT if you have been following along.  Uncheck both the Create directory for solution and Create new Git repository and click on the OK button.  At this point all we have done is created a project in Visual Studio and set the location but there is no Database source.  We are missing a step so lets go and do that now.\n{% asset_img ImportDb.png \"Import the Database\" %}\nIn Visual Studio right click on the Widget project and from the context menu that appears select Import and Database.  This opens a dialog box where you can select a connection to the database of the schema we want to import.  From there you can find the same Widget database that we used for the Red Gate solution.  Once a database has been selected the Start button becomes enabled so click it.  This will startup a wizard that will go through the database and pull all the things it find about the database.  This is similar to what the Red Gate tool did when we link a database to Source Control.  When it completes click on the Finish button.  You will now see a bunch of folders with files, which are source code from the database.  It will be structured a little different than the Red Gate tool but the same principal.  I also had about 120 errors and that was because the Widget database that I am using is a Red Gate sample database and it has a number of unit tests that it uses to demonstrate the unit testing capabilities of the Red Gate Unit tester and it is not compatible to what I am pulling into Visual Studio there are a number of references that are outside the pure SQL approach.  I just deleted the folder called tSQLt and all those errors went away with it.\n\nIf you go to the Team Explorer tab and click on the Changes tab you will see all the files were you just need to enter a comment like Initial Commit for SSDT and click on the Commit all and Sync as we are in the same git repository that we setup for the Red Gate solution so it will push these changes up to that same VSTS repository.\n\nAll done, now you have the Database in source control from two different solutions but overall the same similar results, the tools that you use are different.  For SSDT we used Visual Studio and for the Red Gate solution we used SSMS.  Lets go back to our Red Gate Solution and see how we can get an Artifact built in the build system that we can use to deploy.\n\n## Creating the Artifact from Red Gate Source\nOkay so now we have our database source in Source Control lets make sure our target database is up to date.  To do that we will start with an artifact which will be a NuGet package that represents the snapshot of the schema from the latest set of changes.  We are going to do this on the VSTS build system.  If you are not familiar with how builds work in VSTS there are several blogs and articles out there that cover this topic in depth for now I am just going to cover the steps we need to take to produce our artifact, the build.\n\nWe start by creating a new empty build as there is no template (yet) for the Red Gate Project.  When we open this new Build Definition the first thing we will need to do is set the settings in the Process section which is right at the top of the build definition.  Here we can name our Build Definition, I have called mine \"WidgetShop DLM\".  Then you need to set the Agent queue, here I have selected the Hosted VS2017 but if you have a private agent queue you certainly could use that.  The only thing you will need to do is to make sure that the Red Gate tool, DMAutomation (which is part of the SQL Toolbelt) is installed on that machine.\n\nThe second thing to do is to select our git repository where the source code for our Widget database lives.  So a few things I have set here is that the source is VSTS Git, my project which I am calling Demo and my repository called WidgetShop.  The default branch is set to master and that is fine for now.  Leave the rest of the settings as they are.\n\nYou should have an Agent Phase section (probably called Phase 1), we won't change anything about the settings of this section, but I will mention that in a single Build Definition you could have several different Phase's and each Phase could use a different Agent queue if you desired, the default is to inherit from the definition (the one we set in the Process area).  What we will do in this Phase is to add the Redgate DLM Automation 2:Build task and you do that by clicking on the green \"+\" symbol.  The easiest way to find the task we are looking for is to enter the letters \"DLM\" in the search box.  If this set of tasks has never been installed in your instance of VSTS you will be able to do that from this page, otherwise just select the \"Redgate DML Automation 2:Build\" by hovering over the item and clicking the Add button.\n\n{% asset_img DLMBuildTask.png \"DLM Build Task\" %}\n\nThere are just a few settings that we need to change here.\n- Operation: \"Build a database package from Redgate SQL Source Control\"\n- Database folder: \"Database scripts folder is a sub-folder of the VCS root\"  If you followed my steps and placed the source in the dbScripts folder then this would be the choice.  The other choice is if you did not put the source in a folder then you would make the other choice of it being in the root.\n- Subfolder Path: \"dbScripts\" Actually you can use the ellipse button on the right of this text box and navigate to the actual folder in the git repository.\n- Output NuGet package ID: \"WidgetShop\"  This will be the name of the NuGet package that gets created.\n- Temporary server type: \"SQL LocalDB (recommended)\"  Since we are using a hosted agent and it has full versions of Visual Studio installed I know that this option is available to me and simple to do.  If you have a large database that is using the full feature of SQL Server then you might want to point to an actual SQL Server and when you change to that option it will prompt your for the Server Name, Database, Authentication method and if you use SQL authentication a place for you to provide the user name and password.\n- Show advanced options and confirm that the Publish Artifact checkbox is checked.\n\nSave you definition and Queue a build.  After the build completes you can click on the link of the specific build where displayed in the sub menu is an item named Artifacts.  As you drill though that using the explorer you will see that you now have a WidgtShop.1.0.nupkg which is the package that we will be using to deploy these changes to our environments but first lets see how you would do these same process with and SSDT project.\n\n## Creating the Artifact from the SSDT Source\nWe start this process by creating a new build definition and the first thing that the wizard is going to ask you is the repository that this project is in.  Again we are using the same WidgetShop git repository in our Demo project this is exactly the same step that you did with the Red Gate Project.  For the SSDT project we take a slightly different approach for the selection of the build template.  Here we select the .NET Desktop and click on the Apply button.  This populates the buid definition with 7 tasks that are ready to go.  Right out of the gate, the Agent queue is probably pointing to \"Hosted VS2017\" and that is what we want for this demonstation but if you have a private build server you certainly are welcome to that.\n\nAt this point we can just run the build as this template will know how to find the project and build it and publish our artifact for us.  One thing you might notice here is that there is no mention of a database so it is using MSBuild to create this package directly from the source in source control.\n\nIn the end an artifact is created so if we click on the build number of the build after it has completed and click on the Artifacts sub menu, using the Artifacts explorer we walk down the tree and at the end of that rainbow we will see probably about 4 files but the one we are really interested in is the Widget.dacpac file.  This is the file that represents a snapshot in time of the database schema at the time of this build and this is the file we will use to do our deployments which we will do next.\n\n## Deploying the Red Gate Artifact\nSimilar to the build steps that we used to create the artifact, Red Gate provides another task that we use in the release definition called \"Redgate DLM Automation 2: Release\".  Lets go over the steps on how to implement that into an environment.  In my environment I am using Deployment Groups to deploy to a virtual machine that I have setup in Azure but this is exactly the same process that you would use to deploy to a private machine in your network.  I am not going to go into details about setting up Deployment Groups but for more information follow this link: [https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts](https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts)\n\nWe start this part of the exercise by clicking on the Create Release Definition button and for the template select the Empty process, which is right at the top of the list of available templates.  Then you can give your environment a proper name other than \"Environment 1\" like Dev, Stage, or Test to signify what this environment represents.  Next you will want to add the Artifacts from the build, so click on the + Add link beside the Artifacts header.  Here you will select the Project (in my case this is Demo) then select the build definition.  If you have been following the names I have been giving things then this should be the \"WidgetShop DLM\".  The Default version will default to the Latest and that is what we want and accept the Source alias that is provided.  Click on the Add button.\n\nNow we will start to add the tasks to the release in our environment so either click on the Tasks dropdown and select your task or click on the link in the environments where it probably says 1 phase, 0 task.  Either way this will bring you to the same place where you will see your environment block (another chance to rename your environment), and probably an Agent phase (which is the default).  We don't want the Agent phase so click on it to select it and then on the right hand side of the screen click on the Remove link.\n\nThis will take you back to the Environment block with a red message that your Environment should have at least one phase.  Click on the ellipse button to the right of this block and select \"Add deployment group phase\" and in the Deployment group select the Deployment group that you setup.  That is all you need to do for this part.  Next we need to add some tasks into this Deployment Group and you do that by clicking on the \"+\" button on the Deployment group phase block.  Enter DLM in the search box and if you have the \"Redgate DLM Automation 2: Release\" installed select it and click on the Add button.  If you have not installed it you can select it in the Marketplace area and click on the Install button first then add the task.\n\nWith the task added to the Deployment group phase, we configure it with the following settings.\n- Operation: choose \"Deploy database changes from a package\" from the dropdown control.\n- Package path:  The best way to select this required file is to use the ellipse button at the end of the text box and navigate to the file probably called \"WidgetShop.1.0.nupkg\" \n- Target SQL Server instance:  I have used \".\\SQLExpress\" because that is what I have installed on my machine you should but the instance name or \".\" if this is the Default instance on this Server.\n- Tarket database name:  For this I named mine \"WidgetCI\".  One other thing to note is that this database needs to exist before this tool can work.  It can be an empty database, that is fine but it must exist or else this task will fail.\n- Authentication method:  I choose \"SQL Server authentication\" from the drop down as I am not in a domain so windows authentication would have been harder to pull off.  When you choose SQL Server authentication it will prompt you for Username and Password.\n- Usernme: Enter the username for your SQL Server\n- Password: Enter the variable name for the password $(SQLPassword) and in the Variables section create a new variable SQLPassword and in the value put in the actual password and then click on the lock button to hide the actual password.\n\nWith that you can kick off a new release and the deployment to this environment will begin.\n\n## Deploying the SSDT Artifact \nTo deploy the SSDT dacpac file we follow the same steps to get an environment and get to the Deployment Group phase.  Then click on the \"+\" button on the deployment group and in the search window I just typed SQL and up at the top appeared the task I was looking for \"SQL Server Database Deploy\"\n\nWe just need to configure a few things on this task to complete this:\n- Deploy SQL Using: \"Sql Dacpac\" select this from the dropdown list.\n- DACPAC File: Click on the ellipse button and walk down the linked artifacts tree until you get down to the Dacpac file you are going to deploy.\n- Specify SQL Using: \"Server\" select this from the dropdown.\n- Server Name:  could leave this as localhost if it is the default server instance on this machine.  I used \".\\SQLExpress\" as that is the server and instance name.\n- Database Name: I named this database \"WidgetSSDT\" just to make this deployment won't override my Redgate one.  This database does not need to exist as it will be created if not found.\n- Authentication method:  I choose \"SQL Server authentication\" from the drop down.\n- SQL User name: Enter the user name for your SQL Server\n- SQL Password: Because I used the same user name and password I just entered the $(SQLPassword) variable that I setup with working with the Redgate tool task.\n\nWith that you can kick off a new release and the deployment to this environment will begin.\n\n## Conclusion\nAs you can see the two tools have a very similar model just a different approach from a different starting point.  With SSDT we start in Visual Studio and with the Redgate tools we are starting in SQL Server Management Studio.  I am sure for some DBA's that have lived most of their career in SSMS they will likely lean towards the Redgate solution.  That is okay, it does provide a good ALM/DevOps solution but you must be careful as well.  The version control is not a true git version control but does more of a sync operation.  For instance in a true git client I should be able to switch from one tool to another and because the tools should be reading directly from the repository you should be able to start in one tool and finish in another.  However the Redgate source control does not work this way.  It builds other workspaces outside of the git repo to manage some of these changes.  what may happen is if you make a change outside of SSMS, the source control in SSMS may not see your changes and things could fall out of sync.  This is also a reason why they don't have good support for branching.  You can still use branching if you create the branch first and check it out before you start to work on your changes.\n\n","slug":"Red-Gate-tools-vs-SQL-Server-Data-Tools","published":1,"updated":"2020-01-05T00:36:05.033Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg5000js4ufwybzfnwq","content":"<img src=\"/images/redgate.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"Redgate\">\n<p>Recently I have been tasked with showing a development team how to version their SQL databases using the Red Gate tools.  Normally I mentor and give guidance in these kinds of projects using the SQL Server Data Tools but because their databases were so large they found that the SSDT approach would just not work for them.  They did find in their own experiments that the Red Gate tools did not impose these limitations and worked quite well dispite their database size.</p>\n<p>I was faced with the task of learning about a technology that I was not that familiar with.  I would provide them with guidance from an ALM and DevOps perspective.  In working with this tool I thought it would be a good exercise to blog my experience and show how these two tools are similar in what they do and yet have slightly different approaches.</p>\n<h2 id=\"Installing-Red-Gate-Source-Control\"><a href=\"#Installing-Red-Gate-Source-Control\" class=\"headerlink\" title=\"Installing Red Gate Source Control\"></a>Installing Red Gate Source Control</h2><img src=\"/images/RedGateInstalled.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Toolbar\">\n<p>To start things off you will need to install the Red Gate Source Control for SQL Server Management Studio (SSMS).  This is probably the big difference between the two approaches were the SSDT approach all centers around Visual Studio to manage the code and the source control, Red Gate does this in SSMS. The following link: <a href=\"https://www.red-gate.com/products/sql-development/sql-source-control/\" target=\"_blank\" rel=\"noopener\">https://www.red-gate.com/products/sql-development/sql-source-control/</a> will get you to the Red Gate page where you can download a 28-day free trial or buy a license.  Installation is straight forward, just run the executable that you downloaded which will add a couple of items in the toolbar of SSMS.</p>\n<h2 id=\"Adding-a-Database-to-Source-Control-with-Red-Gate\"><a href=\"#Adding-a-Database-to-Source-Control-with-Red-Gate\" class=\"headerlink\" title=\"Adding a Database to Source Control with Red Gate\"></a>Adding a Database to Source Control with Red Gate</h2><p>In this walk through I am using git as my repository and in fact my remote repository is going to be in Visual Studio Team Services (VSTS).  We need to create a git repository before we can grab the code from our database.  We will start with creating a local repository and push this up to VSTS afterwards.  I use a variety of tools when it comes to git and for this first step the easiest way to get an empty git repository created is with the command prompt and my favorite way of doing that is in PowerShell with the Posh Git Extension module. <a href=\"https://github.com/dahlbyk/posh-git\" target=\"_blank\" rel=\"noopener\">https://github.com/dahlbyk/posh-git</a><br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir C:<span class=\"symbol\">\\g</span>it<span class=\"symbol\">\\W</span>idget<span class=\"symbol\">\\d</span>bScript</span><br><span class=\"line\">cd C:<span class=\"symbol\">\\g</span>it<span class=\"symbol\">\\W</span>idget</span><br><span class=\"line\">git init</span><br></pre></td></tr></table></figure></p>\n  <img src=\"/images/RedGateInstalled.png\" class=\"left\" width=\"200\" height=\"200\" title=\"Toolbar\">\n<p>  For the Red Gate solution we don’t use Visual Studio at all but instead fire up SSMS.  If the SQL Source Control tab is not visible when the application finishes loading, click on the SQL Source Control button found in the tool bar, which will load up the screen.  The next part is really simple, you can select the database from the left side of the IDE and do a right click and select “Link database to source control” from the context sensitive menu.  Or if the SQL Source Control tab’s sub menu is set to Setup, as you click on Databases that are not in source control will give you the option to Link to my source control system.  Either way it will get you to the same page.  Select that first option (Link to my source control system) and click on the Next button.</p>\n<p>The screen should now show you a selection of repository types that it supports.  Even though we are going to VSTS (TFS in the cloud) we don’t want to use that Source control system type but instead select Git.  For the folder point it to the location of the dbScript folder. (C:\\git\\Widget\\dbScript) and click on the Link button.  Click on the OK button to close the confirmation screen and you should see that your database icon in SSMS is now green indicating that it is linked to Source Control.  There is no source in this container quite yet.</p>\n<p>Click on the Commit sub tab of the SQL Source Control and there you will see a list of SQL files that just linking to the source control has found out of the database.  These files however, are not in your git repository workspace yet.  You will need to enter a commit “Initial Commit” is a good one for the first commit and press the Commit button.  The files are now in the folder you designated and checked into your local git repository.  If we had the remote setup there might have been a visible option to push these changes up to the remote.  Instead we will do this manually now.<br><img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/NewWidgitRepo.png\" title=\"New Git Repository\"><br>Go to your account on VSTS and create a new git repository call it Widget just to keep things consistent.  Don’t add any extras to this like the README or the .gitignore options that are available to you.  We want a clean empty repository that we can push to easily.  Click on the Create button.  After the repository is created it will take you to the Widgit is empty page.  </p>\n<img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/pushRepoCopyButton.png\" title=\"Push Repo to VSTS\">\n<p>Click on the copy button for the “or push an existing repository from command line” section of this page.  Go to the PowerShell command window that you used to create the local git repository and paste it in and run the two lines.  This will push your local repository back up to the remote server and now you have your database versioned in Source Control.  We will come back to this in a minute, but first lets see what we need to do to get a SQL Database into Source Control using the SSDT.</p>\n<h2 id=\"Adding-a-Database-to-Source-Control-with-SSDT\"><a href=\"#Adding-a-Database-to-Source-Control-with-SSDT\" class=\"headerlink\" title=\"Adding a Database to Source Control with SSDT\"></a>Adding a Database to Source Control with SSDT</h2><p>For this exercise we start in Visual Studio and create a new Project.  For the Project type select “SQL Server Database Project” which you will find under the Other Languages in side the SQL Server item of the New Project type tree.  We could use the very same database that we used in the previous exercise “Widget” and now that we have a git repository started for our experiment we can push it there as well.  So to make sure we have a nice clean separation of these two approaches lets make a new folder in our git repository called SSDT and make this the same level as the dbScript folder that we used in the Red Gate experiment.</p>\n<p>Call this Project Widget and the location should be C:\\git\\Widget\\SSDT if you have been following along.  Uncheck both the Create directory for solution and Create new Git repository and click on the OK button.  At this point all we have done is created a project in Visual Studio and set the location but there is no Database source.  We are missing a step so lets go and do that now.<br><img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/ImportDb.png\" title=\"Import the Database\"><br>In Visual Studio right click on the Widget project and from the context menu that appears select Import and Database.  This opens a dialog box where you can select a connection to the database of the schema we want to import.  From there you can find the same Widget database that we used for the Red Gate solution.  Once a database has been selected the Start button becomes enabled so click it.  This will startup a wizard that will go through the database and pull all the things it find about the database.  This is similar to what the Red Gate tool did when we link a database to Source Control.  When it completes click on the Finish button.  You will now see a bunch of folders with files, which are source code from the database.  It will be structured a little different than the Red Gate tool but the same principal.  I also had about 120 errors and that was because the Widget database that I am using is a Red Gate sample database and it has a number of unit tests that it uses to demonstrate the unit testing capabilities of the Red Gate Unit tester and it is not compatible to what I am pulling into Visual Studio there are a number of references that are outside the pure SQL approach.  I just deleted the folder called tSQLt and all those errors went away with it.</p>\n<p>If you go to the Team Explorer tab and click on the Changes tab you will see all the files were you just need to enter a comment like Initial Commit for SSDT and click on the Commit all and Sync as we are in the same git repository that we setup for the Red Gate solution so it will push these changes up to that same VSTS repository.</p>\n<p>All done, now you have the Database in source control from two different solutions but overall the same similar results, the tools that you use are different.  For SSDT we used Visual Studio and for the Red Gate solution we used SSMS.  Lets go back to our Red Gate Solution and see how we can get an Artifact built in the build system that we can use to deploy.</p>\n<h2 id=\"Creating-the-Artifact-from-Red-Gate-Source\"><a href=\"#Creating-the-Artifact-from-Red-Gate-Source\" class=\"headerlink\" title=\"Creating the Artifact from Red Gate Source\"></a>Creating the Artifact from Red Gate Source</h2><p>Okay so now we have our database source in Source Control lets make sure our target database is up to date.  To do that we will start with an artifact which will be a NuGet package that represents the snapshot of the schema from the latest set of changes.  We are going to do this on the VSTS build system.  If you are not familiar with how builds work in VSTS there are several blogs and articles out there that cover this topic in depth for now I am just going to cover the steps we need to take to produce our artifact, the build.</p>\n<p>We start by creating a new empty build as there is no template (yet) for the Red Gate Project.  When we open this new Build Definition the first thing we will need to do is set the settings in the Process section which is right at the top of the build definition.  Here we can name our Build Definition, I have called mine “WidgetShop DLM”.  Then you need to set the Agent queue, here I have selected the Hosted VS2017 but if you have a private agent queue you certainly could use that.  The only thing you will need to do is to make sure that the Red Gate tool, DMAutomation (which is part of the SQL Toolbelt) is installed on that machine.</p>\n<p>The second thing to do is to select our git repository where the source code for our Widget database lives.  So a few things I have set here is that the source is VSTS Git, my project which I am calling Demo and my repository called WidgetShop.  The default branch is set to master and that is fine for now.  Leave the rest of the settings as they are.</p>\n<p>You should have an Agent Phase section (probably called Phase 1), we won’t change anything about the settings of this section, but I will mention that in a single Build Definition you could have several different Phase’s and each Phase could use a different Agent queue if you desired, the default is to inherit from the definition (the one we set in the Process area).  What we will do in this Phase is to add the Redgate DLM Automation 2:Build task and you do that by clicking on the green “+” symbol.  The easiest way to find the task we are looking for is to enter the letters “DLM” in the search box.  If this set of tasks has never been installed in your instance of VSTS you will be able to do that from this page, otherwise just select the “Redgate DML Automation 2:Build” by hovering over the item and clicking the Add button.</p>\n<img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/DLMBuildTask.png\" title=\"DLM Build Task\">\n<p>There are just a few settings that we need to change here.</p>\n<ul>\n<li>Operation: “Build a database package from Redgate SQL Source Control”</li>\n<li>Database folder: “Database scripts folder is a sub-folder of the VCS root”  If you followed my steps and placed the source in the dbScripts folder then this would be the choice.  The other choice is if you did not put the source in a folder then you would make the other choice of it being in the root.</li>\n<li>Subfolder Path: “dbScripts” Actually you can use the ellipse button on the right of this text box and navigate to the actual folder in the git repository.</li>\n<li>Output NuGet package ID: “WidgetShop”  This will be the name of the NuGet package that gets created.</li>\n<li>Temporary server type: “SQL LocalDB (recommended)”  Since we are using a hosted agent and it has full versions of Visual Studio installed I know that this option is available to me and simple to do.  If you have a large database that is using the full feature of SQL Server then you might want to point to an actual SQL Server and when you change to that option it will prompt your for the Server Name, Database, Authentication method and if you use SQL authentication a place for you to provide the user name and password.</li>\n<li>Show advanced options and confirm that the Publish Artifact checkbox is checked.</li>\n</ul>\n<p>Save you definition and Queue a build.  After the build completes you can click on the link of the specific build where displayed in the sub menu is an item named Artifacts.  As you drill though that using the explorer you will see that you now have a WidgtShop.1.0.nupkg which is the package that we will be using to deploy these changes to our environments but first lets see how you would do these same process with and SSDT project.</p>\n<h2 id=\"Creating-the-Artifact-from-the-SSDT-Source\"><a href=\"#Creating-the-Artifact-from-the-SSDT-Source\" class=\"headerlink\" title=\"Creating the Artifact from the SSDT Source\"></a>Creating the Artifact from the SSDT Source</h2><p>We start this process by creating a new build definition and the first thing that the wizard is going to ask you is the repository that this project is in.  Again we are using the same WidgetShop git repository in our Demo project this is exactly the same step that you did with the Red Gate Project.  For the SSDT project we take a slightly different approach for the selection of the build template.  Here we select the .NET Desktop and click on the Apply button.  This populates the buid definition with 7 tasks that are ready to go.  Right out of the gate, the Agent queue is probably pointing to “Hosted VS2017” and that is what we want for this demonstation but if you have a private build server you certainly are welcome to that.</p>\n<p>At this point we can just run the build as this template will know how to find the project and build it and publish our artifact for us.  One thing you might notice here is that there is no mention of a database so it is using MSBuild to create this package directly from the source in source control.</p>\n<p>In the end an artifact is created so if we click on the build number of the build after it has completed and click on the Artifacts sub menu, using the Artifacts explorer we walk down the tree and at the end of that rainbow we will see probably about 4 files but the one we are really interested in is the Widget.dacpac file.  This is the file that represents a snapshot in time of the database schema at the time of this build and this is the file we will use to do our deployments which we will do next.</p>\n<h2 id=\"Deploying-the-Red-Gate-Artifact\"><a href=\"#Deploying-the-Red-Gate-Artifact\" class=\"headerlink\" title=\"Deploying the Red Gate Artifact\"></a>Deploying the Red Gate Artifact</h2><p>Similar to the build steps that we used to create the artifact, Red Gate provides another task that we use in the release definition called “Redgate DLM Automation 2: Release”.  Lets go over the steps on how to implement that into an environment.  In my environment I am using Deployment Groups to deploy to a virtual machine that I have setup in Azure but this is exactly the same process that you would use to deploy to a private machine in your network.  I am not going to go into details about setting up Deployment Groups but for more information follow this link: <a href=\"https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts\" target=\"_blank\" rel=\"noopener\">https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts</a></p>\n<p>We start this part of the exercise by clicking on the Create Release Definition button and for the template select the Empty process, which is right at the top of the list of available templates.  Then you can give your environment a proper name other than “Environment 1” like Dev, Stage, or Test to signify what this environment represents.  Next you will want to add the Artifacts from the build, so click on the + Add link beside the Artifacts header.  Here you will select the Project (in my case this is Demo) then select the build definition.  If you have been following the names I have been giving things then this should be the “WidgetShop DLM”.  The Default version will default to the Latest and that is what we want and accept the Source alias that is provided.  Click on the Add button.</p>\n<p>Now we will start to add the tasks to the release in our environment so either click on the Tasks dropdown and select your task or click on the link in the environments where it probably says 1 phase, 0 task.  Either way this will bring you to the same place where you will see your environment block (another chance to rename your environment), and probably an Agent phase (which is the default).  We don’t want the Agent phase so click on it to select it and then on the right hand side of the screen click on the Remove link.</p>\n<p>This will take you back to the Environment block with a red message that your Environment should have at least one phase.  Click on the ellipse button to the right of this block and select “Add deployment group phase” and in the Deployment group select the Deployment group that you setup.  That is all you need to do for this part.  Next we need to add some tasks into this Deployment Group and you do that by clicking on the “+” button on the Deployment group phase block.  Enter DLM in the search box and if you have the “Redgate DLM Automation 2: Release” installed select it and click on the Add button.  If you have not installed it you can select it in the Marketplace area and click on the Install button first then add the task.</p>\n<p>With the task added to the Deployment group phase, we configure it with the following settings.</p>\n<ul>\n<li>Operation: choose “Deploy database changes from a package” from the dropdown control.</li>\n<li>Package path:  The best way to select this required file is to use the ellipse button at the end of the text box and navigate to the file probably called “WidgetShop.1.0.nupkg” </li>\n<li>Target SQL Server instance:  I have used “.\\SQLExpress” because that is what I have installed on my machine you should but the instance name or “.” if this is the Default instance on this Server.</li>\n<li>Tarket database name:  For this I named mine “WidgetCI”.  One other thing to note is that this database needs to exist before this tool can work.  It can be an empty database, that is fine but it must exist or else this task will fail.</li>\n<li>Authentication method:  I choose “SQL Server authentication” from the drop down as I am not in a domain so windows authentication would have been harder to pull off.  When you choose SQL Server authentication it will prompt you for Username and Password.</li>\n<li>Usernme: Enter the username for your SQL Server</li>\n<li>Password: Enter the variable name for the password $(SQLPassword) and in the Variables section create a new variable SQLPassword and in the value put in the actual password and then click on the lock button to hide the actual password.</li>\n</ul>\n<p>With that you can kick off a new release and the deployment to this environment will begin.</p>\n<h2 id=\"Deploying-the-SSDT-Artifact\"><a href=\"#Deploying-the-SSDT-Artifact\" class=\"headerlink\" title=\"Deploying the SSDT Artifact\"></a>Deploying the SSDT Artifact</h2><p>To deploy the SSDT dacpac file we follow the same steps to get an environment and get to the Deployment Group phase.  Then click on the “+” button on the deployment group and in the search window I just typed SQL and up at the top appeared the task I was looking for “SQL Server Database Deploy”</p>\n<p>We just need to configure a few things on this task to complete this:</p>\n<ul>\n<li>Deploy SQL Using: “Sql Dacpac” select this from the dropdown list.</li>\n<li>DACPAC File: Click on the ellipse button and walk down the linked artifacts tree until you get down to the Dacpac file you are going to deploy.</li>\n<li>Specify SQL Using: “Server” select this from the dropdown.</li>\n<li>Server Name:  could leave this as localhost if it is the default server instance on this machine.  I used “.\\SQLExpress” as that is the server and instance name.</li>\n<li>Database Name: I named this database “WidgetSSDT” just to make this deployment won’t override my Redgate one.  This database does not need to exist as it will be created if not found.</li>\n<li>Authentication method:  I choose “SQL Server authentication” from the drop down.</li>\n<li>SQL User name: Enter the user name for your SQL Server</li>\n<li>SQL Password: Because I used the same user name and password I just entered the $(SQLPassword) variable that I setup with working with the Redgate tool task.</li>\n</ul>\n<p>With that you can kick off a new release and the deployment to this environment will begin.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>As you can see the two tools have a very similar model just a different approach from a different starting point.  With SSDT we start in Visual Studio and with the Redgate tools we are starting in SQL Server Management Studio.  I am sure for some DBA’s that have lived most of their career in SSMS they will likely lean towards the Redgate solution.  That is okay, it does provide a good ALM/DevOps solution but you must be careful as well.  The version control is not a true git version control but does more of a sync operation.  For instance in a true git client I should be able to switch from one tool to another and because the tools should be reading directly from the repository you should be able to start in one tool and finish in another.  However the Redgate source control does not work this way.  It builds other workspaces outside of the git repo to manage some of these changes.  what may happen is if you make a change outside of SSMS, the source control in SSMS may not see your changes and things could fall out of sync.  This is also a reason why they don’t have good support for branching.  You can still use branching if you create the branch first and check it out before you start to work on your changes.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/redgate.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"Redgate\">\n<p>Recently I have been tasked with showing a development team how to version their SQL databases using the Red Gate tools.  Normally I mentor and give guidance in these kinds of projects using the SQL Server Data Tools but because their databases were so large they found that the SSDT approach would just not work for them.  They did find in their own experiments that the Red Gate tools did not impose these limitations and worked quite well dispite their database size.</p>\n<p>I was faced with the task of learning about a technology that I was not that familiar with.  I would provide them with guidance from an ALM and DevOps perspective.  In working with this tool I thought it would be a good exercise to blog my experience and show how these two tools are similar in what they do and yet have slightly different approaches.</p>\n<h2 id=\"Installing-Red-Gate-Source-Control\"><a href=\"#Installing-Red-Gate-Source-Control\" class=\"headerlink\" title=\"Installing Red Gate Source Control\"></a>Installing Red Gate Source Control</h2><img src=\"/images/RedGateInstalled.png\" class=\"right\" width=\"300\" height=\"300\" title=\"Toolbar\">\n<p>To start things off you will need to install the Red Gate Source Control for SQL Server Management Studio (SSMS).  This is probably the big difference between the two approaches were the SSDT approach all centers around Visual Studio to manage the code and the source control, Red Gate does this in SSMS. The following link: <a href=\"https://www.red-gate.com/products/sql-development/sql-source-control/\" target=\"_blank\" rel=\"noopener\">https://www.red-gate.com/products/sql-development/sql-source-control/</a> will get you to the Red Gate page where you can download a 28-day free trial or buy a license.  Installation is straight forward, just run the executable that you downloaded which will add a couple of items in the toolbar of SSMS.</p>\n<h2 id=\"Adding-a-Database-to-Source-Control-with-Red-Gate\"><a href=\"#Adding-a-Database-to-Source-Control-with-Red-Gate\" class=\"headerlink\" title=\"Adding a Database to Source Control with Red Gate\"></a>Adding a Database to Source Control with Red Gate</h2><p>In this walk through I am using git as my repository and in fact my remote repository is going to be in Visual Studio Team Services (VSTS).  We need to create a git repository before we can grab the code from our database.  We will start with creating a local repository and push this up to VSTS afterwards.  I use a variety of tools when it comes to git and for this first step the easiest way to get an empty git repository created is with the command prompt and my favorite way of doing that is in PowerShell with the Posh Git Extension module. <a href=\"https://github.com/dahlbyk/posh-git\" target=\"_blank\" rel=\"noopener\">https://github.com/dahlbyk/posh-git</a><br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir C:<span class=\"symbol\">\\g</span>it<span class=\"symbol\">\\W</span>idget<span class=\"symbol\">\\d</span>bScript</span><br><span class=\"line\">cd C:<span class=\"symbol\">\\g</span>it<span class=\"symbol\">\\W</span>idget</span><br><span class=\"line\">git init</span><br></pre></td></tr></table></figure></p>\n  <img src=\"/images/RedGateInstalled.png\" class=\"left\" width=\"200\" height=\"200\" title=\"Toolbar\">\n<p>  For the Red Gate solution we don’t use Visual Studio at all but instead fire up SSMS.  If the SQL Source Control tab is not visible when the application finishes loading, click on the SQL Source Control button found in the tool bar, which will load up the screen.  The next part is really simple, you can select the database from the left side of the IDE and do a right click and select “Link database to source control” from the context sensitive menu.  Or if the SQL Source Control tab’s sub menu is set to Setup, as you click on Databases that are not in source control will give you the option to Link to my source control system.  Either way it will get you to the same page.  Select that first option (Link to my source control system) and click on the Next button.</p>\n<p>The screen should now show you a selection of repository types that it supports.  Even though we are going to VSTS (TFS in the cloud) we don’t want to use that Source control system type but instead select Git.  For the folder point it to the location of the dbScript folder. (C:\\git\\Widget\\dbScript) and click on the Link button.  Click on the OK button to close the confirmation screen and you should see that your database icon in SSMS is now green indicating that it is linked to Source Control.  There is no source in this container quite yet.</p>\n<p>Click on the Commit sub tab of the SQL Source Control and there you will see a list of SQL files that just linking to the source control has found out of the database.  These files however, are not in your git repository workspace yet.  You will need to enter a commit “Initial Commit” is a good one for the first commit and press the Commit button.  The files are now in the folder you designated and checked into your local git repository.  If we had the remote setup there might have been a visible option to push these changes up to the remote.  Instead we will do this manually now.<br><img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/NewWidgitRepo.png\" title=\"New Git Repository\"><br>Go to your account on VSTS and create a new git repository call it Widget just to keep things consistent.  Don’t add any extras to this like the README or the .gitignore options that are available to you.  We want a clean empty repository that we can push to easily.  Click on the Create button.  After the repository is created it will take you to the Widgit is empty page.  </p>\n<img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/pushRepoCopyButton.png\" title=\"Push Repo to VSTS\">\n<p>Click on the copy button for the “or push an existing repository from command line” section of this page.  Go to the PowerShell command window that you used to create the local git repository and paste it in and run the two lines.  This will push your local repository back up to the remote server and now you have your database versioned in Source Control.  We will come back to this in a minute, but first lets see what we need to do to get a SQL Database into Source Control using the SSDT.</p>\n<h2 id=\"Adding-a-Database-to-Source-Control-with-SSDT\"><a href=\"#Adding-a-Database-to-Source-Control-with-SSDT\" class=\"headerlink\" title=\"Adding a Database to Source Control with SSDT\"></a>Adding a Database to Source Control with SSDT</h2><p>For this exercise we start in Visual Studio and create a new Project.  For the Project type select “SQL Server Database Project” which you will find under the Other Languages in side the SQL Server item of the New Project type tree.  We could use the very same database that we used in the previous exercise “Widget” and now that we have a git repository started for our experiment we can push it there as well.  So to make sure we have a nice clean separation of these two approaches lets make a new folder in our git repository called SSDT and make this the same level as the dbScript folder that we used in the Red Gate experiment.</p>\n<p>Call this Project Widget and the location should be C:\\git\\Widget\\SSDT if you have been following along.  Uncheck both the Create directory for solution and Create new Git repository and click on the OK button.  At this point all we have done is created a project in Visual Studio and set the location but there is no Database source.  We are missing a step so lets go and do that now.<br><img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/ImportDb.png\" title=\"Import the Database\"><br>In Visual Studio right click on the Widget project and from the context menu that appears select Import and Database.  This opens a dialog box where you can select a connection to the database of the schema we want to import.  From there you can find the same Widget database that we used for the Red Gate solution.  Once a database has been selected the Start button becomes enabled so click it.  This will startup a wizard that will go through the database and pull all the things it find about the database.  This is similar to what the Red Gate tool did when we link a database to Source Control.  When it completes click on the Finish button.  You will now see a bunch of folders with files, which are source code from the database.  It will be structured a little different than the Red Gate tool but the same principal.  I also had about 120 errors and that was because the Widget database that I am using is a Red Gate sample database and it has a number of unit tests that it uses to demonstrate the unit testing capabilities of the Red Gate Unit tester and it is not compatible to what I am pulling into Visual Studio there are a number of references that are outside the pure SQL approach.  I just deleted the folder called tSQLt and all those errors went away with it.</p>\n<p>If you go to the Team Explorer tab and click on the Changes tab you will see all the files were you just need to enter a comment like Initial Commit for SSDT and click on the Commit all and Sync as we are in the same git repository that we setup for the Red Gate solution so it will push these changes up to that same VSTS repository.</p>\n<p>All done, now you have the Database in source control from two different solutions but overall the same similar results, the tools that you use are different.  For SSDT we used Visual Studio and for the Red Gate solution we used SSMS.  Lets go back to our Red Gate Solution and see how we can get an Artifact built in the build system that we can use to deploy.</p>\n<h2 id=\"Creating-the-Artifact-from-Red-Gate-Source\"><a href=\"#Creating-the-Artifact-from-Red-Gate-Source\" class=\"headerlink\" title=\"Creating the Artifact from Red Gate Source\"></a>Creating the Artifact from Red Gate Source</h2><p>Okay so now we have our database source in Source Control lets make sure our target database is up to date.  To do that we will start with an artifact which will be a NuGet package that represents the snapshot of the schema from the latest set of changes.  We are going to do this on the VSTS build system.  If you are not familiar with how builds work in VSTS there are several blogs and articles out there that cover this topic in depth for now I am just going to cover the steps we need to take to produce our artifact, the build.</p>\n<p>We start by creating a new empty build as there is no template (yet) for the Red Gate Project.  When we open this new Build Definition the first thing we will need to do is set the settings in the Process section which is right at the top of the build definition.  Here we can name our Build Definition, I have called mine “WidgetShop DLM”.  Then you need to set the Agent queue, here I have selected the Hosted VS2017 but if you have a private agent queue you certainly could use that.  The only thing you will need to do is to make sure that the Red Gate tool, DMAutomation (which is part of the SQL Toolbelt) is installed on that machine.</p>\n<p>The second thing to do is to select our git repository where the source code for our Widget database lives.  So a few things I have set here is that the source is VSTS Git, my project which I am calling Demo and my repository called WidgetShop.  The default branch is set to master and that is fine for now.  Leave the rest of the settings as they are.</p>\n<p>You should have an Agent Phase section (probably called Phase 1), we won’t change anything about the settings of this section, but I will mention that in a single Build Definition you could have several different Phase’s and each Phase could use a different Agent queue if you desired, the default is to inherit from the definition (the one we set in the Process area).  What we will do in this Phase is to add the Redgate DLM Automation 2:Build task and you do that by clicking on the green “+” symbol.  The easiest way to find the task we are looking for is to enter the letters “DLM” in the search box.  If this set of tasks has never been installed in your instance of VSTS you will be able to do that from this page, otherwise just select the “Redgate DML Automation 2:Build” by hovering over the item and clicking the Add button.</p>\n<img src=\"/2018/05/Red-Gate-tools-vs-SQL-Server-Data-Tools/DLMBuildTask.png\" title=\"DLM Build Task\">\n<p>There are just a few settings that we need to change here.</p>\n<ul>\n<li>Operation: “Build a database package from Redgate SQL Source Control”</li>\n<li>Database folder: “Database scripts folder is a sub-folder of the VCS root”  If you followed my steps and placed the source in the dbScripts folder then this would be the choice.  The other choice is if you did not put the source in a folder then you would make the other choice of it being in the root.</li>\n<li>Subfolder Path: “dbScripts” Actually you can use the ellipse button on the right of this text box and navigate to the actual folder in the git repository.</li>\n<li>Output NuGet package ID: “WidgetShop”  This will be the name of the NuGet package that gets created.</li>\n<li>Temporary server type: “SQL LocalDB (recommended)”  Since we are using a hosted agent and it has full versions of Visual Studio installed I know that this option is available to me and simple to do.  If you have a large database that is using the full feature of SQL Server then you might want to point to an actual SQL Server and when you change to that option it will prompt your for the Server Name, Database, Authentication method and if you use SQL authentication a place for you to provide the user name and password.</li>\n<li>Show advanced options and confirm that the Publish Artifact checkbox is checked.</li>\n</ul>\n<p>Save you definition and Queue a build.  After the build completes you can click on the link of the specific build where displayed in the sub menu is an item named Artifacts.  As you drill though that using the explorer you will see that you now have a WidgtShop.1.0.nupkg which is the package that we will be using to deploy these changes to our environments but first lets see how you would do these same process with and SSDT project.</p>\n<h2 id=\"Creating-the-Artifact-from-the-SSDT-Source\"><a href=\"#Creating-the-Artifact-from-the-SSDT-Source\" class=\"headerlink\" title=\"Creating the Artifact from the SSDT Source\"></a>Creating the Artifact from the SSDT Source</h2><p>We start this process by creating a new build definition and the first thing that the wizard is going to ask you is the repository that this project is in.  Again we are using the same WidgetShop git repository in our Demo project this is exactly the same step that you did with the Red Gate Project.  For the SSDT project we take a slightly different approach for the selection of the build template.  Here we select the .NET Desktop and click on the Apply button.  This populates the buid definition with 7 tasks that are ready to go.  Right out of the gate, the Agent queue is probably pointing to “Hosted VS2017” and that is what we want for this demonstation but if you have a private build server you certainly are welcome to that.</p>\n<p>At this point we can just run the build as this template will know how to find the project and build it and publish our artifact for us.  One thing you might notice here is that there is no mention of a database so it is using MSBuild to create this package directly from the source in source control.</p>\n<p>In the end an artifact is created so if we click on the build number of the build after it has completed and click on the Artifacts sub menu, using the Artifacts explorer we walk down the tree and at the end of that rainbow we will see probably about 4 files but the one we are really interested in is the Widget.dacpac file.  This is the file that represents a snapshot in time of the database schema at the time of this build and this is the file we will use to do our deployments which we will do next.</p>\n<h2 id=\"Deploying-the-Red-Gate-Artifact\"><a href=\"#Deploying-the-Red-Gate-Artifact\" class=\"headerlink\" title=\"Deploying the Red Gate Artifact\"></a>Deploying the Red Gate Artifact</h2><p>Similar to the build steps that we used to create the artifact, Red Gate provides another task that we use in the release definition called “Redgate DLM Automation 2: Release”.  Lets go over the steps on how to implement that into an environment.  In my environment I am using Deployment Groups to deploy to a virtual machine that I have setup in Azure but this is exactly the same process that you would use to deploy to a private machine in your network.  I am not going to go into details about setting up Deployment Groups but for more information follow this link: <a href=\"https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts\" target=\"_blank\" rel=\"noopener\">https://docs.microsoft.com/en-us/vsts/pipelines/release/deployment-groups/index?view=vsts</a></p>\n<p>We start this part of the exercise by clicking on the Create Release Definition button and for the template select the Empty process, which is right at the top of the list of available templates.  Then you can give your environment a proper name other than “Environment 1” like Dev, Stage, or Test to signify what this environment represents.  Next you will want to add the Artifacts from the build, so click on the + Add link beside the Artifacts header.  Here you will select the Project (in my case this is Demo) then select the build definition.  If you have been following the names I have been giving things then this should be the “WidgetShop DLM”.  The Default version will default to the Latest and that is what we want and accept the Source alias that is provided.  Click on the Add button.</p>\n<p>Now we will start to add the tasks to the release in our environment so either click on the Tasks dropdown and select your task or click on the link in the environments where it probably says 1 phase, 0 task.  Either way this will bring you to the same place where you will see your environment block (another chance to rename your environment), and probably an Agent phase (which is the default).  We don’t want the Agent phase so click on it to select it and then on the right hand side of the screen click on the Remove link.</p>\n<p>This will take you back to the Environment block with a red message that your Environment should have at least one phase.  Click on the ellipse button to the right of this block and select “Add deployment group phase” and in the Deployment group select the Deployment group that you setup.  That is all you need to do for this part.  Next we need to add some tasks into this Deployment Group and you do that by clicking on the “+” button on the Deployment group phase block.  Enter DLM in the search box and if you have the “Redgate DLM Automation 2: Release” installed select it and click on the Add button.  If you have not installed it you can select it in the Marketplace area and click on the Install button first then add the task.</p>\n<p>With the task added to the Deployment group phase, we configure it with the following settings.</p>\n<ul>\n<li>Operation: choose “Deploy database changes from a package” from the dropdown control.</li>\n<li>Package path:  The best way to select this required file is to use the ellipse button at the end of the text box and navigate to the file probably called “WidgetShop.1.0.nupkg” </li>\n<li>Target SQL Server instance:  I have used “.\\SQLExpress” because that is what I have installed on my machine you should but the instance name or “.” if this is the Default instance on this Server.</li>\n<li>Tarket database name:  For this I named mine “WidgetCI”.  One other thing to note is that this database needs to exist before this tool can work.  It can be an empty database, that is fine but it must exist or else this task will fail.</li>\n<li>Authentication method:  I choose “SQL Server authentication” from the drop down as I am not in a domain so windows authentication would have been harder to pull off.  When you choose SQL Server authentication it will prompt you for Username and Password.</li>\n<li>Usernme: Enter the username for your SQL Server</li>\n<li>Password: Enter the variable name for the password $(SQLPassword) and in the Variables section create a new variable SQLPassword and in the value put in the actual password and then click on the lock button to hide the actual password.</li>\n</ul>\n<p>With that you can kick off a new release and the deployment to this environment will begin.</p>\n<h2 id=\"Deploying-the-SSDT-Artifact\"><a href=\"#Deploying-the-SSDT-Artifact\" class=\"headerlink\" title=\"Deploying the SSDT Artifact\"></a>Deploying the SSDT Artifact</h2><p>To deploy the SSDT dacpac file we follow the same steps to get an environment and get to the Deployment Group phase.  Then click on the “+” button on the deployment group and in the search window I just typed SQL and up at the top appeared the task I was looking for “SQL Server Database Deploy”</p>\n<p>We just need to configure a few things on this task to complete this:</p>\n<ul>\n<li>Deploy SQL Using: “Sql Dacpac” select this from the dropdown list.</li>\n<li>DACPAC File: Click on the ellipse button and walk down the linked artifacts tree until you get down to the Dacpac file you are going to deploy.</li>\n<li>Specify SQL Using: “Server” select this from the dropdown.</li>\n<li>Server Name:  could leave this as localhost if it is the default server instance on this machine.  I used “.\\SQLExpress” as that is the server and instance name.</li>\n<li>Database Name: I named this database “WidgetSSDT” just to make this deployment won’t override my Redgate one.  This database does not need to exist as it will be created if not found.</li>\n<li>Authentication method:  I choose “SQL Server authentication” from the drop down.</li>\n<li>SQL User name: Enter the user name for your SQL Server</li>\n<li>SQL Password: Because I used the same user name and password I just entered the $(SQLPassword) variable that I setup with working with the Redgate tool task.</li>\n</ul>\n<p>With that you can kick off a new release and the deployment to this environment will begin.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>As you can see the two tools have a very similar model just a different approach from a different starting point.  With SSDT we start in Visual Studio and with the Redgate tools we are starting in SQL Server Management Studio.  I am sure for some DBA’s that have lived most of their career in SSMS they will likely lean towards the Redgate solution.  That is okay, it does provide a good ALM/DevOps solution but you must be careful as well.  The version control is not a true git version control but does more of a sync operation.  For instance in a true git client I should be able to switch from one tool to another and because the tools should be reading directly from the repository you should be able to start in one tool and finish in another.  However the Redgate source control does not work this way.  It builds other workspaces outside of the git repo to manage some of these changes.  what may happen is if you make a change outside of SSMS, the source control in SSMS may not see your changes and things could fall out of sync.  This is also a reason why they don’t have good support for branching.  You can still use branching if you create the branch first and check it out before you start to work on your changes.</p>\n"},{"title":"Sending an Email to the Developer when the Build Failed","date":"2017-05-04T14:41:33.000Z","_content":"Over the many versions of TFS there were existing workarounds that allowed us to send an email to the developer that queued the build and it had failed.  Although these workarounds did work, I always felt that this should have been handled by the alert system within TFS.  What was lacking was some sort of condition that if the build failed it should go to the developer that queued it up.\n\nMore recently I was tasked to find or build another workaround that would work within the vNext version of the Build engine.  Well I started down this quest collecting api's that I could call when I thought I would have one more look at the TFS alerts, maybe there were some updates to that part of the tool.\n\n## New Notification Engine\nWhat do you know, there were alot of changes made with this engine but not for TFS 2015, these updates show up in TFS 2017 Update 1.  One more reason to update to TFS 2017 for all those still using an on premise version of TFS as this has been in VSTS for a while now.  In the remainer of this post I will walk you through the steps to implement this big improvement in the notifications and how to solve that problem of just sending an email to the developer that caused the build to fail.\n\nIf you are on VSTS or TFS 2017 Update 1, the steps are exactly the same which is nice as in my line of work I always hate having to remember two different ways for doing the same thing. \n## New Name\n{% asset_img GearIcon.png \"Click or hover over the settings icon\" %}\nFirst off, the alerts name has been changed to Notifications and you get to them by hovering  on the Gear icon and selecting Notifications.  \n{% asset_img Notifications.png \"Click on Notifications\" %}\nHowever, there is a difference in where you select this gear.  Make sure that you are in a TFS Project, if you see the drop down on the left say Projects, this would indicate that you are a level too high and in that case click on the Project and select one.  After this page loads you should see a big blue button called \"+ New\", click this button.  {% img right /images/NewButton.png 100 100 \"New Button\" %}The page changes to allow you to select \"Build\" under the Category and \"A build fails\" under the template.  After you have done this click on the \"Next\" button.\n{% asset_img NewNotification.png \"Make your selections and click the Next button\" %}\n\nThis opens up a very different looking screen but the conditions to make this work are all there.  First off we select \"Specific team members\" for the Deliver to choice, and in the Roles choice select the \"Requested by\".  This is the portion of the Notification that only selects the team member that queued up the build, in other words requested the build.\n\nAlthough we had to select a project before we could get to this Notification area, in the next section the Filter, we can select \"Any team project\" which would apply this notification to all the TFS Projects.  The filter criteria should be correct and not require any changes as this is basically gets fired off when the build has Failed.  You just click on the \"Finish\" button and the notification is ready for testing.\n{% asset_img NotificationDetails.png \"Complete the details and click the Finish button\" %}\n\n## What did the Notification area above the Project do?\nWell just before I let you go setting up your Notifications using the proper tool, I thought I would let you know what would have happened if we did not select a Project first before we went to the Notification screen.  If you do this you will notice that on the screen some of the criteria information that we used to narrow down the notification down to the developer that requested the build would not be there.  These Notifications are the subscription only ones that have been in TFS since the beginning of that product.  This does feel a bit strange to me, almost like these two concepts should be the opposite of what they currently are.  It is what it is but at long last we can now use the Notification engine to better suite our needs. \n","source":"_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed.md","raw":"---\ntitle: Sending an Email to the Developer when the Build Failed\ndate: 2017-05-04 07:41:33\ntags:\n- ALM\n---\nOver the many versions of TFS there were existing workarounds that allowed us to send an email to the developer that queued the build and it had failed.  Although these workarounds did work, I always felt that this should have been handled by the alert system within TFS.  What was lacking was some sort of condition that if the build failed it should go to the developer that queued it up.\n\nMore recently I was tasked to find or build another workaround that would work within the vNext version of the Build engine.  Well I started down this quest collecting api's that I could call when I thought I would have one more look at the TFS alerts, maybe there were some updates to that part of the tool.\n\n## New Notification Engine\nWhat do you know, there were alot of changes made with this engine but not for TFS 2015, these updates show up in TFS 2017 Update 1.  One more reason to update to TFS 2017 for all those still using an on premise version of TFS as this has been in VSTS for a while now.  In the remainer of this post I will walk you through the steps to implement this big improvement in the notifications and how to solve that problem of just sending an email to the developer that caused the build to fail.\n\nIf you are on VSTS or TFS 2017 Update 1, the steps are exactly the same which is nice as in my line of work I always hate having to remember two different ways for doing the same thing. \n## New Name\n{% asset_img GearIcon.png \"Click or hover over the settings icon\" %}\nFirst off, the alerts name has been changed to Notifications and you get to them by hovering  on the Gear icon and selecting Notifications.  \n{% asset_img Notifications.png \"Click on Notifications\" %}\nHowever, there is a difference in where you select this gear.  Make sure that you are in a TFS Project, if you see the drop down on the left say Projects, this would indicate that you are a level too high and in that case click on the Project and select one.  After this page loads you should see a big blue button called \"+ New\", click this button.  {% img right /images/NewButton.png 100 100 \"New Button\" %}The page changes to allow you to select \"Build\" under the Category and \"A build fails\" under the template.  After you have done this click on the \"Next\" button.\n{% asset_img NewNotification.png \"Make your selections and click the Next button\" %}\n\nThis opens up a very different looking screen but the conditions to make this work are all there.  First off we select \"Specific team members\" for the Deliver to choice, and in the Roles choice select the \"Requested by\".  This is the portion of the Notification that only selects the team member that queued up the build, in other words requested the build.\n\nAlthough we had to select a project before we could get to this Notification area, in the next section the Filter, we can select \"Any team project\" which would apply this notification to all the TFS Projects.  The filter criteria should be correct and not require any changes as this is basically gets fired off when the build has Failed.  You just click on the \"Finish\" button and the notification is ready for testing.\n{% asset_img NotificationDetails.png \"Complete the details and click the Finish button\" %}\n\n## What did the Notification area above the Project do?\nWell just before I let you go setting up your Notifications using the proper tool, I thought I would let you know what would have happened if we did not select a Project first before we went to the Notification screen.  If you do this you will notice that on the screen some of the criteria information that we used to narrow down the notification down to the developer that requested the build would not be there.  These Notifications are the subscription only ones that have been in TFS since the beginning of that product.  This does feel a bit strange to me, almost like these two concepts should be the opposite of what they currently are.  It is what it is but at long last we can now use the Notification engine to better suite our needs. \n","slug":"Sending-an-Email-to-the-Developer-when-the-Build-Failed","published":1,"updated":"2020-01-05T00:36:05.061Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg6000ks4ufqk1lsqql","content":"<p>Over the many versions of TFS there were existing workarounds that allowed us to send an email to the developer that queued the build and it had failed.  Although these workarounds did work, I always felt that this should have been handled by the alert system within TFS.  What was lacking was some sort of condition that if the build failed it should go to the developer that queued it up.</p>\n<p>More recently I was tasked to find or build another workaround that would work within the vNext version of the Build engine.  Well I started down this quest collecting api’s that I could call when I thought I would have one more look at the TFS alerts, maybe there were some updates to that part of the tool.</p>\n<h2 id=\"New-Notification-Engine\"><a href=\"#New-Notification-Engine\" class=\"headerlink\" title=\"New Notification Engine\"></a>New Notification Engine</h2><p>What do you know, there were alot of changes made with this engine but not for TFS 2015, these updates show up in TFS 2017 Update 1.  One more reason to update to TFS 2017 for all those still using an on premise version of TFS as this has been in VSTS for a while now.  In the remainer of this post I will walk you through the steps to implement this big improvement in the notifications and how to solve that problem of just sending an email to the developer that caused the build to fail.</p>\n<p>If you are on VSTS or TFS 2017 Update 1, the steps are exactly the same which is nice as in my line of work I always hate having to remember two different ways for doing the same thing. </p>\n<h2 id=\"New-Name\"><a href=\"#New-Name\" class=\"headerlink\" title=\"New Name\"></a>New Name</h2><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/GearIcon.png\" title=\"Click or hover over the settings icon\">\n<p>First off, the alerts name has been changed to Notifications and you get to them by hovering  on the Gear icon and selecting Notifications.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/Notifications.png\" title=\"Click on Notifications\"><br>However, there is a difference in where you select this gear.  Make sure that you are in a TFS Project, if you see the drop down on the left say Projects, this would indicate that you are a level too high and in that case click on the Project and select one.  After this page loads you should see a big blue button called “+ New”, click this button.  <img src=\"/images/NewButton.png\" class=\"right\" width=\"100\" height=\"100\" title=\"New Button\">The page changes to allow you to select “Build” under the Category and “A build fails” under the template.  After you have done this click on the “Next” button.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NewNotification.png\" title=\"Make your selections and click the Next button\"></p>\n<p>This opens up a very different looking screen but the conditions to make this work are all there.  First off we select “Specific team members” for the Deliver to choice, and in the Roles choice select the “Requested by”.  This is the portion of the Notification that only selects the team member that queued up the build, in other words requested the build.</p>\n<p>Although we had to select a project before we could get to this Notification area, in the next section the Filter, we can select “Any team project” which would apply this notification to all the TFS Projects.  The filter criteria should be correct and not require any changes as this is basically gets fired off when the build has Failed.  You just click on the “Finish” button and the notification is ready for testing.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NotificationDetails.png\" title=\"Complete the details and click the Finish button\"></p>\n<h2 id=\"What-did-the-Notification-area-above-the-Project-do\"><a href=\"#What-did-the-Notification-area-above-the-Project-do\" class=\"headerlink\" title=\"What did the Notification area above the Project do?\"></a>What did the Notification area above the Project do?</h2><p>Well just before I let you go setting up your Notifications using the proper tool, I thought I would let you know what would have happened if we did not select a Project first before we went to the Notification screen.  If you do this you will notice that on the screen some of the criteria information that we used to narrow down the notification down to the developer that requested the build would not be there.  These Notifications are the subscription only ones that have been in TFS since the beginning of that product.  This does feel a bit strange to me, almost like these two concepts should be the opposite of what they currently are.  It is what it is but at long last we can now use the Notification engine to better suite our needs. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Over the many versions of TFS there were existing workarounds that allowed us to send an email to the developer that queued the build and it had failed.  Although these workarounds did work, I always felt that this should have been handled by the alert system within TFS.  What was lacking was some sort of condition that if the build failed it should go to the developer that queued it up.</p>\n<p>More recently I was tasked to find or build another workaround that would work within the vNext version of the Build engine.  Well I started down this quest collecting api’s that I could call when I thought I would have one more look at the TFS alerts, maybe there were some updates to that part of the tool.</p>\n<h2 id=\"New-Notification-Engine\"><a href=\"#New-Notification-Engine\" class=\"headerlink\" title=\"New Notification Engine\"></a>New Notification Engine</h2><p>What do you know, there were alot of changes made with this engine but not for TFS 2015, these updates show up in TFS 2017 Update 1.  One more reason to update to TFS 2017 for all those still using an on premise version of TFS as this has been in VSTS for a while now.  In the remainer of this post I will walk you through the steps to implement this big improvement in the notifications and how to solve that problem of just sending an email to the developer that caused the build to fail.</p>\n<p>If you are on VSTS or TFS 2017 Update 1, the steps are exactly the same which is nice as in my line of work I always hate having to remember two different ways for doing the same thing. </p>\n<h2 id=\"New-Name\"><a href=\"#New-Name\" class=\"headerlink\" title=\"New Name\"></a>New Name</h2><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/GearIcon.png\" title=\"Click or hover over the settings icon\">\n<p>First off, the alerts name has been changed to Notifications and you get to them by hovering  on the Gear icon and selecting Notifications.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/Notifications.png\" title=\"Click on Notifications\"><br>However, there is a difference in where you select this gear.  Make sure that you are in a TFS Project, if you see the drop down on the left say Projects, this would indicate that you are a level too high and in that case click on the Project and select one.  After this page loads you should see a big blue button called “+ New”, click this button.  <img src=\"/images/NewButton.png\" class=\"right\" width=\"100\" height=\"100\" title=\"New Button\">The page changes to allow you to select “Build” under the Category and “A build fails” under the template.  After you have done this click on the “Next” button.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NewNotification.png\" title=\"Make your selections and click the Next button\"></p>\n<p>This opens up a very different looking screen but the conditions to make this work are all there.  First off we select “Specific team members” for the Deliver to choice, and in the Roles choice select the “Requested by”.  This is the portion of the Notification that only selects the team member that queued up the build, in other words requested the build.</p>\n<p>Although we had to select a project before we could get to this Notification area, in the next section the Filter, we can select “Any team project” which would apply this notification to all the TFS Projects.  The filter criteria should be correct and not require any changes as this is basically gets fired off when the build has Failed.  You just click on the “Finish” button and the notification is ready for testing.<br><img src=\"/2017/05/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NotificationDetails.png\" title=\"Complete the details and click the Finish button\"></p>\n<h2 id=\"What-did-the-Notification-area-above-the-Project-do\"><a href=\"#What-did-the-Notification-area-above-the-Project-do\" class=\"headerlink\" title=\"What did the Notification area above the Project do?\"></a>What did the Notification area above the Project do?</h2><p>Well just before I let you go setting up your Notifications using the proper tool, I thought I would let you know what would have happened if we did not select a Project first before we went to the Notification screen.  If you do this you will notice that on the screen some of the criteria information that we used to narrow down the notification down to the developer that requested the build would not be there.  These Notifications are the subscription only ones that have been in TFS since the beginning of that product.  This does feel a bit strange to me, almost like these two concepts should be the opposite of what they currently are.  It is what it is but at long last we can now use the Notification engine to better suite our needs. </p>\n"},{"title":"Some MSDeploy Tricks I've Learned","date":"2016-08-04T01:11:59.000Z","_content":"{% img left /images/WebDeploy.jpg 300 300 \"Web Deploy\" %}\n[In an earlier post I talked about Hexo the tool I use for this Blog](../../../2016/01/A-New-Start-on-an-Old-Blog/index.html).  In that post I talked about how delighted I was with this process except for one thing that did bother me and that was the deployment to the Azure website.  For this process I was using FTP to push the files from the public folder to Azure.  Instead I was hoping for an MSDeploy solution but that is harder than it sounds especially when you are really not using a Visual Studio Project and MSBuild to create the application.\n\nIn this post I will take you on my journey to find a working solution that does enable me to deploy my blog as a MSDeploy package to the Azure website.\n\n## What is in the Public Folder\nFirst off I guess we should talk about what is in this folder that I call Public.  As I have mentioned in my [Hexo Post](../../../2016/01/A-New-Start-on-an-Old-Blog/index.html), the Hexo Generate command takes all my posts written in simple markup and creates the output that is my website and places it in a folder called public. \n\nIt is the output of this folder that I wish to create the MSDeploy package from.  This is quite straight forward as I already knew that you can use MSDeploy to not only deploy a package but also create one.  This will require knowing how to call MSDeploy from the command line.\n\n## Calling MSDeploy directly via Command Line\nThe basic syntax to create a package using MSDeploy is to call the program MSDeploy.exe then the parameter -verb and the verb choice is pretty much always sync.  Then you pass in the parameter -source and this one we are going to say where the source is and finally the -dest which we tell it where to place the package or where to deploy the package to if the source is a package.\n\n## Using Manifest files\nMSDeploy is very powerful with so many options and things you can do with it.  I have found it difficult to learn because as far as I have found, there is no good book or course that you can take that will really take you into any real depth to learn this tool.  I did come across a blog: [DotNet Catch](http://www.dotnetcatch.com/) that covers MSDeploy quite often.  It was there that I did learn about creating and deploying MSDeploy packages using Manifest files.\n\nIn this scenario I have a small xml file that says where the content is found and for that I write out a path to where the public folder is on my build machine.  I call this file: **manifest.source.xml**\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<sitemanifest>\n  <contentPath path=\"C:\\3WInc-Agent\\_work\\8\\s\\public\" />\n  <createApp path=\"\" />\n</sitemanifest>\n```\nWith the source manifest and an existing application that I want to package up sitting in the public folder at the disclosed location, I just have to call the following command to generate an MSDeploy package.  If you are calling this from the commandline on your machine then this should all be on one line.  \n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\" \n-verb:sync \n-source:manifest=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\manifest.source.xml\" \n-dest:package=C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\n```\nIf you are calling this from TFS you would use the commandline task and in the first line called Tool you would put the path to the msdeploy.exe program.  The other two lines would be one line and entered into the Arguments box.\n{% asset_img CreatePackageFromManifest.png \"Build Task to Create Package from Manifest file\"%}\n\nNow in order for that to work I need a similar xml file that is used for the destination file to tell MSDeploy that this package is a sync to the particular website. This file I called: **manifest.dest.xml** \n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<sitemanifest>\n  <contentPath path=\"Default Web Site\" />\n  <createApp path=\"Default Web Site\" />\n</sitemanifest>\n```\nThe syntax to call this blog.zip package and the destination manifest file is:\n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\"\n-verb:sync \n-source:package=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\" \n-dest:manifest=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\manifest.dest.xml\"\n```\nThis works great except that I cannot use the xml files when deploying to my Azure websites as I do not have that kind of control on them.  It is not a virtual machine that I can log onto or use a remote PowerShell script against to do my bidding and this package won't deploy onto that environment without it.  I need another approach to getting this to work the way I need it to.\n\n## Deploy to Build IIS to create a new MSDeploy package\nThis next idea that I came up with is a little strange and I had to get over the fact that I was configuring a web server on my Build Machine but that is exactly what I did do.  My build machine is a Windows Server 2012 R2 virtual machine so I turned on the Web Server Role from the Roles and Features Service.  Then using the above set of commands that I called from a Command Line task just like the test I used to create the package from the public folder I Deployed it to the Build Machine.\n\nAt this point I could even log into the build machine and confirm that I do indeed have a working web site with all my latest posts in it.  I then called MSDeploy once more and created a new Blog.zip package from the web site.\n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\"\n-verb:sync \n-source:iisApp=\"Default Web Site\" \n-dest:package=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\"\n```\nThe resulting blog.zip was easily deployed to my Azure website without any issue what so ever.  As you may have noticed that I have the blog.zip file with the exact same name and place as the old one.  There was no need to keep the first one as that was just used to get it deployed to the build machine so that we could creat the one that we really want.  In order to make sure that went smoothly I deleted the old one before I called this last command which is also a command line task in the build definition.\n## Success on Azure Website\nIn my release definition for the Azure web site deployment I just needed to use the built-in out of the box task called \"Azure Web App Deployment\" point it to where it could find the blog.zip file and tell it the name of my Azure web site and it took care of the rest.\n\n{% asset_img DeployWebsiteAzure.png \"Deploy the zip package to Azure\"%}\n","source":"_posts/Some-MSDeploy-Tricks-I-ve-Learned.md","raw":"title: \"Some MSDeploy Tricks I've Learned\"\ndate: 2016-08-03 18:11:59\ntags:\n- ALM\n- DevOps\n---\n{% img left /images/WebDeploy.jpg 300 300 \"Web Deploy\" %}\n[In an earlier post I talked about Hexo the tool I use for this Blog](../../../2016/01/A-New-Start-on-an-Old-Blog/index.html).  In that post I talked about how delighted I was with this process except for one thing that did bother me and that was the deployment to the Azure website.  For this process I was using FTP to push the files from the public folder to Azure.  Instead I was hoping for an MSDeploy solution but that is harder than it sounds especially when you are really not using a Visual Studio Project and MSBuild to create the application.\n\nIn this post I will take you on my journey to find a working solution that does enable me to deploy my blog as a MSDeploy package to the Azure website.\n\n## What is in the Public Folder\nFirst off I guess we should talk about what is in this folder that I call Public.  As I have mentioned in my [Hexo Post](../../../2016/01/A-New-Start-on-an-Old-Blog/index.html), the Hexo Generate command takes all my posts written in simple markup and creates the output that is my website and places it in a folder called public. \n\nIt is the output of this folder that I wish to create the MSDeploy package from.  This is quite straight forward as I already knew that you can use MSDeploy to not only deploy a package but also create one.  This will require knowing how to call MSDeploy from the command line.\n\n## Calling MSDeploy directly via Command Line\nThe basic syntax to create a package using MSDeploy is to call the program MSDeploy.exe then the parameter -verb and the verb choice is pretty much always sync.  Then you pass in the parameter -source and this one we are going to say where the source is and finally the -dest which we tell it where to place the package or where to deploy the package to if the source is a package.\n\n## Using Manifest files\nMSDeploy is very powerful with so many options and things you can do with it.  I have found it difficult to learn because as far as I have found, there is no good book or course that you can take that will really take you into any real depth to learn this tool.  I did come across a blog: [DotNet Catch](http://www.dotnetcatch.com/) that covers MSDeploy quite often.  It was there that I did learn about creating and deploying MSDeploy packages using Manifest files.\n\nIn this scenario I have a small xml file that says where the content is found and for that I write out a path to where the public folder is on my build machine.  I call this file: **manifest.source.xml**\n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<sitemanifest>\n  <contentPath path=\"C:\\3WInc-Agent\\_work\\8\\s\\public\" />\n  <createApp path=\"\" />\n</sitemanifest>\n```\nWith the source manifest and an existing application that I want to package up sitting in the public folder at the disclosed location, I just have to call the following command to generate an MSDeploy package.  If you are calling this from the commandline on your machine then this should all be on one line.  \n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\" \n-verb:sync \n-source:manifest=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\manifest.source.xml\" \n-dest:package=C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\n```\nIf you are calling this from TFS you would use the commandline task and in the first line called Tool you would put the path to the msdeploy.exe program.  The other two lines would be one line and entered into the Arguments box.\n{% asset_img CreatePackageFromManifest.png \"Build Task to Create Package from Manifest file\"%}\n\nNow in order for that to work I need a similar xml file that is used for the destination file to tell MSDeploy that this package is a sync to the particular website. This file I called: **manifest.dest.xml** \n```\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<sitemanifest>\n  <contentPath path=\"Default Web Site\" />\n  <createApp path=\"Default Web Site\" />\n</sitemanifest>\n```\nThe syntax to call this blog.zip package and the destination manifest file is:\n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\"\n-verb:sync \n-source:package=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\" \n-dest:manifest=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\manifest.dest.xml\"\n```\nThis works great except that I cannot use the xml files when deploying to my Azure websites as I do not have that kind of control on them.  It is not a virtual machine that I can log onto or use a remote PowerShell script against to do my bidding and this package won't deploy onto that environment without it.  I need another approach to getting this to work the way I need it to.\n\n## Deploy to Build IIS to create a new MSDeploy package\nThis next idea that I came up with is a little strange and I had to get over the fact that I was configuring a web server on my Build Machine but that is exactly what I did do.  My build machine is a Windows Server 2012 R2 virtual machine so I turned on the Web Server Role from the Roles and Features Service.  Then using the above set of commands that I called from a Command Line task just like the test I used to create the package from the public folder I Deployed it to the Build Machine.\n\nAt this point I could even log into the build machine and confirm that I do indeed have a working web site with all my latest posts in it.  I then called MSDeploy once more and created a new Blog.zip package from the web site.\n```\n\"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\msdeploy.exe\"\n-verb:sync \n-source:iisApp=\"Default Web Site\" \n-dest:package=\"C:\\3WInc-Agent\\_work\\8\\s\\msdeploy\\blog.zip\"\n```\nThe resulting blog.zip was easily deployed to my Azure website without any issue what so ever.  As you may have noticed that I have the blog.zip file with the exact same name and place as the old one.  There was no need to keep the first one as that was just used to get it deployed to the build machine so that we could creat the one that we really want.  In order to make sure that went smoothly I deleted the old one before I called this last command which is also a command line task in the build definition.\n## Success on Azure Website\nIn my release definition for the Azure web site deployment I just needed to use the built-in out of the box task called \"Azure Web App Deployment\" point it to where it could find the blog.zip file and tell it the name of my Azure web site and it took care of the rest.\n\n{% asset_img DeployWebsiteAzure.png \"Deploy the zip package to Azure\"%}\n","slug":"Some-MSDeploy-Tricks-I-ve-Learned","published":1,"updated":"2020-01-05T00:36:05.066Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg8000ls4uf2kyp1tbo","content":"<img src=\"/images/WebDeploy.jpg\" class=\"left\" width=\"300\" height=\"300\" title=\"Web Deploy\">\n<p><a href=\"../../../2016/01/A-New-Start-on-an-Old-Blog/index.html\">In an earlier post I talked about Hexo the tool I use for this Blog</a>.  In that post I talked about how delighted I was with this process except for one thing that did bother me and that was the deployment to the Azure website.  For this process I was using FTP to push the files from the public folder to Azure.  Instead I was hoping for an MSDeploy solution but that is harder than it sounds especially when you are really not using a Visual Studio Project and MSBuild to create the application.</p>\n<p>In this post I will take you on my journey to find a working solution that does enable me to deploy my blog as a MSDeploy package to the Azure website.</p>\n<h2 id=\"What-is-in-the-Public-Folder\"><a href=\"#What-is-in-the-Public-Folder\" class=\"headerlink\" title=\"What is in the Public Folder\"></a>What is in the Public Folder</h2><p>First off I guess we should talk about what is in this folder that I call Public.  As I have mentioned in my <a href=\"../../../2016/01/A-New-Start-on-an-Old-Blog/index.html\">Hexo Post</a>, the Hexo Generate command takes all my posts written in simple markup and creates the output that is my website and places it in a folder called public. </p>\n<p>It is the output of this folder that I wish to create the MSDeploy package from.  This is quite straight forward as I already knew that you can use MSDeploy to not only deploy a package but also create one.  This will require knowing how to call MSDeploy from the command line.</p>\n<h2 id=\"Calling-MSDeploy-directly-via-Command-Line\"><a href=\"#Calling-MSDeploy-directly-via-Command-Line\" class=\"headerlink\" title=\"Calling MSDeploy directly via Command Line\"></a>Calling MSDeploy directly via Command Line</h2><p>The basic syntax to create a package using MSDeploy is to call the program MSDeploy.exe then the parameter -verb and the verb choice is pretty much always sync.  Then you pass in the parameter -source and this one we are going to say where the source is and finally the -dest which we tell it where to place the package or where to deploy the package to if the source is a package.</p>\n<h2 id=\"Using-Manifest-files\"><a href=\"#Using-Manifest-files\" class=\"headerlink\" title=\"Using Manifest files\"></a>Using Manifest files</h2><p>MSDeploy is very powerful with so many options and things you can do with it.  I have found it difficult to learn because as far as I have found, there is no good book or course that you can take that will really take you into any real depth to learn this tool.  I did come across a blog: <a href=\"http://www.dotnetcatch.com/\" target=\"_blank\" rel=\"noopener\">DotNet Catch</a> that covers MSDeploy quite often.  It was there that I did learn about creating and deploying MSDeploy packages using Manifest files.</p>\n<p>In this scenario I have a small xml file that says where the content is found and for that I write out a path to where the public folder is on my build machine.  I call this file: <strong>manifest.source.xml</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">sitemanifest</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">contentPath</span> <span class=\"attr\">path</span>=<span class=\"string\">\"C:\\3WInc-Agent\\_work\\8\\s\\public\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">createApp</span> <span class=\"attr\">path</span>=<span class=\"string\">\"\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">sitemanifest</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>With the source manifest and an existing application that I want to package up sitting in the public folder at the disclosed location, I just have to call the following command to generate an MSDeploy package.  If you are calling this from the commandline on your machine then this should all be on one line.<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\" </span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:manifest=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\m</span>anifest.source.xml\" </span><br><span class=\"line\">-dest:package=C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip</span><br></pre></td></tr></table></figure></p>\n<p>If you are calling this from TFS you would use the commandline task and in the first line called Tool you would put the path to the msdeploy.exe program.  The other two lines would be one line and entered into the Arguments box.<br><img src=\"/2016/08/Some-MSDeploy-Tricks-I-ve-Learned/CreatePackageFromManifest.png\" title=\"Build Task to Create Package from Manifest file\"></p>\n<p>Now in order for that to work I need a similar xml file that is used for the destination file to tell MSDeploy that this package is a sync to the particular website. This file I called: <strong>manifest.dest.xml</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">sitemanifest</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">contentPath</span> <span class=\"attr\">path</span>=<span class=\"string\">\"Default Web Site\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">createApp</span> <span class=\"attr\">path</span>=<span class=\"string\">\"Default Web Site\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">sitemanifest</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>The syntax to call this blog.zip package and the destination manifest file is:<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\"</span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:package=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip\" </span><br><span class=\"line\">-dest:manifest=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\m</span>anifest.dest.xml\"</span><br></pre></td></tr></table></figure></p>\n<p>This works great except that I cannot use the xml files when deploying to my Azure websites as I do not have that kind of control on them.  It is not a virtual machine that I can log onto or use a remote PowerShell script against to do my bidding and this package won’t deploy onto that environment without it.  I need another approach to getting this to work the way I need it to.</p>\n<h2 id=\"Deploy-to-Build-IIS-to-create-a-new-MSDeploy-package\"><a href=\"#Deploy-to-Build-IIS-to-create-a-new-MSDeploy-package\" class=\"headerlink\" title=\"Deploy to Build IIS to create a new MSDeploy package\"></a>Deploy to Build IIS to create a new MSDeploy package</h2><p>This next idea that I came up with is a little strange and I had to get over the fact that I was configuring a web server on my Build Machine but that is exactly what I did do.  My build machine is a Windows Server 2012 R2 virtual machine so I turned on the Web Server Role from the Roles and Features Service.  Then using the above set of commands that I called from a Command Line task just like the test I used to create the package from the public folder I Deployed it to the Build Machine.</p>\n<p>At this point I could even log into the build machine and confirm that I do indeed have a working web site with all my latest posts in it.  I then called MSDeploy once more and created a new Blog.zip package from the web site.<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\"</span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:iisApp=\"Default Web Site\" </span><br><span class=\"line\">-dest:package=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip\"</span><br></pre></td></tr></table></figure></p>\n<p>The resulting blog.zip was easily deployed to my Azure website without any issue what so ever.  As you may have noticed that I have the blog.zip file with the exact same name and place as the old one.  There was no need to keep the first one as that was just used to get it deployed to the build machine so that we could creat the one that we really want.  In order to make sure that went smoothly I deleted the old one before I called this last command which is also a command line task in the build definition.</p>\n<h2 id=\"Success-on-Azure-Website\"><a href=\"#Success-on-Azure-Website\" class=\"headerlink\" title=\"Success on Azure Website\"></a>Success on Azure Website</h2><p>In my release definition for the Azure web site deployment I just needed to use the built-in out of the box task called “Azure Web App Deployment” point it to where it could find the blog.zip file and tell it the name of my Azure web site and it took care of the rest.</p>\n<img src=\"/2016/08/Some-MSDeploy-Tricks-I-ve-Learned/DeployWebsiteAzure.png\" title=\"Deploy the zip package to Azure\">\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/WebDeploy.jpg\" class=\"left\" width=\"300\" height=\"300\" title=\"Web Deploy\">\n<p><a href=\"../../../2016/01/A-New-Start-on-an-Old-Blog/index.html\">In an earlier post I talked about Hexo the tool I use for this Blog</a>.  In that post I talked about how delighted I was with this process except for one thing that did bother me and that was the deployment to the Azure website.  For this process I was using FTP to push the files from the public folder to Azure.  Instead I was hoping for an MSDeploy solution but that is harder than it sounds especially when you are really not using a Visual Studio Project and MSBuild to create the application.</p>\n<p>In this post I will take you on my journey to find a working solution that does enable me to deploy my blog as a MSDeploy package to the Azure website.</p>\n<h2 id=\"What-is-in-the-Public-Folder\"><a href=\"#What-is-in-the-Public-Folder\" class=\"headerlink\" title=\"What is in the Public Folder\"></a>What is in the Public Folder</h2><p>First off I guess we should talk about what is in this folder that I call Public.  As I have mentioned in my <a href=\"../../../2016/01/A-New-Start-on-an-Old-Blog/index.html\">Hexo Post</a>, the Hexo Generate command takes all my posts written in simple markup and creates the output that is my website and places it in a folder called public. </p>\n<p>It is the output of this folder that I wish to create the MSDeploy package from.  This is quite straight forward as I already knew that you can use MSDeploy to not only deploy a package but also create one.  This will require knowing how to call MSDeploy from the command line.</p>\n<h2 id=\"Calling-MSDeploy-directly-via-Command-Line\"><a href=\"#Calling-MSDeploy-directly-via-Command-Line\" class=\"headerlink\" title=\"Calling MSDeploy directly via Command Line\"></a>Calling MSDeploy directly via Command Line</h2><p>The basic syntax to create a package using MSDeploy is to call the program MSDeploy.exe then the parameter -verb and the verb choice is pretty much always sync.  Then you pass in the parameter -source and this one we are going to say where the source is and finally the -dest which we tell it where to place the package or where to deploy the package to if the source is a package.</p>\n<h2 id=\"Using-Manifest-files\"><a href=\"#Using-Manifest-files\" class=\"headerlink\" title=\"Using Manifest files\"></a>Using Manifest files</h2><p>MSDeploy is very powerful with so many options and things you can do with it.  I have found it difficult to learn because as far as I have found, there is no good book or course that you can take that will really take you into any real depth to learn this tool.  I did come across a blog: <a href=\"http://www.dotnetcatch.com/\" target=\"_blank\" rel=\"noopener\">DotNet Catch</a> that covers MSDeploy quite often.  It was there that I did learn about creating and deploying MSDeploy packages using Manifest files.</p>\n<p>In this scenario I have a small xml file that says where the content is found and for that I write out a path to where the public folder is on my build machine.  I call this file: <strong>manifest.source.xml</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">sitemanifest</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">contentPath</span> <span class=\"attr\">path</span>=<span class=\"string\">\"C:\\3WInc-Agent\\_work\\8\\s\\public\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">createApp</span> <span class=\"attr\">path</span>=<span class=\"string\">\"\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">sitemanifest</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>With the source manifest and an existing application that I want to package up sitting in the public folder at the disclosed location, I just have to call the following command to generate an MSDeploy package.  If you are calling this from the commandline on your machine then this should all be on one line.<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\" </span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:manifest=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\m</span>anifest.source.xml\" </span><br><span class=\"line\">-dest:package=C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip</span><br></pre></td></tr></table></figure></p>\n<p>If you are calling this from TFS you would use the commandline task and in the first line called Tool you would put the path to the msdeploy.exe program.  The other two lines would be one line and entered into the Arguments box.<br><img src=\"/2016/08/Some-MSDeploy-Tricks-I-ve-Learned/CreatePackageFromManifest.png\" title=\"Build Task to Create Package from Manifest file\"></p>\n<p>Now in order for that to work I need a similar xml file that is used for the destination file to tell MSDeploy that this package is a sync to the particular website. This file I called: <strong>manifest.dest.xml</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"php\"><span class=\"meta\">&lt;?</span>xml version=<span class=\"string\">\"1.0\"</span> encoding=<span class=\"string\">\"utf-8\"</span><span class=\"meta\">?&gt;</span></span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">sitemanifest</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">contentPath</span> <span class=\"attr\">path</span>=<span class=\"string\">\"Default Web Site\"</span> /&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">createApp</span> <span class=\"attr\">path</span>=<span class=\"string\">\"Default Web Site\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">sitemanifest</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<p>The syntax to call this blog.zip package and the destination manifest file is:<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\"</span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:package=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip\" </span><br><span class=\"line\">-dest:manifest=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\m</span>anifest.dest.xml\"</span><br></pre></td></tr></table></figure></p>\n<p>This works great except that I cannot use the xml files when deploying to my Azure websites as I do not have that kind of control on them.  It is not a virtual machine that I can log onto or use a remote PowerShell script against to do my bidding and this package won’t deploy onto that environment without it.  I need another approach to getting this to work the way I need it to.</p>\n<h2 id=\"Deploy-to-Build-IIS-to-create-a-new-MSDeploy-package\"><a href=\"#Deploy-to-Build-IIS-to-create-a-new-MSDeploy-package\" class=\"headerlink\" title=\"Deploy to Build IIS to create a new MSDeploy package\"></a>Deploy to Build IIS to create a new MSDeploy package</h2><p>This next idea that I came up with is a little strange and I had to get over the fact that I was configuring a web server on my Build Machine but that is exactly what I did do.  My build machine is a Windows Server 2012 R2 virtual machine so I turned on the Web Server Role from the Roles and Features Service.  Then using the above set of commands that I called from a Command Line task just like the test I used to create the package from the public folder I Deployed it to the Build Machine.</p>\n<p>At this point I could even log into the build machine and confirm that I do indeed have a working web site with all my latest posts in it.  I then called MSDeploy once more and created a new Blog.zip package from the web site.<br><figure class=\"highlight taggerscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\"C:<span class=\"symbol\">\\P</span>rogram Files<span class=\"symbol\">\\I</span>IS<span class=\"symbol\">\\M</span>icrosoft Web Deploy V3<span class=\"symbol\">\\m</span>sdeploy.exe\"</span><br><span class=\"line\">-verb:sync </span><br><span class=\"line\">-source:iisApp=\"Default Web Site\" </span><br><span class=\"line\">-dest:package=\"C:<span class=\"symbol\">\\3</span>WInc-Agent<span class=\"symbol\">\\_</span>work<span class=\"symbol\">\\8</span><span class=\"symbol\">\\s</span><span class=\"symbol\">\\m</span>sdeploy<span class=\"symbol\">\\b</span>log.zip\"</span><br></pre></td></tr></table></figure></p>\n<p>The resulting blog.zip was easily deployed to my Azure website without any issue what so ever.  As you may have noticed that I have the blog.zip file with the exact same name and place as the old one.  There was no need to keep the first one as that was just used to get it deployed to the build machine so that we could creat the one that we really want.  In order to make sure that went smoothly I deleted the old one before I called this last command which is also a command line task in the build definition.</p>\n<h2 id=\"Success-on-Azure-Website\"><a href=\"#Success-on-Azure-Website\" class=\"headerlink\" title=\"Success on Azure Website\"></a>Success on Azure Website</h2><p>In my release definition for the Azure web site deployment I just needed to use the built-in out of the box task called “Azure Web App Deployment” point it to where it could find the blog.zip file and tell it the name of my Azure web site and it took care of the rest.</p>\n<img src=\"/2016/08/Some-MSDeploy-Tricks-I-ve-Learned/DeployWebsiteAzure.png\" title=\"Deploy the zip package to Azure\">\n"},{"title":"Teams, SubTeams and Area Path's","date":"2019-03-22T17:31:47.000Z","_content":"Back in around the 2012 release of TFS (now known as Azure DevOps) we were introduced to the concept of Teams.  This was a logical breakdown of a single project that typically could represent an organization or at least a division.  I come from the camp of one collection and one project as this gives you the most value and best experience and even though the product has evolved to be even more flexible then it has I think this is still the best approach to take today.  If you are unfamiliar with the one Project to rule them all here are some excellent blog posts to get you up to speed on the concept:\n[Why You Should Use a Single TFS Team Project](https://www.imaginet.com/2013/why-you-should-use-single-tfs-team-project/)\n[One Team Project to rule them all](https://nkdagility.com/one-team-project/)\n\n### SubTeam's\nAlmost as soon as we were given teams many of my customers started asking about sub teams if we could break a team up into smaller components but still have them rolling up to a parent or an overall team.  Well right from the beginning of Teams we were sort of able to do this because the team is really defined by the Area Path.  The Area Path is one of two controls that we have in TFS that is a tree control and capable of nested items.  \nSince the 2012 of TFS, when you create a Team Project it creates a Team of the same name as the Project.  This means that any team that you create is really a nested child of this parent team.\nThe default behavior of creating a team is to create an area path with the same name as the team just under the root name of the Project which is parent team.  If we want to create any sub teams we need to do things a little different from the default way that we create them.  Lets walk through those steps.\n1. First be sure you are in the Project and not at the Organization level and at the bottom of the menu you will find a gear icon called Project settings.  Click on the icon.\n{% asset_img ProjectSettings.jpg \"Create a Project\" %}\n1. Under the General section of this menu you should see a menu item called Teams.  Click on that, the line becomes a click item when you hover over it.\n{% asset_img AddTeamButton.jpg \"Add Team Button\" %}\n1. Right under the Header of this page you should see what could be a button called \"New team\".  Click on the button.\n1. We are going to call this team \"Web\" as this will be our parent for the web development that we shall perform.  Fill in the Team name and give it a description.  **Note:** __*for the creation of the parent team we leave the checkbox checked that says \"Create an area path with the name of the team.\"*__  In later steps when creating the sub-teams it is important that we uncheck that box.  More on that a little later.  Click on the Create team button.\n{% asset_img CreateNewParentTeam.jpg \"Create the parent team\" %}\n1. Still on the Project settings page, under the Boards section of this menu you should see a menu item called Project configuration.  Click on that, the line becomes a click item when you hover over it.\n{% asset_img ProjectConfigurationButton.jpg \"Project configuration button\" %}\n1. This by default takes you to the Iterations management area.  Up close to the top of this page beside the Iterations you should see Areas.  Click on that, you mouse should turn into a hand when you hover over the word Areas.\n{% asset_img AreasLink.jpg \"Area link\" %}\n1. Here you will see that the Web area is listed which happened when we created the Web Team (remember the checkbox to create an area path).  Now select the Web in the area and click on the New Child button.\n{% asset_img NewChildButton.jpg \"New child button\" %}\n1. I am going to call this new Area name with Team-1 and click on the Save and close button.\n{% asset_img Team-1AreaPath.jpg \"Team-1 area path\" %}\n1. With the Team-1 still selected we are going to create another child at the same level as this one and is going to be a sibling to this so instead of the New child button we click on the New button as this will create a new Area at the same level as the currently selected area.\n{% asset_img NewSecondChildProcess.jpg \"New second child added\" %}\n1. I am going to call this new Area name with Team-2 and click the Save and close button.\n{% asset_img Team-2AreaPath.jpg \"Team-2 area path\" %}\n1. Still on the Project settings page, under the Boards section of this menu you should see a menu item called Team configuration.  Click on that, again the line becomes a click item when you hover over it.\n{% asset_img TeamConfiguration.jpg \"Team configuration button\" %}\n1. You should still be on the Web Team, click on the Areas link and there you should see the area path of the Web.  You won't see the child area paths in this view but we do want their values to roll up to this master web team page.  Click on the elipse button right beside the text default area.\n{% asset_img SelectWebAreaPath.jpg \"Select Web area path\" %}\n1. In the menu that pop's up select the item that says \"Include sub areas\"\n{% asset_img IncludeSubAreas.jpg \"Include sub areas\" %}\n1. After you clicked that you should see the \"sub-areas are included\" message on the right of the default area.  This is what you want to look for when you are trying to determine why some master teams are not getting their work items rolled up to the top.\n{% asset_img subareaincludedmessage.jpg \"sub areas are included message\" %}\n1. Now we are ready to go back to the teams page and create those other two teams that we created an area path for.  Back on the Project Settings under the General area click on the Teams button.\n{% asset_img BackToTeams.jpg \"Back to the teams\" %}\n1. Click on the New team button\n{% asset_img NewTeamButtonForTeam-1.jpg \"New Team button\" %}\n1. This time in the Team name call this \"Web-Team-1\" because these teams will be listed as a list and the only way to know first hand which are parents and which are children is by naming convention and you will want to keep the parent together with their children and the ony way to do that is to make sure that the parent is the beginning part of the name of the team.\n{% asset_img NewWebTeam-1.jpg \"Title this Web-Team-1\" %}\n1. This next part is very important to observe, make sure that you **un-check** the *Create an area path with the name of the team check box.*  We already have an area path and we will assign that area path to the team in a couple of steps from now.  Enter a discription and then click on the Create team button.\n{% asset_img CreateNewTeam-1.jpg \"Create new Team-1\" %}\n1. Click on the New team button once more.\n{% asset_img NewTeamButtonForTeam-2.jpg \"New Team button for Team-2\" %}\n1. Name this team \"Web-Team-2\", put in a description and remember to **un-check** the *Create an area path with the name of the team* check box and then click on the Create team button.\n{% asset_img CreateNewTeam-2.jpg \"Create New Team-2\" %}\n1. Now that we have the teams created we need to attach them to the correct area path.  First click onto the Web-Team-1 link so that we are on that team.\n{% asset_img ClickTeam-1.jpg \"Click on Team-1\" %}\n1. Click on the Team configuration link.\n{% asset_img TeamConfigurationClick.jpg \"Click on Team Configuration\" %}\n1. You will see a warning message and this warning is related to exactly what we are going to fix right now.  Currently those two sub-teams are in an invalid state.\n{% asset_img ErrorMessage.jpg \"Warning message about invalid state\" %}\n1. Click on the Areas sub tab at the top of this section.\n{% asset_img ClickAreas.jpg \"Click the areas link\" %}\n1. Lower on this page you should see a big green cross called Select area(s), click on this.\n{% asset_img ClickSelectAreasButton.jpg \"Click on the Select Area(s) button\" %}\n1. This opens a new box and you want to expand the carets until you see the Team-1 area.  Click on Team-1.\n{% asset_img ClickTeam-1Area.jpg \"Click on the Team-1 Area choice.\" %}\n1. Click on the Save and close Button.\n{% asset_img ClickSaveAndClose.jpg \"Click on the Save and close button\" %}\n1. Repeat the same thing for Team-2 start from Task #21 and hook it up to the Team-2 area path.\n\nAt this point you will have two sub-teams who's backlog list will roll up to the master web team.  On the web team you will see all the work items from all the sub-teams below it.  With this configuration you get your own dashboard and kanban board as well.  Let's say that you like a similar benefit like separate backlog lists but don't want the overhead of additional dashboards and kanban boards and sub-team names that like flat on the full list of teams.  With the new filtering of the backlog lists this is now possible.  Lets look at that solution next.\n\n### One Team to Manage my Sub-Teams\nTo work with the existing things that we already have I am just going to delete the two sub-teams but leave all the area paths in place and configured as they were.\n1. Deleting a team is just as easy as it was to create them.  While we are still on the Project Settings page we want to click on the Teams button so that we see our list of Teams.\n{% asset_img ClickTeam-1.jpg \"Click on Teams to get to the list of teams\" %}\n1. Click on the ellipse button and on the menu that pops up click on Delete\"\n{% asset_img DeleteButton.jpg \"Click on the red x to delete\" %}\n1. A warning window pops up and on here you click the Delete team button.\n{% asset_img DeleteTeam.jpg \"Confirm, Delete Team\" %}\n1. Repeat the same operation to Delete Team-2\n1. So that we can see some data for there various sub teams we should add some data so that we can see how the filtering works on the one Web Team backlog list.  On the Boards menu click on Work Items.\n{% asset_img BoardsToWorkItems.jpg \"Click on Boards then Work Items\" %}\n1. Click on the New Work Item drop down and choose Bug.\n{% asset_img ChooseBug.jpg \"Choose the bug item\" %}\n1. Give this bug a title of \"This is a team-1 bug\" and then select the correct area path which should be <The Name of Your Project>\\Web\\Team-1 as you probably realized, even though we deleted the team, the area path still stays.  Then click the Save button.\n{% asset_img CreateTheBug.jpg \"Create the bug\" %}\n1. Repeat the process and make another similar bug for Team-2\n1. Now with two bugs from two different teams lets go the backlog list for the Web.  Click on the Backlog menu item under the Boards menu and then expand the All teams backlogs so you can click on the Web backlogs item.\n{% asset_img GetToWebBacklog.jpg \"Navigate to the Web Backlog list\" %}\n1. You will see both of these bugs listed here because back in our previous exercise where we were creating teams we set the default behavior of the areas to roll up to the parent which in this case is Web.\n{% asset_img TwoBugs.jpg \"See the two bugs listed\" %}\n1. Now, lets say we just want to see the bugs from Team-1 which is what you would see if you still had a Team-1 team.  Over in the upper right hand corner there is an icon that looks like a funnel.  This enables the Filter tool bar.\n{% asset_img FilterToolbar.jpg \"Click the Filter toolbar button\" %}\n1. Click on the Area dropdown and you see that I see my Team-1 and Team-2 (these are my sub-area paths).\n{% asset_img AreaDropDown.jpg \"Dropdown Area Filter\" %}\n1. Click on the Team-2 checkbox, instantly the filter kicks in and you only see Team-2 work items.\n{% asset_img OnlyTeam-2.jpg \"Only team-2 items are shown\" %}\n1. This will also work on the Kanban boards you have the same kind of filtering setup here as well.  The thing that I like about this method for handling the sub-teams is that if I had several teams under my parent team I could combine a couple of teams together in any combination that I need which is something I would not be able to do on the fly with the sub-team configuration.\n\nThese are the two ways that I can see working with Teams and Sub-Teams right now.  If you don't need the luxury of a separate burn down chart and capacity planning at the sub-team level I think you get everything you need right here with the single major area and manage the smaller breakdown of work using the nested area path.  Just remember to turn on the rolling up of sub areas up as the default is not to.\n\n","source":"_posts/Teams-SubTeams-and-Area-Path-s.md","raw":"---\ntitle: 'Teams, SubTeams and Area Path''s'\ndate: 2019-03-22 10:31:47\ntags: \n- DevOps\n- TFS\n---\nBack in around the 2012 release of TFS (now known as Azure DevOps) we were introduced to the concept of Teams.  This was a logical breakdown of a single project that typically could represent an organization or at least a division.  I come from the camp of one collection and one project as this gives you the most value and best experience and even though the product has evolved to be even more flexible then it has I think this is still the best approach to take today.  If you are unfamiliar with the one Project to rule them all here are some excellent blog posts to get you up to speed on the concept:\n[Why You Should Use a Single TFS Team Project](https://www.imaginet.com/2013/why-you-should-use-single-tfs-team-project/)\n[One Team Project to rule them all](https://nkdagility.com/one-team-project/)\n\n### SubTeam's\nAlmost as soon as we were given teams many of my customers started asking about sub teams if we could break a team up into smaller components but still have them rolling up to a parent or an overall team.  Well right from the beginning of Teams we were sort of able to do this because the team is really defined by the Area Path.  The Area Path is one of two controls that we have in TFS that is a tree control and capable of nested items.  \nSince the 2012 of TFS, when you create a Team Project it creates a Team of the same name as the Project.  This means that any team that you create is really a nested child of this parent team.\nThe default behavior of creating a team is to create an area path with the same name as the team just under the root name of the Project which is parent team.  If we want to create any sub teams we need to do things a little different from the default way that we create them.  Lets walk through those steps.\n1. First be sure you are in the Project and not at the Organization level and at the bottom of the menu you will find a gear icon called Project settings.  Click on the icon.\n{% asset_img ProjectSettings.jpg \"Create a Project\" %}\n1. Under the General section of this menu you should see a menu item called Teams.  Click on that, the line becomes a click item when you hover over it.\n{% asset_img AddTeamButton.jpg \"Add Team Button\" %}\n1. Right under the Header of this page you should see what could be a button called \"New team\".  Click on the button.\n1. We are going to call this team \"Web\" as this will be our parent for the web development that we shall perform.  Fill in the Team name and give it a description.  **Note:** __*for the creation of the parent team we leave the checkbox checked that says \"Create an area path with the name of the team.\"*__  In later steps when creating the sub-teams it is important that we uncheck that box.  More on that a little later.  Click on the Create team button.\n{% asset_img CreateNewParentTeam.jpg \"Create the parent team\" %}\n1. Still on the Project settings page, under the Boards section of this menu you should see a menu item called Project configuration.  Click on that, the line becomes a click item when you hover over it.\n{% asset_img ProjectConfigurationButton.jpg \"Project configuration button\" %}\n1. This by default takes you to the Iterations management area.  Up close to the top of this page beside the Iterations you should see Areas.  Click on that, you mouse should turn into a hand when you hover over the word Areas.\n{% asset_img AreasLink.jpg \"Area link\" %}\n1. Here you will see that the Web area is listed which happened when we created the Web Team (remember the checkbox to create an area path).  Now select the Web in the area and click on the New Child button.\n{% asset_img NewChildButton.jpg \"New child button\" %}\n1. I am going to call this new Area name with Team-1 and click on the Save and close button.\n{% asset_img Team-1AreaPath.jpg \"Team-1 area path\" %}\n1. With the Team-1 still selected we are going to create another child at the same level as this one and is going to be a sibling to this so instead of the New child button we click on the New button as this will create a new Area at the same level as the currently selected area.\n{% asset_img NewSecondChildProcess.jpg \"New second child added\" %}\n1. I am going to call this new Area name with Team-2 and click the Save and close button.\n{% asset_img Team-2AreaPath.jpg \"Team-2 area path\" %}\n1. Still on the Project settings page, under the Boards section of this menu you should see a menu item called Team configuration.  Click on that, again the line becomes a click item when you hover over it.\n{% asset_img TeamConfiguration.jpg \"Team configuration button\" %}\n1. You should still be on the Web Team, click on the Areas link and there you should see the area path of the Web.  You won't see the child area paths in this view but we do want their values to roll up to this master web team page.  Click on the elipse button right beside the text default area.\n{% asset_img SelectWebAreaPath.jpg \"Select Web area path\" %}\n1. In the menu that pop's up select the item that says \"Include sub areas\"\n{% asset_img IncludeSubAreas.jpg \"Include sub areas\" %}\n1. After you clicked that you should see the \"sub-areas are included\" message on the right of the default area.  This is what you want to look for when you are trying to determine why some master teams are not getting their work items rolled up to the top.\n{% asset_img subareaincludedmessage.jpg \"sub areas are included message\" %}\n1. Now we are ready to go back to the teams page and create those other two teams that we created an area path for.  Back on the Project Settings under the General area click on the Teams button.\n{% asset_img BackToTeams.jpg \"Back to the teams\" %}\n1. Click on the New team button\n{% asset_img NewTeamButtonForTeam-1.jpg \"New Team button\" %}\n1. This time in the Team name call this \"Web-Team-1\" because these teams will be listed as a list and the only way to know first hand which are parents and which are children is by naming convention and you will want to keep the parent together with their children and the ony way to do that is to make sure that the parent is the beginning part of the name of the team.\n{% asset_img NewWebTeam-1.jpg \"Title this Web-Team-1\" %}\n1. This next part is very important to observe, make sure that you **un-check** the *Create an area path with the name of the team check box.*  We already have an area path and we will assign that area path to the team in a couple of steps from now.  Enter a discription and then click on the Create team button.\n{% asset_img CreateNewTeam-1.jpg \"Create new Team-1\" %}\n1. Click on the New team button once more.\n{% asset_img NewTeamButtonForTeam-2.jpg \"New Team button for Team-2\" %}\n1. Name this team \"Web-Team-2\", put in a description and remember to **un-check** the *Create an area path with the name of the team* check box and then click on the Create team button.\n{% asset_img CreateNewTeam-2.jpg \"Create New Team-2\" %}\n1. Now that we have the teams created we need to attach them to the correct area path.  First click onto the Web-Team-1 link so that we are on that team.\n{% asset_img ClickTeam-1.jpg \"Click on Team-1\" %}\n1. Click on the Team configuration link.\n{% asset_img TeamConfigurationClick.jpg \"Click on Team Configuration\" %}\n1. You will see a warning message and this warning is related to exactly what we are going to fix right now.  Currently those two sub-teams are in an invalid state.\n{% asset_img ErrorMessage.jpg \"Warning message about invalid state\" %}\n1. Click on the Areas sub tab at the top of this section.\n{% asset_img ClickAreas.jpg \"Click the areas link\" %}\n1. Lower on this page you should see a big green cross called Select area(s), click on this.\n{% asset_img ClickSelectAreasButton.jpg \"Click on the Select Area(s) button\" %}\n1. This opens a new box and you want to expand the carets until you see the Team-1 area.  Click on Team-1.\n{% asset_img ClickTeam-1Area.jpg \"Click on the Team-1 Area choice.\" %}\n1. Click on the Save and close Button.\n{% asset_img ClickSaveAndClose.jpg \"Click on the Save and close button\" %}\n1. Repeat the same thing for Team-2 start from Task #21 and hook it up to the Team-2 area path.\n\nAt this point you will have two sub-teams who's backlog list will roll up to the master web team.  On the web team you will see all the work items from all the sub-teams below it.  With this configuration you get your own dashboard and kanban board as well.  Let's say that you like a similar benefit like separate backlog lists but don't want the overhead of additional dashboards and kanban boards and sub-team names that like flat on the full list of teams.  With the new filtering of the backlog lists this is now possible.  Lets look at that solution next.\n\n### One Team to Manage my Sub-Teams\nTo work with the existing things that we already have I am just going to delete the two sub-teams but leave all the area paths in place and configured as they were.\n1. Deleting a team is just as easy as it was to create them.  While we are still on the Project Settings page we want to click on the Teams button so that we see our list of Teams.\n{% asset_img ClickTeam-1.jpg \"Click on Teams to get to the list of teams\" %}\n1. Click on the ellipse button and on the menu that pops up click on Delete\"\n{% asset_img DeleteButton.jpg \"Click on the red x to delete\" %}\n1. A warning window pops up and on here you click the Delete team button.\n{% asset_img DeleteTeam.jpg \"Confirm, Delete Team\" %}\n1. Repeat the same operation to Delete Team-2\n1. So that we can see some data for there various sub teams we should add some data so that we can see how the filtering works on the one Web Team backlog list.  On the Boards menu click on Work Items.\n{% asset_img BoardsToWorkItems.jpg \"Click on Boards then Work Items\" %}\n1. Click on the New Work Item drop down and choose Bug.\n{% asset_img ChooseBug.jpg \"Choose the bug item\" %}\n1. Give this bug a title of \"This is a team-1 bug\" and then select the correct area path which should be <The Name of Your Project>\\Web\\Team-1 as you probably realized, even though we deleted the team, the area path still stays.  Then click the Save button.\n{% asset_img CreateTheBug.jpg \"Create the bug\" %}\n1. Repeat the process and make another similar bug for Team-2\n1. Now with two bugs from two different teams lets go the backlog list for the Web.  Click on the Backlog menu item under the Boards menu and then expand the All teams backlogs so you can click on the Web backlogs item.\n{% asset_img GetToWebBacklog.jpg \"Navigate to the Web Backlog list\" %}\n1. You will see both of these bugs listed here because back in our previous exercise where we were creating teams we set the default behavior of the areas to roll up to the parent which in this case is Web.\n{% asset_img TwoBugs.jpg \"See the two bugs listed\" %}\n1. Now, lets say we just want to see the bugs from Team-1 which is what you would see if you still had a Team-1 team.  Over in the upper right hand corner there is an icon that looks like a funnel.  This enables the Filter tool bar.\n{% asset_img FilterToolbar.jpg \"Click the Filter toolbar button\" %}\n1. Click on the Area dropdown and you see that I see my Team-1 and Team-2 (these are my sub-area paths).\n{% asset_img AreaDropDown.jpg \"Dropdown Area Filter\" %}\n1. Click on the Team-2 checkbox, instantly the filter kicks in and you only see Team-2 work items.\n{% asset_img OnlyTeam-2.jpg \"Only team-2 items are shown\" %}\n1. This will also work on the Kanban boards you have the same kind of filtering setup here as well.  The thing that I like about this method for handling the sub-teams is that if I had several teams under my parent team I could combine a couple of teams together in any combination that I need which is something I would not be able to do on the fly with the sub-team configuration.\n\nThese are the two ways that I can see working with Teams and Sub-Teams right now.  If you don't need the luxury of a separate burn down chart and capacity planning at the sub-team level I think you get everything you need right here with the single major area and manage the smaller breakdown of work using the nested area path.  Just remember to turn on the rolling up of sub areas up as the default is not to.\n\n","slug":"Teams-SubTeams-and-Area-Path-s","published":1,"updated":"2020-01-05T00:36:05.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg8000ms4uf1u947z6f","content":"<p>Back in around the 2012 release of TFS (now known as Azure DevOps) we were introduced to the concept of Teams.  This was a logical breakdown of a single project that typically could represent an organization or at least a division.  I come from the camp of one collection and one project as this gives you the most value and best experience and even though the product has evolved to be even more flexible then it has I think this is still the best approach to take today.  If you are unfamiliar with the one Project to rule them all here are some excellent blog posts to get you up to speed on the concept:<br><a href=\"https://www.imaginet.com/2013/why-you-should-use-single-tfs-team-project/\" target=\"_blank\" rel=\"noopener\">Why You Should Use a Single TFS Team Project</a><br><a href=\"https://nkdagility.com/one-team-project/\" target=\"_blank\" rel=\"noopener\">One Team Project to rule them all</a></p>\n<h3 id=\"SubTeam’s\"><a href=\"#SubTeam’s\" class=\"headerlink\" title=\"SubTeam’s\"></a>SubTeam’s</h3><p>Almost as soon as we were given teams many of my customers started asking about sub teams if we could break a team up into smaller components but still have them rolling up to a parent or an overall team.  Well right from the beginning of Teams we were sort of able to do this because the team is really defined by the Area Path.  The Area Path is one of two controls that we have in TFS that is a tree control and capable of nested items.<br>Since the 2012 of TFS, when you create a Team Project it creates a Team of the same name as the Project.  This means that any team that you create is really a nested child of this parent team.<br>The default behavior of creating a team is to create an area path with the same name as the team just under the root name of the Project which is parent team.  If we want to create any sub teams we need to do things a little different from the default way that we create them.  Lets walk through those steps.</p>\n<ol>\n<li>First be sure you are in the Project and not at the Organization level and at the bottom of the menu you will find a gear icon called Project settings.  Click on the icon.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ProjectSettings.jpg\" title=\"Create a Project\"></li>\n<li>Under the General section of this menu you should see a menu item called Teams.  Click on that, the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AddTeamButton.jpg\" title=\"Add Team Button\"></li>\n<li>Right under the Header of this page you should see what could be a button called “New team”.  Click on the button.</li>\n<li>We are going to call this team “Web” as this will be our parent for the web development that we shall perform.  Fill in the Team name and give it a description.  <strong>Note:</strong> <strong><em>for the creation of the parent team we leave the checkbox checked that says “Create an area path with the name of the team.”</em></strong>  In later steps when creating the sub-teams it is important that we uncheck that box.  More on that a little later.  Click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewParentTeam.jpg\" title=\"Create the parent team\"></li>\n<li>Still on the Project settings page, under the Boards section of this menu you should see a menu item called Project configuration.  Click on that, the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ProjectConfigurationButton.jpg\" title=\"Project configuration button\"></li>\n<li>This by default takes you to the Iterations management area.  Up close to the top of this page beside the Iterations you should see Areas.  Click on that, you mouse should turn into a hand when you hover over the word Areas.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AreasLink.jpg\" title=\"Area link\"></li>\n<li>Here you will see that the Web area is listed which happened when we created the Web Team (remember the checkbox to create an area path).  Now select the Web in the area and click on the New Child button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewChildButton.jpg\" title=\"New child button\"></li>\n<li>I am going to call this new Area name with Team-1 and click on the Save and close button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/Team-1AreaPath.jpg\" title=\"Team-1 area path\"></li>\n<li>With the Team-1 still selected we are going to create another child at the same level as this one and is going to be a sibling to this so instead of the New child button we click on the New button as this will create a new Area at the same level as the currently selected area.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewSecondChildProcess.jpg\" title=\"New second child added\"></li>\n<li>I am going to call this new Area name with Team-2 and click the Save and close button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/Team-2AreaPath.jpg\" title=\"Team-2 area path\"></li>\n<li>Still on the Project settings page, under the Boards section of this menu you should see a menu item called Team configuration.  Click on that, again the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TeamConfiguration.jpg\" title=\"Team configuration button\"></li>\n<li>You should still be on the Web Team, click on the Areas link and there you should see the area path of the Web.  You won’t see the child area paths in this view but we do want their values to roll up to this master web team page.  Click on the elipse button right beside the text default area.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/SelectWebAreaPath.jpg\" title=\"Select Web area path\"></li>\n<li>In the menu that pop’s up select the item that says “Include sub areas”<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/IncludeSubAreas.jpg\" title=\"Include sub areas\"></li>\n<li>After you clicked that you should see the “sub-areas are included” message on the right of the default area.  This is what you want to look for when you are trying to determine why some master teams are not getting their work items rolled up to the top.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/subareaincludedmessage.jpg\" title=\"sub areas are included message\"></li>\n<li>Now we are ready to go back to the teams page and create those other two teams that we created an area path for.  Back on the Project Settings under the General area click on the Teams button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/BackToTeams.jpg\" title=\"Back to the teams\"></li>\n<li>Click on the New team button<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-1.jpg\" title=\"New Team button\"></li>\n<li>This time in the Team name call this “Web-Team-1” because these teams will be listed as a list and the only way to know first hand which are parents and which are children is by naming convention and you will want to keep the parent together with their children and the ony way to do that is to make sure that the parent is the beginning part of the name of the team.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewWebTeam-1.jpg\" title=\"Title this Web-Team-1\"></li>\n<li>This next part is very important to observe, make sure that you <strong>un-check</strong> the <em>Create an area path with the name of the team check box.</em>  We already have an area path and we will assign that area path to the team in a couple of steps from now.  Enter a discription and then click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-1.jpg\" title=\"Create new Team-1\"></li>\n<li>Click on the New team button once more.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-2.jpg\" title=\"New Team button for Team-2\"></li>\n<li>Name this team “Web-Team-2”, put in a description and remember to <strong>un-check</strong> the <em>Create an area path with the name of the team</em> check box and then click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-2.jpg\" title=\"Create New Team-2\"></li>\n<li>Now that we have the teams created we need to attach them to the correct area path.  First click onto the Web-Team-1 link so that we are on that team.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg\" title=\"Click on Team-1\"></li>\n<li>Click on the Team configuration link.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TeamConfigurationClick.jpg\" title=\"Click on Team Configuration\"></li>\n<li>You will see a warning message and this warning is related to exactly what we are going to fix right now.  Currently those two sub-teams are in an invalid state.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ErrorMessage.jpg\" title=\"Warning message about invalid state\"></li>\n<li>Click on the Areas sub tab at the top of this section.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickAreas.jpg\" title=\"Click the areas link\"></li>\n<li>Lower on this page you should see a big green cross called Select area(s), click on this.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickSelectAreasButton.jpg\" title=\"Click on the Select Area(s) button\"></li>\n<li>This opens a new box and you want to expand the carets until you see the Team-1 area.  Click on Team-1.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1Area.jpg\" title=\"Click on the Team-1 Area choice.\"></li>\n<li>Click on the Save and close Button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickSaveAndClose.jpg\" title=\"Click on the Save and close button\"></li>\n<li>Repeat the same thing for Team-2 start from Task #21 and hook it up to the Team-2 area path.</li>\n</ol>\n<p>At this point you will have two sub-teams who’s backlog list will roll up to the master web team.  On the web team you will see all the work items from all the sub-teams below it.  With this configuration you get your own dashboard and kanban board as well.  Let’s say that you like a similar benefit like separate backlog lists but don’t want the overhead of additional dashboards and kanban boards and sub-team names that like flat on the full list of teams.  With the new filtering of the backlog lists this is now possible.  Lets look at that solution next.</p>\n<h3 id=\"One-Team-to-Manage-my-Sub-Teams\"><a href=\"#One-Team-to-Manage-my-Sub-Teams\" class=\"headerlink\" title=\"One Team to Manage my Sub-Teams\"></a>One Team to Manage my Sub-Teams</h3><p>To work with the existing things that we already have I am just going to delete the two sub-teams but leave all the area paths in place and configured as they were.</p>\n<ol>\n<li>Deleting a team is just as easy as it was to create them.  While we are still on the Project Settings page we want to click on the Teams button so that we see our list of Teams.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg\" title=\"Click on Teams to get to the list of teams\"></li>\n<li>Click on the ellipse button and on the menu that pops up click on Delete”<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/DeleteButton.jpg\" title=\"Click on the red x to delete\"></li>\n<li>A warning window pops up and on here you click the Delete team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/DeleteTeam.jpg\" title=\"Confirm, Delete Team\"></li>\n<li>Repeat the same operation to Delete Team-2</li>\n<li>So that we can see some data for there various sub teams we should add some data so that we can see how the filtering works on the one Web Team backlog list.  On the Boards menu click on Work Items.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/BoardsToWorkItems.jpg\" title=\"Click on Boards then Work Items\"></li>\n<li>Click on the New Work Item drop down and choose Bug.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ChooseBug.jpg\" title=\"Choose the bug item\"></li>\n<li>Give this bug a title of “This is a team-1 bug” and then select the correct area path which should be <the name=\"\" of=\"\" your=\"\" project=\"\">\\Web\\Team-1 as you probably realized, even though we deleted the team, the area path still stays.  Then click the Save button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateTheBug.jpg\" title=\"Create the bug\"></the></li>\n<li>Repeat the process and make another similar bug for Team-2</li>\n<li>Now with two bugs from two different teams lets go the backlog list for the Web.  Click on the Backlog menu item under the Boards menu and then expand the All teams backlogs so you can click on the Web backlogs item.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/GetToWebBacklog.jpg\" title=\"Navigate to the Web Backlog list\"></li>\n<li>You will see both of these bugs listed here because back in our previous exercise where we were creating teams we set the default behavior of the areas to roll up to the parent which in this case is Web.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TwoBugs.jpg\" title=\"See the two bugs listed\"></li>\n<li>Now, lets say we just want to see the bugs from Team-1 which is what you would see if you still had a Team-1 team.  Over in the upper right hand corner there is an icon that looks like a funnel.  This enables the Filter tool bar.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/FilterToolbar.jpg\" title=\"Click the Filter toolbar button\"></li>\n<li>Click on the Area dropdown and you see that I see my Team-1 and Team-2 (these are my sub-area paths).<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AreaDropDown.jpg\" title=\"Dropdown Area Filter\"></li>\n<li>Click on the Team-2 checkbox, instantly the filter kicks in and you only see Team-2 work items.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/OnlyTeam-2.jpg\" title=\"Only team-2 items are shown\"></li>\n<li>This will also work on the Kanban boards you have the same kind of filtering setup here as well.  The thing that I like about this method for handling the sub-teams is that if I had several teams under my parent team I could combine a couple of teams together in any combination that I need which is something I would not be able to do on the fly with the sub-team configuration.</li>\n</ol>\n<p>These are the two ways that I can see working with Teams and Sub-Teams right now.  If you don’t need the luxury of a separate burn down chart and capacity planning at the sub-team level I think you get everything you need right here with the single major area and manage the smaller breakdown of work using the nested area path.  Just remember to turn on the rolling up of sub areas up as the default is not to.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Back in around the 2012 release of TFS (now known as Azure DevOps) we were introduced to the concept of Teams.  This was a logical breakdown of a single project that typically could represent an organization or at least a division.  I come from the camp of one collection and one project as this gives you the most value and best experience and even though the product has evolved to be even more flexible then it has I think this is still the best approach to take today.  If you are unfamiliar with the one Project to rule them all here are some excellent blog posts to get you up to speed on the concept:<br><a href=\"https://www.imaginet.com/2013/why-you-should-use-single-tfs-team-project/\" target=\"_blank\" rel=\"noopener\">Why You Should Use a Single TFS Team Project</a><br><a href=\"https://nkdagility.com/one-team-project/\" target=\"_blank\" rel=\"noopener\">One Team Project to rule them all</a></p>\n<h3 id=\"SubTeam’s\"><a href=\"#SubTeam’s\" class=\"headerlink\" title=\"SubTeam’s\"></a>SubTeam’s</h3><p>Almost as soon as we were given teams many of my customers started asking about sub teams if we could break a team up into smaller components but still have them rolling up to a parent or an overall team.  Well right from the beginning of Teams we were sort of able to do this because the team is really defined by the Area Path.  The Area Path is one of two controls that we have in TFS that is a tree control and capable of nested items.<br>Since the 2012 of TFS, when you create a Team Project it creates a Team of the same name as the Project.  This means that any team that you create is really a nested child of this parent team.<br>The default behavior of creating a team is to create an area path with the same name as the team just under the root name of the Project which is parent team.  If we want to create any sub teams we need to do things a little different from the default way that we create them.  Lets walk through those steps.</p>\n<ol>\n<li>First be sure you are in the Project and not at the Organization level and at the bottom of the menu you will find a gear icon called Project settings.  Click on the icon.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ProjectSettings.jpg\" title=\"Create a Project\"></li>\n<li>Under the General section of this menu you should see a menu item called Teams.  Click on that, the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AddTeamButton.jpg\" title=\"Add Team Button\"></li>\n<li>Right under the Header of this page you should see what could be a button called “New team”.  Click on the button.</li>\n<li>We are going to call this team “Web” as this will be our parent for the web development that we shall perform.  Fill in the Team name and give it a description.  <strong>Note:</strong> <strong><em>for the creation of the parent team we leave the checkbox checked that says “Create an area path with the name of the team.”</em></strong>  In later steps when creating the sub-teams it is important that we uncheck that box.  More on that a little later.  Click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewParentTeam.jpg\" title=\"Create the parent team\"></li>\n<li>Still on the Project settings page, under the Boards section of this menu you should see a menu item called Project configuration.  Click on that, the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ProjectConfigurationButton.jpg\" title=\"Project configuration button\"></li>\n<li>This by default takes you to the Iterations management area.  Up close to the top of this page beside the Iterations you should see Areas.  Click on that, you mouse should turn into a hand when you hover over the word Areas.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AreasLink.jpg\" title=\"Area link\"></li>\n<li>Here you will see that the Web area is listed which happened when we created the Web Team (remember the checkbox to create an area path).  Now select the Web in the area and click on the New Child button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewChildButton.jpg\" title=\"New child button\"></li>\n<li>I am going to call this new Area name with Team-1 and click on the Save and close button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/Team-1AreaPath.jpg\" title=\"Team-1 area path\"></li>\n<li>With the Team-1 still selected we are going to create another child at the same level as this one and is going to be a sibling to this so instead of the New child button we click on the New button as this will create a new Area at the same level as the currently selected area.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewSecondChildProcess.jpg\" title=\"New second child added\"></li>\n<li>I am going to call this new Area name with Team-2 and click the Save and close button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/Team-2AreaPath.jpg\" title=\"Team-2 area path\"></li>\n<li>Still on the Project settings page, under the Boards section of this menu you should see a menu item called Team configuration.  Click on that, again the line becomes a click item when you hover over it.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TeamConfiguration.jpg\" title=\"Team configuration button\"></li>\n<li>You should still be on the Web Team, click on the Areas link and there you should see the area path of the Web.  You won’t see the child area paths in this view but we do want their values to roll up to this master web team page.  Click on the elipse button right beside the text default area.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/SelectWebAreaPath.jpg\" title=\"Select Web area path\"></li>\n<li>In the menu that pop’s up select the item that says “Include sub areas”<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/IncludeSubAreas.jpg\" title=\"Include sub areas\"></li>\n<li>After you clicked that you should see the “sub-areas are included” message on the right of the default area.  This is what you want to look for when you are trying to determine why some master teams are not getting their work items rolled up to the top.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/subareaincludedmessage.jpg\" title=\"sub areas are included message\"></li>\n<li>Now we are ready to go back to the teams page and create those other two teams that we created an area path for.  Back on the Project Settings under the General area click on the Teams button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/BackToTeams.jpg\" title=\"Back to the teams\"></li>\n<li>Click on the New team button<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-1.jpg\" title=\"New Team button\"></li>\n<li>This time in the Team name call this “Web-Team-1” because these teams will be listed as a list and the only way to know first hand which are parents and which are children is by naming convention and you will want to keep the parent together with their children and the ony way to do that is to make sure that the parent is the beginning part of the name of the team.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewWebTeam-1.jpg\" title=\"Title this Web-Team-1\"></li>\n<li>This next part is very important to observe, make sure that you <strong>un-check</strong> the <em>Create an area path with the name of the team check box.</em>  We already have an area path and we will assign that area path to the team in a couple of steps from now.  Enter a discription and then click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-1.jpg\" title=\"Create new Team-1\"></li>\n<li>Click on the New team button once more.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-2.jpg\" title=\"New Team button for Team-2\"></li>\n<li>Name this team “Web-Team-2”, put in a description and remember to <strong>un-check</strong> the <em>Create an area path with the name of the team</em> check box and then click on the Create team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-2.jpg\" title=\"Create New Team-2\"></li>\n<li>Now that we have the teams created we need to attach them to the correct area path.  First click onto the Web-Team-1 link so that we are on that team.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg\" title=\"Click on Team-1\"></li>\n<li>Click on the Team configuration link.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TeamConfigurationClick.jpg\" title=\"Click on Team Configuration\"></li>\n<li>You will see a warning message and this warning is related to exactly what we are going to fix right now.  Currently those two sub-teams are in an invalid state.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ErrorMessage.jpg\" title=\"Warning message about invalid state\"></li>\n<li>Click on the Areas sub tab at the top of this section.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickAreas.jpg\" title=\"Click the areas link\"></li>\n<li>Lower on this page you should see a big green cross called Select area(s), click on this.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickSelectAreasButton.jpg\" title=\"Click on the Select Area(s) button\"></li>\n<li>This opens a new box and you want to expand the carets until you see the Team-1 area.  Click on Team-1.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1Area.jpg\" title=\"Click on the Team-1 Area choice.\"></li>\n<li>Click on the Save and close Button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickSaveAndClose.jpg\" title=\"Click on the Save and close button\"></li>\n<li>Repeat the same thing for Team-2 start from Task #21 and hook it up to the Team-2 area path.</li>\n</ol>\n<p>At this point you will have two sub-teams who’s backlog list will roll up to the master web team.  On the web team you will see all the work items from all the sub-teams below it.  With this configuration you get your own dashboard and kanban board as well.  Let’s say that you like a similar benefit like separate backlog lists but don’t want the overhead of additional dashboards and kanban boards and sub-team names that like flat on the full list of teams.  With the new filtering of the backlog lists this is now possible.  Lets look at that solution next.</p>\n<h3 id=\"One-Team-to-Manage-my-Sub-Teams\"><a href=\"#One-Team-to-Manage-my-Sub-Teams\" class=\"headerlink\" title=\"One Team to Manage my Sub-Teams\"></a>One Team to Manage my Sub-Teams</h3><p>To work with the existing things that we already have I am just going to delete the two sub-teams but leave all the area paths in place and configured as they were.</p>\n<ol>\n<li>Deleting a team is just as easy as it was to create them.  While we are still on the Project Settings page we want to click on the Teams button so that we see our list of Teams.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg\" title=\"Click on Teams to get to the list of teams\"></li>\n<li>Click on the ellipse button and on the menu that pops up click on Delete”<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/DeleteButton.jpg\" title=\"Click on the red x to delete\"></li>\n<li>A warning window pops up and on here you click the Delete team button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/DeleteTeam.jpg\" title=\"Confirm, Delete Team\"></li>\n<li>Repeat the same operation to Delete Team-2</li>\n<li>So that we can see some data for there various sub teams we should add some data so that we can see how the filtering works on the one Web Team backlog list.  On the Boards menu click on Work Items.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/BoardsToWorkItems.jpg\" title=\"Click on Boards then Work Items\"></li>\n<li>Click on the New Work Item drop down and choose Bug.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/ChooseBug.jpg\" title=\"Choose the bug item\"></li>\n<li>Give this bug a title of “This is a team-1 bug” and then select the correct area path which should be <the name=\"\" of=\"\" your=\"\" project=\"\">\\Web\\Team-1 as you probably realized, even though we deleted the team, the area path still stays.  Then click the Save button.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/CreateTheBug.jpg\" title=\"Create the bug\"></the></li>\n<li>Repeat the process and make another similar bug for Team-2</li>\n<li>Now with two bugs from two different teams lets go the backlog list for the Web.  Click on the Backlog menu item under the Boards menu and then expand the All teams backlogs so you can click on the Web backlogs item.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/GetToWebBacklog.jpg\" title=\"Navigate to the Web Backlog list\"></li>\n<li>You will see both of these bugs listed here because back in our previous exercise where we were creating teams we set the default behavior of the areas to roll up to the parent which in this case is Web.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/TwoBugs.jpg\" title=\"See the two bugs listed\"></li>\n<li>Now, lets say we just want to see the bugs from Team-1 which is what you would see if you still had a Team-1 team.  Over in the upper right hand corner there is an icon that looks like a funnel.  This enables the Filter tool bar.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/FilterToolbar.jpg\" title=\"Click the Filter toolbar button\"></li>\n<li>Click on the Area dropdown and you see that I see my Team-1 and Team-2 (these are my sub-area paths).<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/AreaDropDown.jpg\" title=\"Dropdown Area Filter\"></li>\n<li>Click on the Team-2 checkbox, instantly the filter kicks in and you only see Team-2 work items.<img src=\"/2019/03/Teams-SubTeams-and-Area-Path-s/OnlyTeam-2.jpg\" title=\"Only team-2 items are shown\"></li>\n<li>This will also work on the Kanban boards you have the same kind of filtering setup here as well.  The thing that I like about this method for handling the sub-teams is that if I had several teams under my parent team I could combine a couple of teams together in any combination that I need which is something I would not be able to do on the fly with the sub-team configuration.</li>\n</ol>\n<p>These are the two ways that I can see working with Teams and Sub-Teams right now.  If you don’t need the luxury of a separate burn down chart and capacity planning at the sub-team level I think you get everything you need right here with the single major area and manage the smaller breakdown of work using the nested area path.  Just remember to turn on the rolling up of sub areas up as the default is not to.</p>\n"},{"title":"The Power of Time Tracking","date":"2008-06-21T07:00:00.000Z","_content":"I love to keep track of time.  It could be related to my love of data and all the information that I can extract from it: how much fuel does my car use, how much time do I spend on stuff each week, how many hours am I away from my family. \n\nActually my attraction to time tracking goes much deeper than that.  I never planned to start my own business when I made the career change into software development so many years ago.  I had seen how it worked being self employed as I grew up in a family owned business and my father was in charge of the service department of an auto / agriculture dealership.  One of the things I noticed was that as the billing was being discussed from the details obtained from the back of the work order, the customers would be requesting a discount because they did not see enough detail that explained why the job took as long as it did.  This was not something I was looking forward to experiencing myself being self employed. \n\nI do not like to negotiate.  I am not a good negotiator.  Instead I would like to have my work speak for itself and it has for many, many years.  So when I did end up being self employed presented my client with an invoice, I also had the opportunity to present them with a detailed accounting of what that invoice represented.  Sometimes it read just like a book, but I never had to explain my work.  There was never any negotiation about the amount of the invoice that I was presenting and I always got paid on time.  Mission accomplished. \n\nThat was my original motivation to really get into time tracking, I have built various pieces of internal software (after all, I am a software developer) that have helped me to maintain my goals.  Since then I have discovered other benefits to keeping tracking of time and for the rest of this article I want to detail these benefits. \n## Deliverable\nWhen working on a long project many times the client would only pay on some sort of a deliverable.  We can all agree that we don’t want to pay for something we have not yet received.  I did discover that I could use my detailed time tracking entries as a deliverable since the client was able to receive something from me that they could use to justify approval of payment. \n\nWhen I was in college an instructor said that we should be paid for each stage of the software life cycle.  At first I had trouble with the concept because at that time I only pictured the deliverable as the final finished product.  However, my perception was very small in the overall grasp of developing software for a client.  I soon discovered that sometimes all that I ended up doing for the client was research and some feasibility studies, sometimes working on the specifications and never got a chance to work on the actual software.  Also, I have worked on projects where the specification phase went on for almost a year; collecting rules and processes and writing about the software that would be built.  I needed regular pay periods.  The only way to do this and justify my demands was to provide a deliverable that ended up being the details from my time tracking efforts. \n## Resolve Disputes \nAnytime you need to resolve a dispute, details play a very important role.  I worked on a project quite some time ago that did lead to some legal confrontation.  My recorded project details were used to justify the amount of time that was spent on the project and why a deposit should not be returned.  It is important to keep track of how and where you spend your time. \n\nI know that this is also a good rule with dealing with tax situations.  Revenue Canada and the IRS want details and many legal actions have been taken against individuals simply because they could not produce enough details.  I can hardly remember what I did a few hours ago let alone days, months or years, but if I have details in front of me, it sure helps jog my memory. \n## How Much Does it Cost? \nWe all have ideas as to how long we work on a task or even a project.  Sometimes I can hardly believe that a certain task took me as long as it did.  It felt like five minutes but in reality it took me four hours.  When you start tracking details of your day with real time, you get very clear evidence of the time that was really consumed. \n\nIn my own anal way I not only record the time and detail that I can bill my clients, I also keep track of how long I spend on the road, reading my technical books and papers, and the internal projects.  This altogether tells me how much time this is costing me away from my family.  It helps me keep my life in perspective and allows me to make better decisions.  If I have only allowed for a few hours of time to spend with the family this week, maybe it is time to go and have that game of handball with my stepdaughter or go for a nice leisurely stroll with my wife or give my stepson a call just to see how he is doing. \n\nThis is a very monetary world and time costs us money.  Sometime this is good since it helps us to provide for our families and sometimes the cost is great because it takes us away from them.  However, if you don’t track it, how are you going to know how well you are balancing your life?  What is the cost? \n\nThis raises another thought from another life. When I was into the financial world, (okay I was an accountant for the family business), I would have people asking me advise on how they could construct a budget.  My advise is always the same; you first need to be extremely anal about tracking your spending, because before you can start budgeting you need to know where you money is going.  That is how I see Time Tracking. \n## The Plug \nOver the years I have built several applications that tracked time.  First with an Access Database, then a VB front end and SQL backend.  The problem with both of these was the synchronization to a central data store.  For the last three-and-a-half years I worked for a company that built a time machine on the web.  I thought that I would continue working for them until I decided to move on or retired, so I stopped thinking about building a better time tracker.  Their software allowed me to keep track of all the things I had become accustomed to tracking. \n\nI regretted this decision when the company got sold to a medium size corporation that was acquiring software companies across the country.  The head office insisted that we use a multi-million dollar time tracking system which was in my eyes worthless.  I could not maintain the level of detail to which I had grown accustom.  None of us could see the point of this since it did not produce detailed invoices for our clients.  Now red flags were flying for our clients, they all loved to know the details of what we were doing for them.  Anyway, the company closed its doors and I found myself again being an independent software developer and needing some form of time tracking system, so I built Time Tracker.  The product is still evolving and I may release a commercial version of the product some time in the future. \n{% asset_img image.jpg \"Time I spent away from my family in September\" %}\nIf you would like to know more about my Time Tracker program and or are interested in finding out how you can implement Time Tracker in your facility, send email to: [TimeTracker@TheWebWeWeave.net](mailto:TimeTracker@TheWebWeWeave.net) \n\nMy name is Donald L. Schulz and I like to keep track of my time. \n","source":"_posts/The-Power-of-Time-Tracking.md","raw":"title: The Power of Time Tracking\ndate: 2008-06-21\ntags: \n- Time Tracker\n- Products\n---\nI love to keep track of time.  It could be related to my love of data and all the information that I can extract from it: how much fuel does my car use, how much time do I spend on stuff each week, how many hours am I away from my family. \n\nActually my attraction to time tracking goes much deeper than that.  I never planned to start my own business when I made the career change into software development so many years ago.  I had seen how it worked being self employed as I grew up in a family owned business and my father was in charge of the service department of an auto / agriculture dealership.  One of the things I noticed was that as the billing was being discussed from the details obtained from the back of the work order, the customers would be requesting a discount because they did not see enough detail that explained why the job took as long as it did.  This was not something I was looking forward to experiencing myself being self employed. \n\nI do not like to negotiate.  I am not a good negotiator.  Instead I would like to have my work speak for itself and it has for many, many years.  So when I did end up being self employed presented my client with an invoice, I also had the opportunity to present them with a detailed accounting of what that invoice represented.  Sometimes it read just like a book, but I never had to explain my work.  There was never any negotiation about the amount of the invoice that I was presenting and I always got paid on time.  Mission accomplished. \n\nThat was my original motivation to really get into time tracking, I have built various pieces of internal software (after all, I am a software developer) that have helped me to maintain my goals.  Since then I have discovered other benefits to keeping tracking of time and for the rest of this article I want to detail these benefits. \n## Deliverable\nWhen working on a long project many times the client would only pay on some sort of a deliverable.  We can all agree that we don’t want to pay for something we have not yet received.  I did discover that I could use my detailed time tracking entries as a deliverable since the client was able to receive something from me that they could use to justify approval of payment. \n\nWhen I was in college an instructor said that we should be paid for each stage of the software life cycle.  At first I had trouble with the concept because at that time I only pictured the deliverable as the final finished product.  However, my perception was very small in the overall grasp of developing software for a client.  I soon discovered that sometimes all that I ended up doing for the client was research and some feasibility studies, sometimes working on the specifications and never got a chance to work on the actual software.  Also, I have worked on projects where the specification phase went on for almost a year; collecting rules and processes and writing about the software that would be built.  I needed regular pay periods.  The only way to do this and justify my demands was to provide a deliverable that ended up being the details from my time tracking efforts. \n## Resolve Disputes \nAnytime you need to resolve a dispute, details play a very important role.  I worked on a project quite some time ago that did lead to some legal confrontation.  My recorded project details were used to justify the amount of time that was spent on the project and why a deposit should not be returned.  It is important to keep track of how and where you spend your time. \n\nI know that this is also a good rule with dealing with tax situations.  Revenue Canada and the IRS want details and many legal actions have been taken against individuals simply because they could not produce enough details.  I can hardly remember what I did a few hours ago let alone days, months or years, but if I have details in front of me, it sure helps jog my memory. \n## How Much Does it Cost? \nWe all have ideas as to how long we work on a task or even a project.  Sometimes I can hardly believe that a certain task took me as long as it did.  It felt like five minutes but in reality it took me four hours.  When you start tracking details of your day with real time, you get very clear evidence of the time that was really consumed. \n\nIn my own anal way I not only record the time and detail that I can bill my clients, I also keep track of how long I spend on the road, reading my technical books and papers, and the internal projects.  This altogether tells me how much time this is costing me away from my family.  It helps me keep my life in perspective and allows me to make better decisions.  If I have only allowed for a few hours of time to spend with the family this week, maybe it is time to go and have that game of handball with my stepdaughter or go for a nice leisurely stroll with my wife or give my stepson a call just to see how he is doing. \n\nThis is a very monetary world and time costs us money.  Sometime this is good since it helps us to provide for our families and sometimes the cost is great because it takes us away from them.  However, if you don’t track it, how are you going to know how well you are balancing your life?  What is the cost? \n\nThis raises another thought from another life. When I was into the financial world, (okay I was an accountant for the family business), I would have people asking me advise on how they could construct a budget.  My advise is always the same; you first need to be extremely anal about tracking your spending, because before you can start budgeting you need to know where you money is going.  That is how I see Time Tracking. \n## The Plug \nOver the years I have built several applications that tracked time.  First with an Access Database, then a VB front end and SQL backend.  The problem with both of these was the synchronization to a central data store.  For the last three-and-a-half years I worked for a company that built a time machine on the web.  I thought that I would continue working for them until I decided to move on or retired, so I stopped thinking about building a better time tracker.  Their software allowed me to keep track of all the things I had become accustomed to tracking. \n\nI regretted this decision when the company got sold to a medium size corporation that was acquiring software companies across the country.  The head office insisted that we use a multi-million dollar time tracking system which was in my eyes worthless.  I could not maintain the level of detail to which I had grown accustom.  None of us could see the point of this since it did not produce detailed invoices for our clients.  Now red flags were flying for our clients, they all loved to know the details of what we were doing for them.  Anyway, the company closed its doors and I found myself again being an independent software developer and needing some form of time tracking system, so I built Time Tracker.  The product is still evolving and I may release a commercial version of the product some time in the future. \n{% asset_img image.jpg \"Time I spent away from my family in September\" %}\nIf you would like to know more about my Time Tracker program and or are interested in finding out how you can implement Time Tracker in your facility, send email to: [TimeTracker@TheWebWeWeave.net](mailto:TimeTracker@TheWebWeWeave.net) \n\nMy name is Donald L. Schulz and I like to keep track of my time. \n","slug":"The-Power-of-Time-Tracking","published":1,"updated":"2020-01-05T00:36:05.103Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgg9000ns4uf2xflmle5","content":"<p>I love to keep track of time.  It could be related to my love of data and all the information that I can extract from it: how much fuel does my car use, how much time do I spend on stuff each week, how many hours am I away from my family. </p>\n<p>Actually my attraction to time tracking goes much deeper than that.  I never planned to start my own business when I made the career change into software development so many years ago.  I had seen how it worked being self employed as I grew up in a family owned business and my father was in charge of the service department of an auto / agriculture dealership.  One of the things I noticed was that as the billing was being discussed from the details obtained from the back of the work order, the customers would be requesting a discount because they did not see enough detail that explained why the job took as long as it did.  This was not something I was looking forward to experiencing myself being self employed. </p>\n<p>I do not like to negotiate.  I am not a good negotiator.  Instead I would like to have my work speak for itself and it has for many, many years.  So when I did end up being self employed presented my client with an invoice, I also had the opportunity to present them with a detailed accounting of what that invoice represented.  Sometimes it read just like a book, but I never had to explain my work.  There was never any negotiation about the amount of the invoice that I was presenting and I always got paid on time.  Mission accomplished. </p>\n<p>That was my original motivation to really get into time tracking, I have built various pieces of internal software (after all, I am a software developer) that have helped me to maintain my goals.  Since then I have discovered other benefits to keeping tracking of time and for the rest of this article I want to detail these benefits. </p>\n<h2 id=\"Deliverable\"><a href=\"#Deliverable\" class=\"headerlink\" title=\"Deliverable\"></a>Deliverable</h2><p>When working on a long project many times the client would only pay on some sort of a deliverable.  We can all agree that we don’t want to pay for something we have not yet received.  I did discover that I could use my detailed time tracking entries as a deliverable since the client was able to receive something from me that they could use to justify approval of payment. </p>\n<p>When I was in college an instructor said that we should be paid for each stage of the software life cycle.  At first I had trouble with the concept because at that time I only pictured the deliverable as the final finished product.  However, my perception was very small in the overall grasp of developing software for a client.  I soon discovered that sometimes all that I ended up doing for the client was research and some feasibility studies, sometimes working on the specifications and never got a chance to work on the actual software.  Also, I have worked on projects where the specification phase went on for almost a year; collecting rules and processes and writing about the software that would be built.  I needed regular pay periods.  The only way to do this and justify my demands was to provide a deliverable that ended up being the details from my time tracking efforts. </p>\n<h2 id=\"Resolve-Disputes\"><a href=\"#Resolve-Disputes\" class=\"headerlink\" title=\"Resolve Disputes\"></a>Resolve Disputes</h2><p>Anytime you need to resolve a dispute, details play a very important role.  I worked on a project quite some time ago that did lead to some legal confrontation.  My recorded project details were used to justify the amount of time that was spent on the project and why a deposit should not be returned.  It is important to keep track of how and where you spend your time. </p>\n<p>I know that this is also a good rule with dealing with tax situations.  Revenue Canada and the IRS want details and many legal actions have been taken against individuals simply because they could not produce enough details.  I can hardly remember what I did a few hours ago let alone days, months or years, but if I have details in front of me, it sure helps jog my memory. </p>\n<h2 id=\"How-Much-Does-it-Cost\"><a href=\"#How-Much-Does-it-Cost\" class=\"headerlink\" title=\"How Much Does it Cost?\"></a>How Much Does it Cost?</h2><p>We all have ideas as to how long we work on a task or even a project.  Sometimes I can hardly believe that a certain task took me as long as it did.  It felt like five minutes but in reality it took me four hours.  When you start tracking details of your day with real time, you get very clear evidence of the time that was really consumed. </p>\n<p>In my own anal way I not only record the time and detail that I can bill my clients, I also keep track of how long I spend on the road, reading my technical books and papers, and the internal projects.  This altogether tells me how much time this is costing me away from my family.  It helps me keep my life in perspective and allows me to make better decisions.  If I have only allowed for a few hours of time to spend with the family this week, maybe it is time to go and have that game of handball with my stepdaughter or go for a nice leisurely stroll with my wife or give my stepson a call just to see how he is doing. </p>\n<p>This is a very monetary world and time costs us money.  Sometime this is good since it helps us to provide for our families and sometimes the cost is great because it takes us away from them.  However, if you don’t track it, how are you going to know how well you are balancing your life?  What is the cost? </p>\n<p>This raises another thought from another life. When I was into the financial world, (okay I was an accountant for the family business), I would have people asking me advise on how they could construct a budget.  My advise is always the same; you first need to be extremely anal about tracking your spending, because before you can start budgeting you need to know where you money is going.  That is how I see Time Tracking. </p>\n<h2 id=\"The-Plug\"><a href=\"#The-Plug\" class=\"headerlink\" title=\"The Plug\"></a>The Plug</h2><p>Over the years I have built several applications that tracked time.  First with an Access Database, then a VB front end and SQL backend.  The problem with both of these was the synchronization to a central data store.  For the last three-and-a-half years I worked for a company that built a time machine on the web.  I thought that I would continue working for them until I decided to move on or retired, so I stopped thinking about building a better time tracker.  Their software allowed me to keep track of all the things I had become accustomed to tracking. </p>\n<p>I regretted this decision when the company got sold to a medium size corporation that was acquiring software companies across the country.  The head office insisted that we use a multi-million dollar time tracking system which was in my eyes worthless.  I could not maintain the level of detail to which I had grown accustom.  None of us could see the point of this since it did not produce detailed invoices for our clients.  Now red flags were flying for our clients, they all loved to know the details of what we were doing for them.  Anyway, the company closed its doors and I found myself again being an independent software developer and needing some form of time tracking system, so I built Time Tracker.  The product is still evolving and I may release a commercial version of the product some time in the future.<br><img src=\"/2008/06/The-Power-of-Time-Tracking/image.jpg\" title=\"Time I spent away from my family in September\"><br>If you would like to know more about my Time Tracker program and or are interested in finding out how you can implement Time Tracker in your facility, send email to: <a href=\"mailto:TimeTracker@TheWebWeWeave.net\" target=\"_blank\" rel=\"noopener\">TimeTracker@TheWebWeWeave.net</a> </p>\n<p>My name is Donald L. Schulz and I like to keep track of my time. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>I love to keep track of time.  It could be related to my love of data and all the information that I can extract from it: how much fuel does my car use, how much time do I spend on stuff each week, how many hours am I away from my family. </p>\n<p>Actually my attraction to time tracking goes much deeper than that.  I never planned to start my own business when I made the career change into software development so many years ago.  I had seen how it worked being self employed as I grew up in a family owned business and my father was in charge of the service department of an auto / agriculture dealership.  One of the things I noticed was that as the billing was being discussed from the details obtained from the back of the work order, the customers would be requesting a discount because they did not see enough detail that explained why the job took as long as it did.  This was not something I was looking forward to experiencing myself being self employed. </p>\n<p>I do not like to negotiate.  I am not a good negotiator.  Instead I would like to have my work speak for itself and it has for many, many years.  So when I did end up being self employed presented my client with an invoice, I also had the opportunity to present them with a detailed accounting of what that invoice represented.  Sometimes it read just like a book, but I never had to explain my work.  There was never any negotiation about the amount of the invoice that I was presenting and I always got paid on time.  Mission accomplished. </p>\n<p>That was my original motivation to really get into time tracking, I have built various pieces of internal software (after all, I am a software developer) that have helped me to maintain my goals.  Since then I have discovered other benefits to keeping tracking of time and for the rest of this article I want to detail these benefits. </p>\n<h2 id=\"Deliverable\"><a href=\"#Deliverable\" class=\"headerlink\" title=\"Deliverable\"></a>Deliverable</h2><p>When working on a long project many times the client would only pay on some sort of a deliverable.  We can all agree that we don’t want to pay for something we have not yet received.  I did discover that I could use my detailed time tracking entries as a deliverable since the client was able to receive something from me that they could use to justify approval of payment. </p>\n<p>When I was in college an instructor said that we should be paid for each stage of the software life cycle.  At first I had trouble with the concept because at that time I only pictured the deliverable as the final finished product.  However, my perception was very small in the overall grasp of developing software for a client.  I soon discovered that sometimes all that I ended up doing for the client was research and some feasibility studies, sometimes working on the specifications and never got a chance to work on the actual software.  Also, I have worked on projects where the specification phase went on for almost a year; collecting rules and processes and writing about the software that would be built.  I needed regular pay periods.  The only way to do this and justify my demands was to provide a deliverable that ended up being the details from my time tracking efforts. </p>\n<h2 id=\"Resolve-Disputes\"><a href=\"#Resolve-Disputes\" class=\"headerlink\" title=\"Resolve Disputes\"></a>Resolve Disputes</h2><p>Anytime you need to resolve a dispute, details play a very important role.  I worked on a project quite some time ago that did lead to some legal confrontation.  My recorded project details were used to justify the amount of time that was spent on the project and why a deposit should not be returned.  It is important to keep track of how and where you spend your time. </p>\n<p>I know that this is also a good rule with dealing with tax situations.  Revenue Canada and the IRS want details and many legal actions have been taken against individuals simply because they could not produce enough details.  I can hardly remember what I did a few hours ago let alone days, months or years, but if I have details in front of me, it sure helps jog my memory. </p>\n<h2 id=\"How-Much-Does-it-Cost\"><a href=\"#How-Much-Does-it-Cost\" class=\"headerlink\" title=\"How Much Does it Cost?\"></a>How Much Does it Cost?</h2><p>We all have ideas as to how long we work on a task or even a project.  Sometimes I can hardly believe that a certain task took me as long as it did.  It felt like five minutes but in reality it took me four hours.  When you start tracking details of your day with real time, you get very clear evidence of the time that was really consumed. </p>\n<p>In my own anal way I not only record the time and detail that I can bill my clients, I also keep track of how long I spend on the road, reading my technical books and papers, and the internal projects.  This altogether tells me how much time this is costing me away from my family.  It helps me keep my life in perspective and allows me to make better decisions.  If I have only allowed for a few hours of time to spend with the family this week, maybe it is time to go and have that game of handball with my stepdaughter or go for a nice leisurely stroll with my wife or give my stepson a call just to see how he is doing. </p>\n<p>This is a very monetary world and time costs us money.  Sometime this is good since it helps us to provide for our families and sometimes the cost is great because it takes us away from them.  However, if you don’t track it, how are you going to know how well you are balancing your life?  What is the cost? </p>\n<p>This raises another thought from another life. When I was into the financial world, (okay I was an accountant for the family business), I would have people asking me advise on how they could construct a budget.  My advise is always the same; you first need to be extremely anal about tracking your spending, because before you can start budgeting you need to know where you money is going.  That is how I see Time Tracking. </p>\n<h2 id=\"The-Plug\"><a href=\"#The-Plug\" class=\"headerlink\" title=\"The Plug\"></a>The Plug</h2><p>Over the years I have built several applications that tracked time.  First with an Access Database, then a VB front end and SQL backend.  The problem with both of these was the synchronization to a central data store.  For the last three-and-a-half years I worked for a company that built a time machine on the web.  I thought that I would continue working for them until I decided to move on or retired, so I stopped thinking about building a better time tracker.  Their software allowed me to keep track of all the things I had become accustomed to tracking. </p>\n<p>I regretted this decision when the company got sold to a medium size corporation that was acquiring software companies across the country.  The head office insisted that we use a multi-million dollar time tracking system which was in my eyes worthless.  I could not maintain the level of detail to which I had grown accustom.  None of us could see the point of this since it did not produce detailed invoices for our clients.  Now red flags were flying for our clients, they all loved to know the details of what we were doing for them.  Anyway, the company closed its doors and I found myself again being an independent software developer and needing some form of time tracking system, so I built Time Tracker.  The product is still evolving and I may release a commercial version of the product some time in the future.<br><img src=\"/2008/06/The-Power-of-Time-Tracking/image.jpg\" title=\"Time I spent away from my family in September\"><br>If you would like to know more about my Time Tracker program and or are interested in finding out how you can implement Time Tracker in your facility, send email to: <a href=\"mailto:TimeTracker@TheWebWeWeave.net\" target=\"_blank\" rel=\"noopener\">TimeTracker@TheWebWeWeave.net</a> </p>\n<p>My name is Donald L. Schulz and I like to keep track of my time. </p>\n"},{"title":"The Two Opposite IT Agenda's","date":"2009-08-17T07:00:00.000Z","_content":"## The Problem\nI have been in the Information Technology (IT) field for a long time and most of that time has been spent in the development space.  Each environment different from the previous one and in some cases there were huge gaps in the level of technology that was used and made available in each location.  This has stumped me for a long time why this was.  You go to a user group meeting and when ever the speaker was speaking about a technology that was current and he would conduct a quick survey around the room how many were using this technology, the results would be very mixed.  There would even be lots of users at these meetings where they were still using technologies that were over 10 years old and no sign of moving forward. \n## Why is this happening?\nGood question, and after thinking about this for a long, long time I think I have the answer.  It really depends on which aspect of the IT spectrum is controlling the development side.  I think it has become quite acceptable to break up the whole IT space into two major groups, the **IT Professionals**, and the **Software Developers**.  When I first moved to California I worked for a company that was a software developer and they did software development for their clients on a time and materials basis.  There was no question as to which wing of IT influenced the company with regards to technology and hardware.  The developers in this case were golden, if you needed special tools, you got them.  Need a faster computer, more monitors, special machines to run beta versions of the latest OS and development tools, you got it.  You were on the bleeding edge and the IT Professionals were there to help you slow down the bleeding when that go out of control.  However, this company was always current got the best jobs and in a lot of cases when we deployed our final product to their production systems that would be the point at which their IT department would then be forced to update their systems and move to the new round of technology.\n## Welcome to the Other Side\nWhat happens when the influence is on the other foot, the IT Professionals.  They have a different agenda as their number one goal is stability, security, and easy deployment.  However this does come with a cost, especially when the company is heavily relying on technology to push its products.  I have heard this from many different companies all with in this same situation, that they are not a technology company, the technology is just the tool or one of the tools to deliver their products.  When this group controls the budget and the overall technical agenda things like this will happen.  Moving forward will be very, very slow and the focus will be purely on deployment issues and keeping those costs under control and not on the cost of development which could get very expensive as the technology changes and you are not able to take advantage of those opportunities.  Over time, the customers that receive your products will start to evaluate your future as not being able to move fast enough for them because they are going to expect you to be out there and fully tested these waters before they move there and if your not it is not going to look favorable in their eyes.  This is especially true if you have some completion in your space that are adapting the new technologies faster then your company is. \n\nThere is another side to this that I have witnessed which bothers me even more.  The decision to move all enterprise applications to the web was never from the development side of IT but came from the IT Professionals.  Remember one of their big agendas is the easy, easy deployment and as a result they have made software development so expensive that we have been forced to move as much as we can to off shore development companies.  In most cases this hasn't even come close to a cost savings for the applications as you never seem to get what you thought your were designing and it is not always the fault of the off shore companies, they are giving your exactly what you asked for.  In more cases it is the wrong technology for the solution.  Most high volume enterprise applications were desktop applications with a lot of state (data that you are working with).  The web is stateless and over the years many things have been developed to make the web appear state full but is it not.  I have seen projects spend 100 times more time and money into implementing a features on a web to make it appear and feel like a desktop application.  Now to be clear this argument started when deployment of desktop applications was hard as in the early days there was no easy way to keep all the desktops up to date except to physically go around and update them as patches and newer versions became available.  However, in the last 5 years or more that has totally changed with things like click-once technology you can implement full automatic updates and license enforcement just as easily as web sites and maybe even better.  We all know there are new tricks every day to spoof a web site into some new security trick. \n## What's the Answer\nI don't really have the answer but I do have a few thoughts that I have been thinking about and I would love to hear about some other ideas that you might have.  My thought is that you should separate the IT push down two paths and this advice is for the companies that are currently being held back by the stabilizing IT Professionals.  I would even go so far as to keep the developers on a separate network then the rest of the employees this will keep the bleeding on the other side of the fence and not affect your sales and support staff which are there to sell and support products that are stable and need to keep them that way.  This will allow the development to improve and expand their technical expertise and provide better and more relevant solutions for your customers, internal and external.  ","source":"_posts/The-Two-Opposite-IT-Agenda-s.md","raw":"title: \"The Two Opposite IT Agenda's\"\ndate: 2009-08-17\ntags:\n- Politics\n---\n## The Problem\nI have been in the Information Technology (IT) field for a long time and most of that time has been spent in the development space.  Each environment different from the previous one and in some cases there were huge gaps in the level of technology that was used and made available in each location.  This has stumped me for a long time why this was.  You go to a user group meeting and when ever the speaker was speaking about a technology that was current and he would conduct a quick survey around the room how many were using this technology, the results would be very mixed.  There would even be lots of users at these meetings where they were still using technologies that were over 10 years old and no sign of moving forward. \n## Why is this happening?\nGood question, and after thinking about this for a long, long time I think I have the answer.  It really depends on which aspect of the IT spectrum is controlling the development side.  I think it has become quite acceptable to break up the whole IT space into two major groups, the **IT Professionals**, and the **Software Developers**.  When I first moved to California I worked for a company that was a software developer and they did software development for their clients on a time and materials basis.  There was no question as to which wing of IT influenced the company with regards to technology and hardware.  The developers in this case were golden, if you needed special tools, you got them.  Need a faster computer, more monitors, special machines to run beta versions of the latest OS and development tools, you got it.  You were on the bleeding edge and the IT Professionals were there to help you slow down the bleeding when that go out of control.  However, this company was always current got the best jobs and in a lot of cases when we deployed our final product to their production systems that would be the point at which their IT department would then be forced to update their systems and move to the new round of technology.\n## Welcome to the Other Side\nWhat happens when the influence is on the other foot, the IT Professionals.  They have a different agenda as their number one goal is stability, security, and easy deployment.  However this does come with a cost, especially when the company is heavily relying on technology to push its products.  I have heard this from many different companies all with in this same situation, that they are not a technology company, the technology is just the tool or one of the tools to deliver their products.  When this group controls the budget and the overall technical agenda things like this will happen.  Moving forward will be very, very slow and the focus will be purely on deployment issues and keeping those costs under control and not on the cost of development which could get very expensive as the technology changes and you are not able to take advantage of those opportunities.  Over time, the customers that receive your products will start to evaluate your future as not being able to move fast enough for them because they are going to expect you to be out there and fully tested these waters before they move there and if your not it is not going to look favorable in their eyes.  This is especially true if you have some completion in your space that are adapting the new technologies faster then your company is. \n\nThere is another side to this that I have witnessed which bothers me even more.  The decision to move all enterprise applications to the web was never from the development side of IT but came from the IT Professionals.  Remember one of their big agendas is the easy, easy deployment and as a result they have made software development so expensive that we have been forced to move as much as we can to off shore development companies.  In most cases this hasn't even come close to a cost savings for the applications as you never seem to get what you thought your were designing and it is not always the fault of the off shore companies, they are giving your exactly what you asked for.  In more cases it is the wrong technology for the solution.  Most high volume enterprise applications were desktop applications with a lot of state (data that you are working with).  The web is stateless and over the years many things have been developed to make the web appear state full but is it not.  I have seen projects spend 100 times more time and money into implementing a features on a web to make it appear and feel like a desktop application.  Now to be clear this argument started when deployment of desktop applications was hard as in the early days there was no easy way to keep all the desktops up to date except to physically go around and update them as patches and newer versions became available.  However, in the last 5 years or more that has totally changed with things like click-once technology you can implement full automatic updates and license enforcement just as easily as web sites and maybe even better.  We all know there are new tricks every day to spoof a web site into some new security trick. \n## What's the Answer\nI don't really have the answer but I do have a few thoughts that I have been thinking about and I would love to hear about some other ideas that you might have.  My thought is that you should separate the IT push down two paths and this advice is for the companies that are currently being held back by the stabilizing IT Professionals.  I would even go so far as to keep the developers on a separate network then the rest of the employees this will keep the bleeding on the other side of the fence and not affect your sales and support staff which are there to sell and support products that are stable and need to keep them that way.  This will allow the development to improve and expand their technical expertise and provide better and more relevant solutions for your customers, internal and external.  ","slug":"The-Two-Opposite-IT-Agenda-s","published":1,"updated":"2020-01-05T00:36:05.105Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgga000os4ufdjubhwln","content":"<h2 id=\"The-Problem\"><a href=\"#The-Problem\" class=\"headerlink\" title=\"The Problem\"></a>The Problem</h2><p>I have been in the Information Technology (IT) field for a long time and most of that time has been spent in the development space.  Each environment different from the previous one and in some cases there were huge gaps in the level of technology that was used and made available in each location.  This has stumped me for a long time why this was.  You go to a user group meeting and when ever the speaker was speaking about a technology that was current and he would conduct a quick survey around the room how many were using this technology, the results would be very mixed.  There would even be lots of users at these meetings where they were still using technologies that were over 10 years old and no sign of moving forward. </p>\n<h2 id=\"Why-is-this-happening\"><a href=\"#Why-is-this-happening\" class=\"headerlink\" title=\"Why is this happening?\"></a>Why is this happening?</h2><p>Good question, and after thinking about this for a long, long time I think I have the answer.  It really depends on which aspect of the IT spectrum is controlling the development side.  I think it has become quite acceptable to break up the whole IT space into two major groups, the <strong>IT Professionals</strong>, and the <strong>Software Developers</strong>.  When I first moved to California I worked for a company that was a software developer and they did software development for their clients on a time and materials basis.  There was no question as to which wing of IT influenced the company with regards to technology and hardware.  The developers in this case were golden, if you needed special tools, you got them.  Need a faster computer, more monitors, special machines to run beta versions of the latest OS and development tools, you got it.  You were on the bleeding edge and the IT Professionals were there to help you slow down the bleeding when that go out of control.  However, this company was always current got the best jobs and in a lot of cases when we deployed our final product to their production systems that would be the point at which their IT department would then be forced to update their systems and move to the new round of technology.</p>\n<h2 id=\"Welcome-to-the-Other-Side\"><a href=\"#Welcome-to-the-Other-Side\" class=\"headerlink\" title=\"Welcome to the Other Side\"></a>Welcome to the Other Side</h2><p>What happens when the influence is on the other foot, the IT Professionals.  They have a different agenda as their number one goal is stability, security, and easy deployment.  However this does come with a cost, especially when the company is heavily relying on technology to push its products.  I have heard this from many different companies all with in this same situation, that they are not a technology company, the technology is just the tool or one of the tools to deliver their products.  When this group controls the budget and the overall technical agenda things like this will happen.  Moving forward will be very, very slow and the focus will be purely on deployment issues and keeping those costs under control and not on the cost of development which could get very expensive as the technology changes and you are not able to take advantage of those opportunities.  Over time, the customers that receive your products will start to evaluate your future as not being able to move fast enough for them because they are going to expect you to be out there and fully tested these waters before they move there and if your not it is not going to look favorable in their eyes.  This is especially true if you have some completion in your space that are adapting the new technologies faster then your company is. </p>\n<p>There is another side to this that I have witnessed which bothers me even more.  The decision to move all enterprise applications to the web was never from the development side of IT but came from the IT Professionals.  Remember one of their big agendas is the easy, easy deployment and as a result they have made software development so expensive that we have been forced to move as much as we can to off shore development companies.  In most cases this hasn’t even come close to a cost savings for the applications as you never seem to get what you thought your were designing and it is not always the fault of the off shore companies, they are giving your exactly what you asked for.  In more cases it is the wrong technology for the solution.  Most high volume enterprise applications were desktop applications with a lot of state (data that you are working with).  The web is stateless and over the years many things have been developed to make the web appear state full but is it not.  I have seen projects spend 100 times more time and money into implementing a features on a web to make it appear and feel like a desktop application.  Now to be clear this argument started when deployment of desktop applications was hard as in the early days there was no easy way to keep all the desktops up to date except to physically go around and update them as patches and newer versions became available.  However, in the last 5 years or more that has totally changed with things like click-once technology you can implement full automatic updates and license enforcement just as easily as web sites and maybe even better.  We all know there are new tricks every day to spoof a web site into some new security trick. </p>\n<h2 id=\"What’s-the-Answer\"><a href=\"#What’s-the-Answer\" class=\"headerlink\" title=\"What’s the Answer\"></a>What’s the Answer</h2><p>I don’t really have the answer but I do have a few thoughts that I have been thinking about and I would love to hear about some other ideas that you might have.  My thought is that you should separate the IT push down two paths and this advice is for the companies that are currently being held back by the stabilizing IT Professionals.  I would even go so far as to keep the developers on a separate network then the rest of the employees this will keep the bleeding on the other side of the fence and not affect your sales and support staff which are there to sell and support products that are stable and need to keep them that way.  This will allow the development to improve and expand their technical expertise and provide better and more relevant solutions for your customers, internal and external.  </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"The-Problem\"><a href=\"#The-Problem\" class=\"headerlink\" title=\"The Problem\"></a>The Problem</h2><p>I have been in the Information Technology (IT) field for a long time and most of that time has been spent in the development space.  Each environment different from the previous one and in some cases there were huge gaps in the level of technology that was used and made available in each location.  This has stumped me for a long time why this was.  You go to a user group meeting and when ever the speaker was speaking about a technology that was current and he would conduct a quick survey around the room how many were using this technology, the results would be very mixed.  There would even be lots of users at these meetings where they were still using technologies that were over 10 years old and no sign of moving forward. </p>\n<h2 id=\"Why-is-this-happening\"><a href=\"#Why-is-this-happening\" class=\"headerlink\" title=\"Why is this happening?\"></a>Why is this happening?</h2><p>Good question, and after thinking about this for a long, long time I think I have the answer.  It really depends on which aspect of the IT spectrum is controlling the development side.  I think it has become quite acceptable to break up the whole IT space into two major groups, the <strong>IT Professionals</strong>, and the <strong>Software Developers</strong>.  When I first moved to California I worked for a company that was a software developer and they did software development for their clients on a time and materials basis.  There was no question as to which wing of IT influenced the company with regards to technology and hardware.  The developers in this case were golden, if you needed special tools, you got them.  Need a faster computer, more monitors, special machines to run beta versions of the latest OS and development tools, you got it.  You were on the bleeding edge and the IT Professionals were there to help you slow down the bleeding when that go out of control.  However, this company was always current got the best jobs and in a lot of cases when we deployed our final product to their production systems that would be the point at which their IT department would then be forced to update their systems and move to the new round of technology.</p>\n<h2 id=\"Welcome-to-the-Other-Side\"><a href=\"#Welcome-to-the-Other-Side\" class=\"headerlink\" title=\"Welcome to the Other Side\"></a>Welcome to the Other Side</h2><p>What happens when the influence is on the other foot, the IT Professionals.  They have a different agenda as their number one goal is stability, security, and easy deployment.  However this does come with a cost, especially when the company is heavily relying on technology to push its products.  I have heard this from many different companies all with in this same situation, that they are not a technology company, the technology is just the tool or one of the tools to deliver their products.  When this group controls the budget and the overall technical agenda things like this will happen.  Moving forward will be very, very slow and the focus will be purely on deployment issues and keeping those costs under control and not on the cost of development which could get very expensive as the technology changes and you are not able to take advantage of those opportunities.  Over time, the customers that receive your products will start to evaluate your future as not being able to move fast enough for them because they are going to expect you to be out there and fully tested these waters before they move there and if your not it is not going to look favorable in their eyes.  This is especially true if you have some completion in your space that are adapting the new technologies faster then your company is. </p>\n<p>There is another side to this that I have witnessed which bothers me even more.  The decision to move all enterprise applications to the web was never from the development side of IT but came from the IT Professionals.  Remember one of their big agendas is the easy, easy deployment and as a result they have made software development so expensive that we have been forced to move as much as we can to off shore development companies.  In most cases this hasn’t even come close to a cost savings for the applications as you never seem to get what you thought your were designing and it is not always the fault of the off shore companies, they are giving your exactly what you asked for.  In more cases it is the wrong technology for the solution.  Most high volume enterprise applications were desktop applications with a lot of state (data that you are working with).  The web is stateless and over the years many things have been developed to make the web appear state full but is it not.  I have seen projects spend 100 times more time and money into implementing a features on a web to make it appear and feel like a desktop application.  Now to be clear this argument started when deployment of desktop applications was hard as in the early days there was no easy way to keep all the desktops up to date except to physically go around and update them as patches and newer versions became available.  However, in the last 5 years or more that has totally changed with things like click-once technology you can implement full automatic updates and license enforcement just as easily as web sites and maybe even better.  We all know there are new tricks every day to spoof a web site into some new security trick. </p>\n<h2 id=\"What’s-the-Answer\"><a href=\"#What’s-the-Answer\" class=\"headerlink\" title=\"What’s the Answer\"></a>What’s the Answer</h2><p>I don’t really have the answer but I do have a few thoughts that I have been thinking about and I would love to hear about some other ideas that you might have.  My thought is that you should separate the IT push down two paths and this advice is for the companies that are currently being held back by the stabilizing IT Professionals.  I would even go so far as to keep the developers on a separate network then the rest of the employees this will keep the bleeding on the other side of the fence and not affect your sales and support staff which are there to sell and support products that are stable and need to keep them that way.  This will allow the development to improve and expand their technical expertise and provide better and more relevant solutions for your customers, internal and external.  </p>\n"},{"title":"When is Waterfall a Good Choice","date":"2017-04-19T14:45:58.000Z","_content":"{% img left /images/waterfall.jpg 110 110 \"Waterfall\" %}\nIn my work as an ALM consultant I will often be asked the question or told that a team can't go and practice agile they have to do waterfall.  I think they are looking at this in the wrong way.  One of the things to think about in waterfall versus agile is what these two methodologies are really all about. Is waterfall really all that bad?  The answer to that question is: No, waterfall is actually a great methodology and a great pattern that has worked for some projects. Not a lot in the software field; simply because in Waterfall you need to know all the requirements up front and work towards completing that plan.  In other words you are working the plan and the schedule becomes king not the actual priority or benefit that you can provide to the end user.\n\n### Inspection and Adaption\nIn an agile setting, we know up front that we will not know everything that there is to know about this solution that we are designing and coding until we start.  We recognize and acknowledge that as we start to develop and get early feedback that we may have to go back to the work that we have done and make changes.  This is part of the Inspection and Adaption that goes on in Agile regularly.  This is totally missing in Waterfall.  {% img right /images/attack.jpg 200 200 \"Do not attack me, yet\" %} Before you start attaching me with your arrows and spears, yes I know you can make changes in Waterfall, heaven knows how many times I have heard the excuses that projects were not completed on time or at all because the scope kept on changing.  However, lets explore that thought process for a minute.  Lets go through the steps that it takes to make a change in a Waterfall project.  \n\nFirst off someone has to invoke a change request in order to make that change.  This is likely coming from the development team as they ran into a road block and would not be able to complete the project by the way the requirements were written.  This change request would almost never be coming from the end user because they won't likely be able to provide any feedback until the application has moved into testing, which is always done near the end of the project.  Next there has to be an impact analysis on how this change may affect the rest of the project.  What I find interesting here is that we are still bound to theories.  The requirements were developed on theories of how things should work in the minds of a Business Analysis and now we have an impact analysis which is also based on the same air, how we think it should work.  One of the things about any agile project is that it is based on living breathing code.  If something isn't working the way we need it to we can make changes and continue to get feedback until everyone is happy.\n\n### Big Design Up Front\nWith waterfall, things need to happen in a very precise set of steps.  After the requirements are gathered and everyone is in agreement on what should go into this application, it goes to the architects who will come up with the design.  Many of the choices that are made during this step are made based on the specific known aspects of the requirements documentation.  The problem here is that the organization may not know if these are all the requirements and there is a high level of possibilities that they aren't. {% img left /images/unicorn.jpg 100 100 \"Unicorn Moment\" %}  There is a myth that the Scrum community states in their training material.  The myth is that the longer you spend studying and analyzing the problem the more precise your solution will be.\n\nThe worst part of \"Big Design Up Front\" is that there might have been weeks and maybe even months of work to create this design.  Which might be okay if the project was to come together in exactly that way.  Chances are, there is going to be a change request that comes along that is going to break the design, sending the architects back to the drawing board.  In any agile environment, we expect that the application and the design will probably change many times as we continue to inspect and adapt during development.  The big difference here is that agile does not spend a lot of time up front on the design but continue to design and redesign as the project moves forward.\n\n### But We Need those Big Requirement Documents\n{% img right /images/stackofpapers.jpg 300 300 \"Papers I will never read\" %}\nOh really?  I have challenged many of my clients to prove me wrong on this.  No one likes to read these because of the amount of boiler plate material that is in the document.  Way back when I was leading teams on a new project I would often get these 60 to 70 page documents.  I would go through them with my highlighter and find about a page and a half of things that we needed to do, the rest was filler.  I am not alone in this thinking as I have seen lots of teams doing something similar to my highlighter exercise except they may put them into tools similar to Team Foundation Server.  These teams would love to have the BA's enter the requirements directly into TFS, but they struggle with having to have these big documents.\n\nAgain I ask you who are you writing these documents for?  Many hours are spent to put these documents together to only end up in an archive somewhere.  Developers don't want them, they want the actual requirements or stories pulled out and track the things that they need to build.  Testers tend to follow closer to what is going on in development than the big document simply because there is so much boiler plate stuff in these documents that makes it hard to work with.  Now, you might be getting upset with me again because there is important stuff in that boiler plate text.  True, but I don't think it belongs in this document which is just a document.  The problem with a document is that unless it has some way of being enforced it is just ink on a piece of paper.  Everyone who has spent any time with your company knows what these requirements are that have to be in every product so wouldn't it make more sense to have them in a Regression Test that gets run at least once before we release into Production.  How about the really important ones being in a series of smoke tests which must get run before the Testers are even going to look at that build.  This way those boiler plate requirements will be enforced.\n\n### Conclusion\nOkay, I will admit I used the title of this blog post to attract development teams that are determined to work in a waterfall methodology and you probably thought this post would give you some much needed ammunition to fire back at the agile folks.  Waterfall works great if everything is known about the project and you have done this same kind of thing many, many times.  In those circumstances it is going to work great as you will know exactly how long it is going to take you and when the end user can have it in their hands.  However, I have to say that very little of that kind of development is done in the United States or Canada.  Those are the kinds of projects that can easily be done off shore for a lot less money because it then becomes just a labor exercise.{% img left /images/startreck.jpg 375 375 \"Go where no one has gone before\" %}  Real development involves going where no one has ever gone before and doing things you were not even sure could be possible.  That is why you need to adhere to an agile approach and take a couple of steps forward, and expect to take a step back, adjust and move forward again.  Development involves trying things and retrying things until you get the results that the end users are expecting.","source":"_posts/When-is-Waterfall-a-Good-Choice.md","raw":"---\ntitle: When is Waterfall a Good Choice\ndate: 2017-04-19 07:45:58\ntags:\n- ALM\n- Compare\n---\n{% img left /images/waterfall.jpg 110 110 \"Waterfall\" %}\nIn my work as an ALM consultant I will often be asked the question or told that a team can't go and practice agile they have to do waterfall.  I think they are looking at this in the wrong way.  One of the things to think about in waterfall versus agile is what these two methodologies are really all about. Is waterfall really all that bad?  The answer to that question is: No, waterfall is actually a great methodology and a great pattern that has worked for some projects. Not a lot in the software field; simply because in Waterfall you need to know all the requirements up front and work towards completing that plan.  In other words you are working the plan and the schedule becomes king not the actual priority or benefit that you can provide to the end user.\n\n### Inspection and Adaption\nIn an agile setting, we know up front that we will not know everything that there is to know about this solution that we are designing and coding until we start.  We recognize and acknowledge that as we start to develop and get early feedback that we may have to go back to the work that we have done and make changes.  This is part of the Inspection and Adaption that goes on in Agile regularly.  This is totally missing in Waterfall.  {% img right /images/attack.jpg 200 200 \"Do not attack me, yet\" %} Before you start attaching me with your arrows and spears, yes I know you can make changes in Waterfall, heaven knows how many times I have heard the excuses that projects were not completed on time or at all because the scope kept on changing.  However, lets explore that thought process for a minute.  Lets go through the steps that it takes to make a change in a Waterfall project.  \n\nFirst off someone has to invoke a change request in order to make that change.  This is likely coming from the development team as they ran into a road block and would not be able to complete the project by the way the requirements were written.  This change request would almost never be coming from the end user because they won't likely be able to provide any feedback until the application has moved into testing, which is always done near the end of the project.  Next there has to be an impact analysis on how this change may affect the rest of the project.  What I find interesting here is that we are still bound to theories.  The requirements were developed on theories of how things should work in the minds of a Business Analysis and now we have an impact analysis which is also based on the same air, how we think it should work.  One of the things about any agile project is that it is based on living breathing code.  If something isn't working the way we need it to we can make changes and continue to get feedback until everyone is happy.\n\n### Big Design Up Front\nWith waterfall, things need to happen in a very precise set of steps.  After the requirements are gathered and everyone is in agreement on what should go into this application, it goes to the architects who will come up with the design.  Many of the choices that are made during this step are made based on the specific known aspects of the requirements documentation.  The problem here is that the organization may not know if these are all the requirements and there is a high level of possibilities that they aren't. {% img left /images/unicorn.jpg 100 100 \"Unicorn Moment\" %}  There is a myth that the Scrum community states in their training material.  The myth is that the longer you spend studying and analyzing the problem the more precise your solution will be.\n\nThe worst part of \"Big Design Up Front\" is that there might have been weeks and maybe even months of work to create this design.  Which might be okay if the project was to come together in exactly that way.  Chances are, there is going to be a change request that comes along that is going to break the design, sending the architects back to the drawing board.  In any agile environment, we expect that the application and the design will probably change many times as we continue to inspect and adapt during development.  The big difference here is that agile does not spend a lot of time up front on the design but continue to design and redesign as the project moves forward.\n\n### But We Need those Big Requirement Documents\n{% img right /images/stackofpapers.jpg 300 300 \"Papers I will never read\" %}\nOh really?  I have challenged many of my clients to prove me wrong on this.  No one likes to read these because of the amount of boiler plate material that is in the document.  Way back when I was leading teams on a new project I would often get these 60 to 70 page documents.  I would go through them with my highlighter and find about a page and a half of things that we needed to do, the rest was filler.  I am not alone in this thinking as I have seen lots of teams doing something similar to my highlighter exercise except they may put them into tools similar to Team Foundation Server.  These teams would love to have the BA's enter the requirements directly into TFS, but they struggle with having to have these big documents.\n\nAgain I ask you who are you writing these documents for?  Many hours are spent to put these documents together to only end up in an archive somewhere.  Developers don't want them, they want the actual requirements or stories pulled out and track the things that they need to build.  Testers tend to follow closer to what is going on in development than the big document simply because there is so much boiler plate stuff in these documents that makes it hard to work with.  Now, you might be getting upset with me again because there is important stuff in that boiler plate text.  True, but I don't think it belongs in this document which is just a document.  The problem with a document is that unless it has some way of being enforced it is just ink on a piece of paper.  Everyone who has spent any time with your company knows what these requirements are that have to be in every product so wouldn't it make more sense to have them in a Regression Test that gets run at least once before we release into Production.  How about the really important ones being in a series of smoke tests which must get run before the Testers are even going to look at that build.  This way those boiler plate requirements will be enforced.\n\n### Conclusion\nOkay, I will admit I used the title of this blog post to attract development teams that are determined to work in a waterfall methodology and you probably thought this post would give you some much needed ammunition to fire back at the agile folks.  Waterfall works great if everything is known about the project and you have done this same kind of thing many, many times.  In those circumstances it is going to work great as you will know exactly how long it is going to take you and when the end user can have it in their hands.  However, I have to say that very little of that kind of development is done in the United States or Canada.  Those are the kinds of projects that can easily be done off shore for a lot less money because it then becomes just a labor exercise.{% img left /images/startreck.jpg 375 375 \"Go where no one has gone before\" %}  Real development involves going where no one has ever gone before and doing things you were not even sure could be possible.  That is why you need to adhere to an agile approach and take a couple of steps forward, and expect to take a step back, adjust and move forward again.  Development involves trying things and retrying things until you get the results that the end users are expecting.","slug":"When-is-Waterfall-a-Good-Choice","published":1,"updated":"2020-01-05T00:36:05.107Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqggb000ps4ufum1ti0mg","content":"<img src=\"/images/waterfall.jpg\" class=\"left\" width=\"110\" height=\"110\" title=\"Waterfall\">\n<p>In my work as an ALM consultant I will often be asked the question or told that a team can’t go and practice agile they have to do waterfall.  I think they are looking at this in the wrong way.  One of the things to think about in waterfall versus agile is what these two methodologies are really all about. Is waterfall really all that bad?  The answer to that question is: No, waterfall is actually a great methodology and a great pattern that has worked for some projects. Not a lot in the software field; simply because in Waterfall you need to know all the requirements up front and work towards completing that plan.  In other words you are working the plan and the schedule becomes king not the actual priority or benefit that you can provide to the end user.</p>\n<h3 id=\"Inspection-and-Adaption\"><a href=\"#Inspection-and-Adaption\" class=\"headerlink\" title=\"Inspection and Adaption\"></a>Inspection and Adaption</h3><p>In an agile setting, we know up front that we will not know everything that there is to know about this solution that we are designing and coding until we start.  We recognize and acknowledge that as we start to develop and get early feedback that we may have to go back to the work that we have done and make changes.  This is part of the Inspection and Adaption that goes on in Agile regularly.  This is totally missing in Waterfall.  <img src=\"/images/attack.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Do not attack me, yet\"> Before you start attaching me with your arrows and spears, yes I know you can make changes in Waterfall, heaven knows how many times I have heard the excuses that projects were not completed on time or at all because the scope kept on changing.  However, lets explore that thought process for a minute.  Lets go through the steps that it takes to make a change in a Waterfall project.  </p>\n<p>First off someone has to invoke a change request in order to make that change.  This is likely coming from the development team as they ran into a road block and would not be able to complete the project by the way the requirements were written.  This change request would almost never be coming from the end user because they won’t likely be able to provide any feedback until the application has moved into testing, which is always done near the end of the project.  Next there has to be an impact analysis on how this change may affect the rest of the project.  What I find interesting here is that we are still bound to theories.  The requirements were developed on theories of how things should work in the minds of a Business Analysis and now we have an impact analysis which is also based on the same air, how we think it should work.  One of the things about any agile project is that it is based on living breathing code.  If something isn’t working the way we need it to we can make changes and continue to get feedback until everyone is happy.</p>\n<h3 id=\"Big-Design-Up-Front\"><a href=\"#Big-Design-Up-Front\" class=\"headerlink\" title=\"Big Design Up Front\"></a>Big Design Up Front</h3><p>With waterfall, things need to happen in a very precise set of steps.  After the requirements are gathered and everyone is in agreement on what should go into this application, it goes to the architects who will come up with the design.  Many of the choices that are made during this step are made based on the specific known aspects of the requirements documentation.  The problem here is that the organization may not know if these are all the requirements and there is a high level of possibilities that they aren’t. <img src=\"/images/unicorn.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"Unicorn Moment\">  There is a myth that the Scrum community states in their training material.  The myth is that the longer you spend studying and analyzing the problem the more precise your solution will be.</p>\n<p>The worst part of “Big Design Up Front” is that there might have been weeks and maybe even months of work to create this design.  Which might be okay if the project was to come together in exactly that way.  Chances are, there is going to be a change request that comes along that is going to break the design, sending the architects back to the drawing board.  In any agile environment, we expect that the application and the design will probably change many times as we continue to inspect and adapt during development.  The big difference here is that agile does not spend a lot of time up front on the design but continue to design and redesign as the project moves forward.</p>\n<h3 id=\"But-We-Need-those-Big-Requirement-Documents\"><a href=\"#But-We-Need-those-Big-Requirement-Documents\" class=\"headerlink\" title=\"But We Need those Big Requirement Documents\"></a>But We Need those Big Requirement Documents</h3><img src=\"/images/stackofpapers.jpg\" class=\"right\" width=\"300\" height=\"300\" title=\"Papers I will never read\">\n<p>Oh really?  I have challenged many of my clients to prove me wrong on this.  No one likes to read these because of the amount of boiler plate material that is in the document.  Way back when I was leading teams on a new project I would often get these 60 to 70 page documents.  I would go through them with my highlighter and find about a page and a half of things that we needed to do, the rest was filler.  I am not alone in this thinking as I have seen lots of teams doing something similar to my highlighter exercise except they may put them into tools similar to Team Foundation Server.  These teams would love to have the BA’s enter the requirements directly into TFS, but they struggle with having to have these big documents.</p>\n<p>Again I ask you who are you writing these documents for?  Many hours are spent to put these documents together to only end up in an archive somewhere.  Developers don’t want them, they want the actual requirements or stories pulled out and track the things that they need to build.  Testers tend to follow closer to what is going on in development than the big document simply because there is so much boiler plate stuff in these documents that makes it hard to work with.  Now, you might be getting upset with me again because there is important stuff in that boiler plate text.  True, but I don’t think it belongs in this document which is just a document.  The problem with a document is that unless it has some way of being enforced it is just ink on a piece of paper.  Everyone who has spent any time with your company knows what these requirements are that have to be in every product so wouldn’t it make more sense to have them in a Regression Test that gets run at least once before we release into Production.  How about the really important ones being in a series of smoke tests which must get run before the Testers are even going to look at that build.  This way those boiler plate requirements will be enforced.</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Okay, I will admit I used the title of this blog post to attract development teams that are determined to work in a waterfall methodology and you probably thought this post would give you some much needed ammunition to fire back at the agile folks.  Waterfall works great if everything is known about the project and you have done this same kind of thing many, many times.  In those circumstances it is going to work great as you will know exactly how long it is going to take you and when the end user can have it in their hands.  However, I have to say that very little of that kind of development is done in the United States or Canada.  Those are the kinds of projects that can easily be done off shore for a lot less money because it then becomes just a labor exercise.<img src=\"/images/startreck.jpg\" class=\"left\" width=\"375\" height=\"375\" title=\"Go where no one has gone before\">  Real development involves going where no one has ever gone before and doing things you were not even sure could be possible.  That is why you need to adhere to an agile approach and take a couple of steps forward, and expect to take a step back, adjust and move forward again.  Development involves trying things and retrying things until you get the results that the end users are expecting.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/waterfall.jpg\" class=\"left\" width=\"110\" height=\"110\" title=\"Waterfall\">\n<p>In my work as an ALM consultant I will often be asked the question or told that a team can’t go and practice agile they have to do waterfall.  I think they are looking at this in the wrong way.  One of the things to think about in waterfall versus agile is what these two methodologies are really all about. Is waterfall really all that bad?  The answer to that question is: No, waterfall is actually a great methodology and a great pattern that has worked for some projects. Not a lot in the software field; simply because in Waterfall you need to know all the requirements up front and work towards completing that plan.  In other words you are working the plan and the schedule becomes king not the actual priority or benefit that you can provide to the end user.</p>\n<h3 id=\"Inspection-and-Adaption\"><a href=\"#Inspection-and-Adaption\" class=\"headerlink\" title=\"Inspection and Adaption\"></a>Inspection and Adaption</h3><p>In an agile setting, we know up front that we will not know everything that there is to know about this solution that we are designing and coding until we start.  We recognize and acknowledge that as we start to develop and get early feedback that we may have to go back to the work that we have done and make changes.  This is part of the Inspection and Adaption that goes on in Agile regularly.  This is totally missing in Waterfall.  <img src=\"/images/attack.jpg\" class=\"right\" width=\"200\" height=\"200\" title=\"Do not attack me, yet\"> Before you start attaching me with your arrows and spears, yes I know you can make changes in Waterfall, heaven knows how many times I have heard the excuses that projects were not completed on time or at all because the scope kept on changing.  However, lets explore that thought process for a minute.  Lets go through the steps that it takes to make a change in a Waterfall project.  </p>\n<p>First off someone has to invoke a change request in order to make that change.  This is likely coming from the development team as they ran into a road block and would not be able to complete the project by the way the requirements were written.  This change request would almost never be coming from the end user because they won’t likely be able to provide any feedback until the application has moved into testing, which is always done near the end of the project.  Next there has to be an impact analysis on how this change may affect the rest of the project.  What I find interesting here is that we are still bound to theories.  The requirements were developed on theories of how things should work in the minds of a Business Analysis and now we have an impact analysis which is also based on the same air, how we think it should work.  One of the things about any agile project is that it is based on living breathing code.  If something isn’t working the way we need it to we can make changes and continue to get feedback until everyone is happy.</p>\n<h3 id=\"Big-Design-Up-Front\"><a href=\"#Big-Design-Up-Front\" class=\"headerlink\" title=\"Big Design Up Front\"></a>Big Design Up Front</h3><p>With waterfall, things need to happen in a very precise set of steps.  After the requirements are gathered and everyone is in agreement on what should go into this application, it goes to the architects who will come up with the design.  Many of the choices that are made during this step are made based on the specific known aspects of the requirements documentation.  The problem here is that the organization may not know if these are all the requirements and there is a high level of possibilities that they aren’t. <img src=\"/images/unicorn.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"Unicorn Moment\">  There is a myth that the Scrum community states in their training material.  The myth is that the longer you spend studying and analyzing the problem the more precise your solution will be.</p>\n<p>The worst part of “Big Design Up Front” is that there might have been weeks and maybe even months of work to create this design.  Which might be okay if the project was to come together in exactly that way.  Chances are, there is going to be a change request that comes along that is going to break the design, sending the architects back to the drawing board.  In any agile environment, we expect that the application and the design will probably change many times as we continue to inspect and adapt during development.  The big difference here is that agile does not spend a lot of time up front on the design but continue to design and redesign as the project moves forward.</p>\n<h3 id=\"But-We-Need-those-Big-Requirement-Documents\"><a href=\"#But-We-Need-those-Big-Requirement-Documents\" class=\"headerlink\" title=\"But We Need those Big Requirement Documents\"></a>But We Need those Big Requirement Documents</h3><img src=\"/images/stackofpapers.jpg\" class=\"right\" width=\"300\" height=\"300\" title=\"Papers I will never read\">\n<p>Oh really?  I have challenged many of my clients to prove me wrong on this.  No one likes to read these because of the amount of boiler plate material that is in the document.  Way back when I was leading teams on a new project I would often get these 60 to 70 page documents.  I would go through them with my highlighter and find about a page and a half of things that we needed to do, the rest was filler.  I am not alone in this thinking as I have seen lots of teams doing something similar to my highlighter exercise except they may put them into tools similar to Team Foundation Server.  These teams would love to have the BA’s enter the requirements directly into TFS, but they struggle with having to have these big documents.</p>\n<p>Again I ask you who are you writing these documents for?  Many hours are spent to put these documents together to only end up in an archive somewhere.  Developers don’t want them, they want the actual requirements or stories pulled out and track the things that they need to build.  Testers tend to follow closer to what is going on in development than the big document simply because there is so much boiler plate stuff in these documents that makes it hard to work with.  Now, you might be getting upset with me again because there is important stuff in that boiler plate text.  True, but I don’t think it belongs in this document which is just a document.  The problem with a document is that unless it has some way of being enforced it is just ink on a piece of paper.  Everyone who has spent any time with your company knows what these requirements are that have to be in every product so wouldn’t it make more sense to have them in a Regression Test that gets run at least once before we release into Production.  How about the really important ones being in a series of smoke tests which must get run before the Testers are even going to look at that build.  This way those boiler plate requirements will be enforced.</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Okay, I will admit I used the title of this blog post to attract development teams that are determined to work in a waterfall methodology and you probably thought this post would give you some much needed ammunition to fire back at the agile folks.  Waterfall works great if everything is known about the project and you have done this same kind of thing many, many times.  In those circumstances it is going to work great as you will know exactly how long it is going to take you and when the end user can have it in their hands.  However, I have to say that very little of that kind of development is done in the United States or Canada.  Those are the kinds of projects that can easily be done off shore for a lot less money because it then becomes just a labor exercise.<img src=\"/images/startreck.jpg\" class=\"left\" width=\"375\" height=\"375\" title=\"Go where no one has gone before\">  Real development involves going where no one has ever gone before and doing things you were not even sure could be possible.  That is why you need to adhere to an agile approach and take a couple of steps forward, and expect to take a step back, adjust and move forward again.  Development involves trying things and retrying things until you get the results that the end users are expecting.</p>\n"},{"title":"What is The Web We Weave, Inc?","date":"2008-06-14T07:00:00.000Z","_content":"A little more than a year ago, Mary and I had a discussion about the many projects that we both have had in the backs of our minds and would like to make a reality.  We thought a corporation would be good in that it could provide us with the legal entity and a single structure in which we could register our copyrights and trademarks.  It might also provide us with some tax relief and if things went well, could very well represent a major part of our future. \n\nWell, these things are all well and good until you find that our vast array of projects are just that, vast, varied and hard to find a simple way to describe what our company “The Web We Weave, Inc.” is all about.  I guess the best way to present this is to go through our current list, and how we came about these as they all lead to a very interesting story, at least to Mary and me. \n\n### Fuel Consumption\nI have been interested in fuel consumption and fuel consumption tracking since about as far back as 1990 or 1991.  Somewhere around this time, I was spending a lot of my time traveling between Canada and the USA.  I would often travel with my laptop and liked to keep an eye on my fuel consumption.  There were a few calculator type programs that could do this when in Canada and we used them at the family-owned dealership to verify fuel consumption for our customers.  The problem that I had was when I traveled to the US; I had to do all these extra conversions from a US gallon into liters in order for this other program to do the calculation.  I figured that there had to be a better way to do this that would take the various measurements and do the conversions on the fly in the background. \n\nAs a result of this, I built an application that I later distributed as shareware called “win-Fuel” and it was written as a desktop application in Visual Basic 1.0.  It did exactly what I had in mind.  Radio buttons that switched between miles and kilometers and a second set of radio buttons to switch between Liters, Imperial Gallons, and US Gallons.  The display of the results showed the consumption calculations in four formats; Miles Per US Gallons, Miles Per Imperial Gallons, Miles Per Liter, and the number of Miles per 100 Liters (the official metric calculation).  I did have some success with the application but more importantly to me than the success was that it provided me with the experience of building a simple application and taking it through all the stages; from concept, development, to distribution.  I also built a context sensitive “help” and a “setup” program to complete the project. \n\nI had always planned to go back to this little application and add the ability to store the information in a database of some sort and use that data to compare past trips with the current calculation.  However, that never did happen.  But now with the availability of the Internet and the ease at which what was once deemed a desktop application can now be transported into a web application and provide an even larger data source, this is a possibility.  It is my belief that the collected data could be quite valuable to governments, environmental groups, car manufactures as well as individuals.  Up to the present, fuel consumption has always been measured under laboratory conditions and real data has never been taken into consideration. \n\nI see this site as a free service to the users of the application which will allow me to be able to provide a market with the collective value of the data.  I also see this as a central place where fuel consumptions can be discussed in the form of forums and discussions, as well as, articles on fuel consumption such as tips on getting better gas mileage and regular vs. premium grade gasoline. \n\n### Stock Market Analysis\nMary has always had an interest in the Stock Market and has been very good at doing the research and analysis necessary to make good stock market picks.  From this interest she has wanted to be able to share some of this research with others in the rather unique way of only looking at and grading stocks that can be bought directly from companies, the official term is DRIPS (Direct Reimbursement Investment Programs). \n\nOne of the many interesting things that I learned about this as Mary has been explaining it to me, is that stock bought through a stock broker is not in your name but instead is being held for you under the brokers’ name.  Buying directly from companies enables you to buy stock in your own name, have a right to vote at shareholders meetings, avoid stock broker fees and a whole lot of other benefits that Mary can tell you about on her new site.  Sounds pretty good, doesn’t it? \n\nAnyway, the model for this site is a choice of either a monthly or yearly subscription basis or on a per use basis.  The idea being that during the valid times that the client has chosen they can go into the site and pick out the various stocks that are graded for their safety and growth potential and provide links to the web sites if available, to the various companies that do indeed sell their stock directly to you.  The data itself would come from a variety of sources and it can be assured that Mary has already validated that all the stocks listed on her site are available as DRIPS. \n\n### Graduation and Awards Program\nThen in the last year that Mary was working in the Activities Department of a High School, she came across an interesting opportunity.  It seems that this High School would type up a number of lists for the Awards Night and the Graduation Ceremony.  One list would be a list of all the Awards, the Presenters and the students that received that award.  This would be used during their Awards Night Presentation.  Then on Graduation there would be a program that listed all the students and the awards that they received during the Awards Night.  Besides this being a lot of repetitive work, it was always easy to have many errors and a lot of time was spent proofreading the lists.  \n\nMary knew there had to be a better solution to this and brought the problem home where we designed an Access Application where Students, Presenters, and Awards were only entered once, with links to each other and the result being two Access Reports that could be either exported into Word for some further formatting adjustments or Printed out right from Access to be given to the Printer to print these two programs.  This proved to be quite successful and the next year, this High School had Mary come back to provide some instruction and make some minor tweaks to the application. \n\nIt did not take long for Mary and I to realize that if this High School had such a large task in front of them when it came to the end of the school year, so would every other High School and Middle School in the country.  However, the Access Application has a bit of a quick and dirty feel to it and would need to be re-engineered into a more commercial product, especially if we were going to support it.  Plans are in motion to build a complete application even though the decisions for the final name have not been reached yet.  Our plan is to build and then market this application starting with all the schools here in Southern California. \n\n### Custom Software Development\nSeveral months ago, Mary was questioning me about the future of our little corporation.  We had the company in place, although we had not opened any bank accounts or anything further then shelling out the legal fees involved in getting ourselves set up.  There was no point in going too fast, as all the projects that we had been talking about so far would cost us money and time to develop and we really were not sitting on top of any real surpluses of either.  Still wanting to do all these projects, we would just go about it more slowly.  There was a need to upgrade our entire network, as the servers and workstations were old and having great difficulty keeping up with the technology that we were using for development and storage. \n\nThen near the end of January 2002, everything changed.  The company that I had been working for over the last 3.5 years closed its doors.  Now I was officially unemployed, and The Web We Weave, Inc. was not able to support us at this time, or so we thought.  The last client that I worked for through my employer was attempting to put a number of the team members on their project back together, offering them short term contracts.  \n\nI was one of the lucky ones, and as it turns out it worked out quite well in that we put the contract in the name of The Web We Weave, Inc. and now we are doing custom software development. \n\n### Beyond Technical\nBesides all of these more or less technical projects, Mary and I have interests in writing.  Mary has a number of ideas for books that she would like to write and I still have a desire to do things with music.  We both would like to write articles for various publications as we both feel we have something to say and would like to share it with the rest of the world. \n\n### What is “The Web We Weave, Inc.”?\nWell, we are back to this question, what is “The Web We Weave, Inc.”?  And you are probably as confused about this as we are.  We made a list of words that we thought were some descriptions of what describes the nature of our company, but still no simple mission statement. \n-Internet \n- web-FUEL \n- Nothing but Direct \n- Education \n- Fuel Consumption \n- Software Development \n- Web Development \n- AGP Maker \n- Research \n- Business Intelligence \n- E-commerce \n- Consulting \n- Organizing \n- Tracking \n- Money Making \n- Profitable \n- Service Provider \n- Hardware \n- Relaxed \n- Confident  \n- Professional \n- Cutting Edge Technology \n\nWe will keep working on it, to find that perfect mission statement and motto that clears up exactly what “The Web We Weave, Inc.” is all about.  It just shows that I was wrong; you need more then just a cool name. \n","source":"_posts/What-is-The-Web-We-Weave-Inc.md","raw":"title: 'What is The Web We Weave, Inc?'\ndate: 2008-06-14\ntags: \n- 3WInc\n- Corporation\n- Products\n---\nA little more than a year ago, Mary and I had a discussion about the many projects that we both have had in the backs of our minds and would like to make a reality.  We thought a corporation would be good in that it could provide us with the legal entity and a single structure in which we could register our copyrights and trademarks.  It might also provide us with some tax relief and if things went well, could very well represent a major part of our future. \n\nWell, these things are all well and good until you find that our vast array of projects are just that, vast, varied and hard to find a simple way to describe what our company “The Web We Weave, Inc.” is all about.  I guess the best way to present this is to go through our current list, and how we came about these as they all lead to a very interesting story, at least to Mary and me. \n\n### Fuel Consumption\nI have been interested in fuel consumption and fuel consumption tracking since about as far back as 1990 or 1991.  Somewhere around this time, I was spending a lot of my time traveling between Canada and the USA.  I would often travel with my laptop and liked to keep an eye on my fuel consumption.  There were a few calculator type programs that could do this when in Canada and we used them at the family-owned dealership to verify fuel consumption for our customers.  The problem that I had was when I traveled to the US; I had to do all these extra conversions from a US gallon into liters in order for this other program to do the calculation.  I figured that there had to be a better way to do this that would take the various measurements and do the conversions on the fly in the background. \n\nAs a result of this, I built an application that I later distributed as shareware called “win-Fuel” and it was written as a desktop application in Visual Basic 1.0.  It did exactly what I had in mind.  Radio buttons that switched between miles and kilometers and a second set of radio buttons to switch between Liters, Imperial Gallons, and US Gallons.  The display of the results showed the consumption calculations in four formats; Miles Per US Gallons, Miles Per Imperial Gallons, Miles Per Liter, and the number of Miles per 100 Liters (the official metric calculation).  I did have some success with the application but more importantly to me than the success was that it provided me with the experience of building a simple application and taking it through all the stages; from concept, development, to distribution.  I also built a context sensitive “help” and a “setup” program to complete the project. \n\nI had always planned to go back to this little application and add the ability to store the information in a database of some sort and use that data to compare past trips with the current calculation.  However, that never did happen.  But now with the availability of the Internet and the ease at which what was once deemed a desktop application can now be transported into a web application and provide an even larger data source, this is a possibility.  It is my belief that the collected data could be quite valuable to governments, environmental groups, car manufactures as well as individuals.  Up to the present, fuel consumption has always been measured under laboratory conditions and real data has never been taken into consideration. \n\nI see this site as a free service to the users of the application which will allow me to be able to provide a market with the collective value of the data.  I also see this as a central place where fuel consumptions can be discussed in the form of forums and discussions, as well as, articles on fuel consumption such as tips on getting better gas mileage and regular vs. premium grade gasoline. \n\n### Stock Market Analysis\nMary has always had an interest in the Stock Market and has been very good at doing the research and analysis necessary to make good stock market picks.  From this interest she has wanted to be able to share some of this research with others in the rather unique way of only looking at and grading stocks that can be bought directly from companies, the official term is DRIPS (Direct Reimbursement Investment Programs). \n\nOne of the many interesting things that I learned about this as Mary has been explaining it to me, is that stock bought through a stock broker is not in your name but instead is being held for you under the brokers’ name.  Buying directly from companies enables you to buy stock in your own name, have a right to vote at shareholders meetings, avoid stock broker fees and a whole lot of other benefits that Mary can tell you about on her new site.  Sounds pretty good, doesn’t it? \n\nAnyway, the model for this site is a choice of either a monthly or yearly subscription basis or on a per use basis.  The idea being that during the valid times that the client has chosen they can go into the site and pick out the various stocks that are graded for their safety and growth potential and provide links to the web sites if available, to the various companies that do indeed sell their stock directly to you.  The data itself would come from a variety of sources and it can be assured that Mary has already validated that all the stocks listed on her site are available as DRIPS. \n\n### Graduation and Awards Program\nThen in the last year that Mary was working in the Activities Department of a High School, she came across an interesting opportunity.  It seems that this High School would type up a number of lists for the Awards Night and the Graduation Ceremony.  One list would be a list of all the Awards, the Presenters and the students that received that award.  This would be used during their Awards Night Presentation.  Then on Graduation there would be a program that listed all the students and the awards that they received during the Awards Night.  Besides this being a lot of repetitive work, it was always easy to have many errors and a lot of time was spent proofreading the lists.  \n\nMary knew there had to be a better solution to this and brought the problem home where we designed an Access Application where Students, Presenters, and Awards were only entered once, with links to each other and the result being two Access Reports that could be either exported into Word for some further formatting adjustments or Printed out right from Access to be given to the Printer to print these two programs.  This proved to be quite successful and the next year, this High School had Mary come back to provide some instruction and make some minor tweaks to the application. \n\nIt did not take long for Mary and I to realize that if this High School had such a large task in front of them when it came to the end of the school year, so would every other High School and Middle School in the country.  However, the Access Application has a bit of a quick and dirty feel to it and would need to be re-engineered into a more commercial product, especially if we were going to support it.  Plans are in motion to build a complete application even though the decisions for the final name have not been reached yet.  Our plan is to build and then market this application starting with all the schools here in Southern California. \n\n### Custom Software Development\nSeveral months ago, Mary was questioning me about the future of our little corporation.  We had the company in place, although we had not opened any bank accounts or anything further then shelling out the legal fees involved in getting ourselves set up.  There was no point in going too fast, as all the projects that we had been talking about so far would cost us money and time to develop and we really were not sitting on top of any real surpluses of either.  Still wanting to do all these projects, we would just go about it more slowly.  There was a need to upgrade our entire network, as the servers and workstations were old and having great difficulty keeping up with the technology that we were using for development and storage. \n\nThen near the end of January 2002, everything changed.  The company that I had been working for over the last 3.5 years closed its doors.  Now I was officially unemployed, and The Web We Weave, Inc. was not able to support us at this time, or so we thought.  The last client that I worked for through my employer was attempting to put a number of the team members on their project back together, offering them short term contracts.  \n\nI was one of the lucky ones, and as it turns out it worked out quite well in that we put the contract in the name of The Web We Weave, Inc. and now we are doing custom software development. \n\n### Beyond Technical\nBesides all of these more or less technical projects, Mary and I have interests in writing.  Mary has a number of ideas for books that she would like to write and I still have a desire to do things with music.  We both would like to write articles for various publications as we both feel we have something to say and would like to share it with the rest of the world. \n\n### What is “The Web We Weave, Inc.”?\nWell, we are back to this question, what is “The Web We Weave, Inc.”?  And you are probably as confused about this as we are.  We made a list of words that we thought were some descriptions of what describes the nature of our company, but still no simple mission statement. \n-Internet \n- web-FUEL \n- Nothing but Direct \n- Education \n- Fuel Consumption \n- Software Development \n- Web Development \n- AGP Maker \n- Research \n- Business Intelligence \n- E-commerce \n- Consulting \n- Organizing \n- Tracking \n- Money Making \n- Profitable \n- Service Provider \n- Hardware \n- Relaxed \n- Confident  \n- Professional \n- Cutting Edge Technology \n\nWe will keep working on it, to find that perfect mission statement and motto that clears up exactly what “The Web We Weave, Inc.” is all about.  It just shows that I was wrong; you need more then just a cool name. \n","slug":"What-is-The-Web-We-Weave-Inc","published":1,"updated":"2020-01-05T00:36:05.106Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqggc000qs4ufy5ymzwcf","content":"<p>A little more than a year ago, Mary and I had a discussion about the many projects that we both have had in the backs of our minds and would like to make a reality.  We thought a corporation would be good in that it could provide us with the legal entity and a single structure in which we could register our copyrights and trademarks.  It might also provide us with some tax relief and if things went well, could very well represent a major part of our future. </p>\n<p>Well, these things are all well and good until you find that our vast array of projects are just that, vast, varied and hard to find a simple way to describe what our company “The Web We Weave, Inc.” is all about.  I guess the best way to present this is to go through our current list, and how we came about these as they all lead to a very interesting story, at least to Mary and me. </p>\n<h3 id=\"Fuel-Consumption\"><a href=\"#Fuel-Consumption\" class=\"headerlink\" title=\"Fuel Consumption\"></a>Fuel Consumption</h3><p>I have been interested in fuel consumption and fuel consumption tracking since about as far back as 1990 or 1991.  Somewhere around this time, I was spending a lot of my time traveling between Canada and the USA.  I would often travel with my laptop and liked to keep an eye on my fuel consumption.  There were a few calculator type programs that could do this when in Canada and we used them at the family-owned dealership to verify fuel consumption for our customers.  The problem that I had was when I traveled to the US; I had to do all these extra conversions from a US gallon into liters in order for this other program to do the calculation.  I figured that there had to be a better way to do this that would take the various measurements and do the conversions on the fly in the background. </p>\n<p>As a result of this, I built an application that I later distributed as shareware called “win-Fuel” and it was written as a desktop application in Visual Basic 1.0.  It did exactly what I had in mind.  Radio buttons that switched between miles and kilometers and a second set of radio buttons to switch between Liters, Imperial Gallons, and US Gallons.  The display of the results showed the consumption calculations in four formats; Miles Per US Gallons, Miles Per Imperial Gallons, Miles Per Liter, and the number of Miles per 100 Liters (the official metric calculation).  I did have some success with the application but more importantly to me than the success was that it provided me with the experience of building a simple application and taking it through all the stages; from concept, development, to distribution.  I also built a context sensitive “help” and a “setup” program to complete the project. </p>\n<p>I had always planned to go back to this little application and add the ability to store the information in a database of some sort and use that data to compare past trips with the current calculation.  However, that never did happen.  But now with the availability of the Internet and the ease at which what was once deemed a desktop application can now be transported into a web application and provide an even larger data source, this is a possibility.  It is my belief that the collected data could be quite valuable to governments, environmental groups, car manufactures as well as individuals.  Up to the present, fuel consumption has always been measured under laboratory conditions and real data has never been taken into consideration. </p>\n<p>I see this site as a free service to the users of the application which will allow me to be able to provide a market with the collective value of the data.  I also see this as a central place where fuel consumptions can be discussed in the form of forums and discussions, as well as, articles on fuel consumption such as tips on getting better gas mileage and regular vs. premium grade gasoline. </p>\n<h3 id=\"Stock-Market-Analysis\"><a href=\"#Stock-Market-Analysis\" class=\"headerlink\" title=\"Stock Market Analysis\"></a>Stock Market Analysis</h3><p>Mary has always had an interest in the Stock Market and has been very good at doing the research and analysis necessary to make good stock market picks.  From this interest she has wanted to be able to share some of this research with others in the rather unique way of only looking at and grading stocks that can be bought directly from companies, the official term is DRIPS (Direct Reimbursement Investment Programs). </p>\n<p>One of the many interesting things that I learned about this as Mary has been explaining it to me, is that stock bought through a stock broker is not in your name but instead is being held for you under the brokers’ name.  Buying directly from companies enables you to buy stock in your own name, have a right to vote at shareholders meetings, avoid stock broker fees and a whole lot of other benefits that Mary can tell you about on her new site.  Sounds pretty good, doesn’t it? </p>\n<p>Anyway, the model for this site is a choice of either a monthly or yearly subscription basis or on a per use basis.  The idea being that during the valid times that the client has chosen they can go into the site and pick out the various stocks that are graded for their safety and growth potential and provide links to the web sites if available, to the various companies that do indeed sell their stock directly to you.  The data itself would come from a variety of sources and it can be assured that Mary has already validated that all the stocks listed on her site are available as DRIPS. </p>\n<h3 id=\"Graduation-and-Awards-Program\"><a href=\"#Graduation-and-Awards-Program\" class=\"headerlink\" title=\"Graduation and Awards Program\"></a>Graduation and Awards Program</h3><p>Then in the last year that Mary was working in the Activities Department of a High School, she came across an interesting opportunity.  It seems that this High School would type up a number of lists for the Awards Night and the Graduation Ceremony.  One list would be a list of all the Awards, the Presenters and the students that received that award.  This would be used during their Awards Night Presentation.  Then on Graduation there would be a program that listed all the students and the awards that they received during the Awards Night.  Besides this being a lot of repetitive work, it was always easy to have many errors and a lot of time was spent proofreading the lists.  </p>\n<p>Mary knew there had to be a better solution to this and brought the problem home where we designed an Access Application where Students, Presenters, and Awards were only entered once, with links to each other and the result being two Access Reports that could be either exported into Word for some further formatting adjustments or Printed out right from Access to be given to the Printer to print these two programs.  This proved to be quite successful and the next year, this High School had Mary come back to provide some instruction and make some minor tweaks to the application. </p>\n<p>It did not take long for Mary and I to realize that if this High School had such a large task in front of them when it came to the end of the school year, so would every other High School and Middle School in the country.  However, the Access Application has a bit of a quick and dirty feel to it and would need to be re-engineered into a more commercial product, especially if we were going to support it.  Plans are in motion to build a complete application even though the decisions for the final name have not been reached yet.  Our plan is to build and then market this application starting with all the schools here in Southern California. </p>\n<h3 id=\"Custom-Software-Development\"><a href=\"#Custom-Software-Development\" class=\"headerlink\" title=\"Custom Software Development\"></a>Custom Software Development</h3><p>Several months ago, Mary was questioning me about the future of our little corporation.  We had the company in place, although we had not opened any bank accounts or anything further then shelling out the legal fees involved in getting ourselves set up.  There was no point in going too fast, as all the projects that we had been talking about so far would cost us money and time to develop and we really were not sitting on top of any real surpluses of either.  Still wanting to do all these projects, we would just go about it more slowly.  There was a need to upgrade our entire network, as the servers and workstations were old and having great difficulty keeping up with the technology that we were using for development and storage. </p>\n<p>Then near the end of January 2002, everything changed.  The company that I had been working for over the last 3.5 years closed its doors.  Now I was officially unemployed, and The Web We Weave, Inc. was not able to support us at this time, or so we thought.  The last client that I worked for through my employer was attempting to put a number of the team members on their project back together, offering them short term contracts.  </p>\n<p>I was one of the lucky ones, and as it turns out it worked out quite well in that we put the contract in the name of The Web We Weave, Inc. and now we are doing custom software development. </p>\n<h3 id=\"Beyond-Technical\"><a href=\"#Beyond-Technical\" class=\"headerlink\" title=\"Beyond Technical\"></a>Beyond Technical</h3><p>Besides all of these more or less technical projects, Mary and I have interests in writing.  Mary has a number of ideas for books that she would like to write and I still have a desire to do things with music.  We both would like to write articles for various publications as we both feel we have something to say and would like to share it with the rest of the world. </p>\n<h3 id=\"What-is-“The-Web-We-Weave-Inc-”\"><a href=\"#What-is-“The-Web-We-Weave-Inc-”\" class=\"headerlink\" title=\"What is “The Web We Weave, Inc.”?\"></a>What is “The Web We Weave, Inc.”?</h3><p>Well, we are back to this question, what is “The Web We Weave, Inc.”?  And you are probably as confused about this as we are.  We made a list of words that we thought were some descriptions of what describes the nature of our company, but still no simple mission statement.<br>-Internet </p>\n<ul>\n<li>web-FUEL </li>\n<li>Nothing but Direct </li>\n<li>Education </li>\n<li>Fuel Consumption </li>\n<li>Software Development </li>\n<li>Web Development </li>\n<li>AGP Maker </li>\n<li>Research </li>\n<li>Business Intelligence </li>\n<li>E-commerce </li>\n<li>Consulting </li>\n<li>Organizing </li>\n<li>Tracking </li>\n<li>Money Making </li>\n<li>Profitable </li>\n<li>Service Provider </li>\n<li>Hardware </li>\n<li>Relaxed </li>\n<li>Confident  </li>\n<li>Professional </li>\n<li>Cutting Edge Technology </li>\n</ul>\n<p>We will keep working on it, to find that perfect mission statement and motto that clears up exactly what “The Web We Weave, Inc.” is all about.  It just shows that I was wrong; you need more then just a cool name. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>A little more than a year ago, Mary and I had a discussion about the many projects that we both have had in the backs of our minds and would like to make a reality.  We thought a corporation would be good in that it could provide us with the legal entity and a single structure in which we could register our copyrights and trademarks.  It might also provide us with some tax relief and if things went well, could very well represent a major part of our future. </p>\n<p>Well, these things are all well and good until you find that our vast array of projects are just that, vast, varied and hard to find a simple way to describe what our company “The Web We Weave, Inc.” is all about.  I guess the best way to present this is to go through our current list, and how we came about these as they all lead to a very interesting story, at least to Mary and me. </p>\n<h3 id=\"Fuel-Consumption\"><a href=\"#Fuel-Consumption\" class=\"headerlink\" title=\"Fuel Consumption\"></a>Fuel Consumption</h3><p>I have been interested in fuel consumption and fuel consumption tracking since about as far back as 1990 or 1991.  Somewhere around this time, I was spending a lot of my time traveling between Canada and the USA.  I would often travel with my laptop and liked to keep an eye on my fuel consumption.  There were a few calculator type programs that could do this when in Canada and we used them at the family-owned dealership to verify fuel consumption for our customers.  The problem that I had was when I traveled to the US; I had to do all these extra conversions from a US gallon into liters in order for this other program to do the calculation.  I figured that there had to be a better way to do this that would take the various measurements and do the conversions on the fly in the background. </p>\n<p>As a result of this, I built an application that I later distributed as shareware called “win-Fuel” and it was written as a desktop application in Visual Basic 1.0.  It did exactly what I had in mind.  Radio buttons that switched between miles and kilometers and a second set of radio buttons to switch between Liters, Imperial Gallons, and US Gallons.  The display of the results showed the consumption calculations in four formats; Miles Per US Gallons, Miles Per Imperial Gallons, Miles Per Liter, and the number of Miles per 100 Liters (the official metric calculation).  I did have some success with the application but more importantly to me than the success was that it provided me with the experience of building a simple application and taking it through all the stages; from concept, development, to distribution.  I also built a context sensitive “help” and a “setup” program to complete the project. </p>\n<p>I had always planned to go back to this little application and add the ability to store the information in a database of some sort and use that data to compare past trips with the current calculation.  However, that never did happen.  But now with the availability of the Internet and the ease at which what was once deemed a desktop application can now be transported into a web application and provide an even larger data source, this is a possibility.  It is my belief that the collected data could be quite valuable to governments, environmental groups, car manufactures as well as individuals.  Up to the present, fuel consumption has always been measured under laboratory conditions and real data has never been taken into consideration. </p>\n<p>I see this site as a free service to the users of the application which will allow me to be able to provide a market with the collective value of the data.  I also see this as a central place where fuel consumptions can be discussed in the form of forums and discussions, as well as, articles on fuel consumption such as tips on getting better gas mileage and regular vs. premium grade gasoline. </p>\n<h3 id=\"Stock-Market-Analysis\"><a href=\"#Stock-Market-Analysis\" class=\"headerlink\" title=\"Stock Market Analysis\"></a>Stock Market Analysis</h3><p>Mary has always had an interest in the Stock Market and has been very good at doing the research and analysis necessary to make good stock market picks.  From this interest she has wanted to be able to share some of this research with others in the rather unique way of only looking at and grading stocks that can be bought directly from companies, the official term is DRIPS (Direct Reimbursement Investment Programs). </p>\n<p>One of the many interesting things that I learned about this as Mary has been explaining it to me, is that stock bought through a stock broker is not in your name but instead is being held for you under the brokers’ name.  Buying directly from companies enables you to buy stock in your own name, have a right to vote at shareholders meetings, avoid stock broker fees and a whole lot of other benefits that Mary can tell you about on her new site.  Sounds pretty good, doesn’t it? </p>\n<p>Anyway, the model for this site is a choice of either a monthly or yearly subscription basis or on a per use basis.  The idea being that during the valid times that the client has chosen they can go into the site and pick out the various stocks that are graded for their safety and growth potential and provide links to the web sites if available, to the various companies that do indeed sell their stock directly to you.  The data itself would come from a variety of sources and it can be assured that Mary has already validated that all the stocks listed on her site are available as DRIPS. </p>\n<h3 id=\"Graduation-and-Awards-Program\"><a href=\"#Graduation-and-Awards-Program\" class=\"headerlink\" title=\"Graduation and Awards Program\"></a>Graduation and Awards Program</h3><p>Then in the last year that Mary was working in the Activities Department of a High School, she came across an interesting opportunity.  It seems that this High School would type up a number of lists for the Awards Night and the Graduation Ceremony.  One list would be a list of all the Awards, the Presenters and the students that received that award.  This would be used during their Awards Night Presentation.  Then on Graduation there would be a program that listed all the students and the awards that they received during the Awards Night.  Besides this being a lot of repetitive work, it was always easy to have many errors and a lot of time was spent proofreading the lists.  </p>\n<p>Mary knew there had to be a better solution to this and brought the problem home where we designed an Access Application where Students, Presenters, and Awards were only entered once, with links to each other and the result being two Access Reports that could be either exported into Word for some further formatting adjustments or Printed out right from Access to be given to the Printer to print these two programs.  This proved to be quite successful and the next year, this High School had Mary come back to provide some instruction and make some minor tweaks to the application. </p>\n<p>It did not take long for Mary and I to realize that if this High School had such a large task in front of them when it came to the end of the school year, so would every other High School and Middle School in the country.  However, the Access Application has a bit of a quick and dirty feel to it and would need to be re-engineered into a more commercial product, especially if we were going to support it.  Plans are in motion to build a complete application even though the decisions for the final name have not been reached yet.  Our plan is to build and then market this application starting with all the schools here in Southern California. </p>\n<h3 id=\"Custom-Software-Development\"><a href=\"#Custom-Software-Development\" class=\"headerlink\" title=\"Custom Software Development\"></a>Custom Software Development</h3><p>Several months ago, Mary was questioning me about the future of our little corporation.  We had the company in place, although we had not opened any bank accounts or anything further then shelling out the legal fees involved in getting ourselves set up.  There was no point in going too fast, as all the projects that we had been talking about so far would cost us money and time to develop and we really were not sitting on top of any real surpluses of either.  Still wanting to do all these projects, we would just go about it more slowly.  There was a need to upgrade our entire network, as the servers and workstations were old and having great difficulty keeping up with the technology that we were using for development and storage. </p>\n<p>Then near the end of January 2002, everything changed.  The company that I had been working for over the last 3.5 years closed its doors.  Now I was officially unemployed, and The Web We Weave, Inc. was not able to support us at this time, or so we thought.  The last client that I worked for through my employer was attempting to put a number of the team members on their project back together, offering them short term contracts.  </p>\n<p>I was one of the lucky ones, and as it turns out it worked out quite well in that we put the contract in the name of The Web We Weave, Inc. and now we are doing custom software development. </p>\n<h3 id=\"Beyond-Technical\"><a href=\"#Beyond-Technical\" class=\"headerlink\" title=\"Beyond Technical\"></a>Beyond Technical</h3><p>Besides all of these more or less technical projects, Mary and I have interests in writing.  Mary has a number of ideas for books that she would like to write and I still have a desire to do things with music.  We both would like to write articles for various publications as we both feel we have something to say and would like to share it with the rest of the world. </p>\n<h3 id=\"What-is-“The-Web-We-Weave-Inc-”\"><a href=\"#What-is-“The-Web-We-Weave-Inc-”\" class=\"headerlink\" title=\"What is “The Web We Weave, Inc.”?\"></a>What is “The Web We Weave, Inc.”?</h3><p>Well, we are back to this question, what is “The Web We Weave, Inc.”?  And you are probably as confused about this as we are.  We made a list of words that we thought were some descriptions of what describes the nature of our company, but still no simple mission statement.<br>-Internet </p>\n<ul>\n<li>web-FUEL </li>\n<li>Nothing but Direct </li>\n<li>Education </li>\n<li>Fuel Consumption </li>\n<li>Software Development </li>\n<li>Web Development </li>\n<li>AGP Maker </li>\n<li>Research </li>\n<li>Business Intelligence </li>\n<li>E-commerce </li>\n<li>Consulting </li>\n<li>Organizing </li>\n<li>Tracking </li>\n<li>Money Making </li>\n<li>Profitable </li>\n<li>Service Provider </li>\n<li>Hardware </li>\n<li>Relaxed </li>\n<li>Confident  </li>\n<li>Professional </li>\n<li>Cutting Edge Technology </li>\n</ul>\n<p>We will keep working on it, to find that perfect mission statement and motto that clears up exactly what “The Web We Weave, Inc.” is all about.  It just shows that I was wrong; you need more then just a cool name. </p>\n"},{"title":"When Should we Move the Work Items to DONE?","date":"2017-10-09T19:36:21.000Z","_content":"{% img left /images/Done.jpg 150 150 \"Done\" %}\nThis is a very common question that I get asked by different software development teams as I make my rounds to helping clients with their ALM practises.  There is a common pattern associated with this question and I know this is the practise when I see a lot of columns on their Kanban boards or worse yet a lot of states that they are tracking on the work items.\n### Fewer States (keep it down to no more than 4 or 5)\nWhen I see more than the 4 or 5 out of the box states that start in a TFS out of the box template it tells me that the team is trying to micro manage the work items.  They are adding more work to their plates then they need to.  It really gets hard to manage the work when it goes beyond doing the work because then the question comes up with who is responsible for moving the work item to Done and when is it Done?\n\nThe goal behind the work items and here I am specifically referring to the Product Backlog Items (the requirement type) and the Bugs is to track the work to complete the described work.  This is in conflict to the pattern way of thinking that I spoke of earlier where the thought is that we need to track this work item through all the environments as we are testing and deploying.  I am telling you that you do not.  Initally when we are in the development cycle we are working closely with the testing team and as soon as we have something ready to test, they can test it right away because we have a proper CI/CD pipeline in place and can approve work that we have completed so that they can have a go at it to confirm that the new functionality or fix works as expected.\n{% img center /images/BuildThroughEnvironment.png 800 700 \"Done\" %}\nIf the functionality is correct, the initial tests are passing then we can go ahead and push the code to the parent branch (could be master or develop, depending on the process you are following) which starts the beginning of the code review and a new set of testing can begin as this should trigger yet another CI/CD pipeline but this time we are testing this against other code as well and making sure that all the code in the build is working nicely together.\n### The Wrong Assumptions\nAn incorrect assumption that comes up when testing some of those very same test cases that were passing when we were doing the functional testing the first round are the same bugs.  Or are they?  The first round of testing you were in an almost isolated environment along side the development team but now that we are working from a merged branch such as master or develop. There is a good chance that they are related or it could be some other piece of code that is acting badly and we just happen to have caught it using the test case we used to test that new bit of functionality.\nNot having that assumption and instead creating a new bug gives us a cleaner slate from where we can analyise this incorrect behaviour.  Remember that test cases live on until they are no longer useful for the purposes of testing the application.  Bugs and PBI's and Stories do not, they always end after the work has been completed.  They can come back as there are times where we might have missed something, but do not assume that is what happened.\n### When Does the State for the Work Item switch to DONE?\nThe simple answer to that question is when the work is done.  The work is done when the coding and testing have been complete but this is going to be functional testing that we are talking about here and that testing was done from that active branch that was created for the development of this work.  We have developers and testers working side by side and in a CI/CD environment this is a very natural flow.  Work gets checked into source control, the build kicks off and deploys to the development environment (not your laptop) where the developer can give it a quick smoke test.  From there they can approve the build to move on into a QA environment.  If the testing from QA is successful then this could be a good place to implement a Pull Request.\n{% img center /images/NewToDone.png 700 700 \"New to Done\" %}\nThe Pull Request does a couple of things, it provides a great opportunity to force a code review and squish the multiple commits into one nice clean commit and to automatically close the work item (set it to DONE).\nThat Pull Request will then start another Build which then deploys to Dev to QA (this time funtion and regression testing) as this could be a potential candidate for production.\n### Work Item is DONE but the testing continues\nIn a previous post [Let the Test Plan Tell the Story](https://donaldonsoftware.azurewebsites.net/2016/04/Let-the-Test-Plan-Tell-the-Story/) I explain how the test plan is the real tool that tells us if the build we are testing is ready for a release into Production.  This is the tool we use to verify that the functionality of the current new changes as well as the older features are working as expected through test cases.  We are not testing the Stories and Bugs directly those are DONE when the work is done.\n{% img center /images/TestResults.png 700 700 \"New to Done\" %}","source":"_posts/When-Should-we-Move-the-Work-Items-to-DONE.md","raw":"---\ntitle: When Should we Move the Work Items to DONE?\ndate: 2017-10-09 12:36:21\ntags:\n- ALM\n- DevOps\n- TFS\n---\n{% img left /images/Done.jpg 150 150 \"Done\" %}\nThis is a very common question that I get asked by different software development teams as I make my rounds to helping clients with their ALM practises.  There is a common pattern associated with this question and I know this is the practise when I see a lot of columns on their Kanban boards or worse yet a lot of states that they are tracking on the work items.\n### Fewer States (keep it down to no more than 4 or 5)\nWhen I see more than the 4 or 5 out of the box states that start in a TFS out of the box template it tells me that the team is trying to micro manage the work items.  They are adding more work to their plates then they need to.  It really gets hard to manage the work when it goes beyond doing the work because then the question comes up with who is responsible for moving the work item to Done and when is it Done?\n\nThe goal behind the work items and here I am specifically referring to the Product Backlog Items (the requirement type) and the Bugs is to track the work to complete the described work.  This is in conflict to the pattern way of thinking that I spoke of earlier where the thought is that we need to track this work item through all the environments as we are testing and deploying.  I am telling you that you do not.  Initally when we are in the development cycle we are working closely with the testing team and as soon as we have something ready to test, they can test it right away because we have a proper CI/CD pipeline in place and can approve work that we have completed so that they can have a go at it to confirm that the new functionality or fix works as expected.\n{% img center /images/BuildThroughEnvironment.png 800 700 \"Done\" %}\nIf the functionality is correct, the initial tests are passing then we can go ahead and push the code to the parent branch (could be master or develop, depending on the process you are following) which starts the beginning of the code review and a new set of testing can begin as this should trigger yet another CI/CD pipeline but this time we are testing this against other code as well and making sure that all the code in the build is working nicely together.\n### The Wrong Assumptions\nAn incorrect assumption that comes up when testing some of those very same test cases that were passing when we were doing the functional testing the first round are the same bugs.  Or are they?  The first round of testing you were in an almost isolated environment along side the development team but now that we are working from a merged branch such as master or develop. There is a good chance that they are related or it could be some other piece of code that is acting badly and we just happen to have caught it using the test case we used to test that new bit of functionality.\nNot having that assumption and instead creating a new bug gives us a cleaner slate from where we can analyise this incorrect behaviour.  Remember that test cases live on until they are no longer useful for the purposes of testing the application.  Bugs and PBI's and Stories do not, they always end after the work has been completed.  They can come back as there are times where we might have missed something, but do not assume that is what happened.\n### When Does the State for the Work Item switch to DONE?\nThe simple answer to that question is when the work is done.  The work is done when the coding and testing have been complete but this is going to be functional testing that we are talking about here and that testing was done from that active branch that was created for the development of this work.  We have developers and testers working side by side and in a CI/CD environment this is a very natural flow.  Work gets checked into source control, the build kicks off and deploys to the development environment (not your laptop) where the developer can give it a quick smoke test.  From there they can approve the build to move on into a QA environment.  If the testing from QA is successful then this could be a good place to implement a Pull Request.\n{% img center /images/NewToDone.png 700 700 \"New to Done\" %}\nThe Pull Request does a couple of things, it provides a great opportunity to force a code review and squish the multiple commits into one nice clean commit and to automatically close the work item (set it to DONE).\nThat Pull Request will then start another Build which then deploys to Dev to QA (this time funtion and regression testing) as this could be a potential candidate for production.\n### Work Item is DONE but the testing continues\nIn a previous post [Let the Test Plan Tell the Story](https://donaldonsoftware.azurewebsites.net/2016/04/Let-the-Test-Plan-Tell-the-Story/) I explain how the test plan is the real tool that tells us if the build we are testing is ready for a release into Production.  This is the tool we use to verify that the functionality of the current new changes as well as the older features are working as expected through test cases.  We are not testing the Stories and Bugs directly those are DONE when the work is done.\n{% img center /images/TestResults.png 700 700 \"New to Done\" %}","slug":"When-Should-we-Move-the-Work-Items-to-DONE","published":1,"updated":"2020-01-05T00:36:05.106Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqggd000rs4ufcy5tohmj","content":"<img src=\"/images/Done.jpg\" class=\"left\" width=\"150\" height=\"150\" title=\"Done\">\n<p>This is a very common question that I get asked by different software development teams as I make my rounds to helping clients with their ALM practises.  There is a common pattern associated with this question and I know this is the practise when I see a lot of columns on their Kanban boards or worse yet a lot of states that they are tracking on the work items.</p>\n<h3 id=\"Fewer-States-keep-it-down-to-no-more-than-4-or-5\"><a href=\"#Fewer-States-keep-it-down-to-no-more-than-4-or-5\" class=\"headerlink\" title=\"Fewer States (keep it down to no more than 4 or 5)\"></a>Fewer States (keep it down to no more than 4 or 5)</h3><p>When I see more than the 4 or 5 out of the box states that start in a TFS out of the box template it tells me that the team is trying to micro manage the work items.  They are adding more work to their plates then they need to.  It really gets hard to manage the work when it goes beyond doing the work because then the question comes up with who is responsible for moving the work item to Done and when is it Done?</p>\n<p>The goal behind the work items and here I am specifically referring to the Product Backlog Items (the requirement type) and the Bugs is to track the work to complete the described work.  This is in conflict to the pattern way of thinking that I spoke of earlier where the thought is that we need to track this work item through all the environments as we are testing and deploying.  I am telling you that you do not.  Initally when we are in the development cycle we are working closely with the testing team and as soon as we have something ready to test, they can test it right away because we have a proper CI/CD pipeline in place and can approve work that we have completed so that they can have a go at it to confirm that the new functionality or fix works as expected.<br><img src=\"/images/BuildThroughEnvironment.png\" class=\"center\" width=\"800\" height=\"700\" title=\"Done\"><br>If the functionality is correct, the initial tests are passing then we can go ahead and push the code to the parent branch (could be master or develop, depending on the process you are following) which starts the beginning of the code review and a new set of testing can begin as this should trigger yet another CI/CD pipeline but this time we are testing this against other code as well and making sure that all the code in the build is working nicely together.</p>\n<h3 id=\"The-Wrong-Assumptions\"><a href=\"#The-Wrong-Assumptions\" class=\"headerlink\" title=\"The Wrong Assumptions\"></a>The Wrong Assumptions</h3><p>An incorrect assumption that comes up when testing some of those very same test cases that were passing when we were doing the functional testing the first round are the same bugs.  Or are they?  The first round of testing you were in an almost isolated environment along side the development team but now that we are working from a merged branch such as master or develop. There is a good chance that they are related or it could be some other piece of code that is acting badly and we just happen to have caught it using the test case we used to test that new bit of functionality.<br>Not having that assumption and instead creating a new bug gives us a cleaner slate from where we can analyise this incorrect behaviour.  Remember that test cases live on until they are no longer useful for the purposes of testing the application.  Bugs and PBI’s and Stories do not, they always end after the work has been completed.  They can come back as there are times where we might have missed something, but do not assume that is what happened.</p>\n<h3 id=\"When-Does-the-State-for-the-Work-Item-switch-to-DONE\"><a href=\"#When-Does-the-State-for-the-Work-Item-switch-to-DONE\" class=\"headerlink\" title=\"When Does the State for the Work Item switch to DONE?\"></a>When Does the State for the Work Item switch to DONE?</h3><p>The simple answer to that question is when the work is done.  The work is done when the coding and testing have been complete but this is going to be functional testing that we are talking about here and that testing was done from that active branch that was created for the development of this work.  We have developers and testers working side by side and in a CI/CD environment this is a very natural flow.  Work gets checked into source control, the build kicks off and deploys to the development environment (not your laptop) where the developer can give it a quick smoke test.  From there they can approve the build to move on into a QA environment.  If the testing from QA is successful then this could be a good place to implement a Pull Request.<br><img src=\"/images/NewToDone.png\" class=\"center\" width=\"700\" height=\"700\" title=\"New to Done\"><br>The Pull Request does a couple of things, it provides a great opportunity to force a code review and squish the multiple commits into one nice clean commit and to automatically close the work item (set it to DONE).<br>That Pull Request will then start another Build which then deploys to Dev to QA (this time funtion and regression testing) as this could be a potential candidate for production.</p>\n<h3 id=\"Work-Item-is-DONE-but-the-testing-continues\"><a href=\"#Work-Item-is-DONE-but-the-testing-continues\" class=\"headerlink\" title=\"Work Item is DONE but the testing continues\"></a>Work Item is DONE but the testing continues</h3><p>In a previous post <a href=\"https://donaldonsoftware.azurewebsites.net/2016/04/Let-the-Test-Plan-Tell-the-Story/\" target=\"_blank\" rel=\"noopener\">Let the Test Plan Tell the Story</a> I explain how the test plan is the real tool that tells us if the build we are testing is ready for a release into Production.  This is the tool we use to verify that the functionality of the current new changes as well as the older features are working as expected through test cases.  We are not testing the Stories and Bugs directly those are DONE when the work is done.<br><img src=\"/images/TestResults.png\" class=\"center\" width=\"700\" height=\"700\" title=\"New to Done\"></p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/Done.jpg\" class=\"left\" width=\"150\" height=\"150\" title=\"Done\">\n<p>This is a very common question that I get asked by different software development teams as I make my rounds to helping clients with their ALM practises.  There is a common pattern associated with this question and I know this is the practise when I see a lot of columns on their Kanban boards or worse yet a lot of states that they are tracking on the work items.</p>\n<h3 id=\"Fewer-States-keep-it-down-to-no-more-than-4-or-5\"><a href=\"#Fewer-States-keep-it-down-to-no-more-than-4-or-5\" class=\"headerlink\" title=\"Fewer States (keep it down to no more than 4 or 5)\"></a>Fewer States (keep it down to no more than 4 or 5)</h3><p>When I see more than the 4 or 5 out of the box states that start in a TFS out of the box template it tells me that the team is trying to micro manage the work items.  They are adding more work to their plates then they need to.  It really gets hard to manage the work when it goes beyond doing the work because then the question comes up with who is responsible for moving the work item to Done and when is it Done?</p>\n<p>The goal behind the work items and here I am specifically referring to the Product Backlog Items (the requirement type) and the Bugs is to track the work to complete the described work.  This is in conflict to the pattern way of thinking that I spoke of earlier where the thought is that we need to track this work item through all the environments as we are testing and deploying.  I am telling you that you do not.  Initally when we are in the development cycle we are working closely with the testing team and as soon as we have something ready to test, they can test it right away because we have a proper CI/CD pipeline in place and can approve work that we have completed so that they can have a go at it to confirm that the new functionality or fix works as expected.<br><img src=\"/images/BuildThroughEnvironment.png\" class=\"center\" width=\"800\" height=\"700\" title=\"Done\"><br>If the functionality is correct, the initial tests are passing then we can go ahead and push the code to the parent branch (could be master or develop, depending on the process you are following) which starts the beginning of the code review and a new set of testing can begin as this should trigger yet another CI/CD pipeline but this time we are testing this against other code as well and making sure that all the code in the build is working nicely together.</p>\n<h3 id=\"The-Wrong-Assumptions\"><a href=\"#The-Wrong-Assumptions\" class=\"headerlink\" title=\"The Wrong Assumptions\"></a>The Wrong Assumptions</h3><p>An incorrect assumption that comes up when testing some of those very same test cases that were passing when we were doing the functional testing the first round are the same bugs.  Or are they?  The first round of testing you were in an almost isolated environment along side the development team but now that we are working from a merged branch such as master or develop. There is a good chance that they are related or it could be some other piece of code that is acting badly and we just happen to have caught it using the test case we used to test that new bit of functionality.<br>Not having that assumption and instead creating a new bug gives us a cleaner slate from where we can analyise this incorrect behaviour.  Remember that test cases live on until they are no longer useful for the purposes of testing the application.  Bugs and PBI’s and Stories do not, they always end after the work has been completed.  They can come back as there are times where we might have missed something, but do not assume that is what happened.</p>\n<h3 id=\"When-Does-the-State-for-the-Work-Item-switch-to-DONE\"><a href=\"#When-Does-the-State-for-the-Work-Item-switch-to-DONE\" class=\"headerlink\" title=\"When Does the State for the Work Item switch to DONE?\"></a>When Does the State for the Work Item switch to DONE?</h3><p>The simple answer to that question is when the work is done.  The work is done when the coding and testing have been complete but this is going to be functional testing that we are talking about here and that testing was done from that active branch that was created for the development of this work.  We have developers and testers working side by side and in a CI/CD environment this is a very natural flow.  Work gets checked into source control, the build kicks off and deploys to the development environment (not your laptop) where the developer can give it a quick smoke test.  From there they can approve the build to move on into a QA environment.  If the testing from QA is successful then this could be a good place to implement a Pull Request.<br><img src=\"/images/NewToDone.png\" class=\"center\" width=\"700\" height=\"700\" title=\"New to Done\"><br>The Pull Request does a couple of things, it provides a great opportunity to force a code review and squish the multiple commits into one nice clean commit and to automatically close the work item (set it to DONE).<br>That Pull Request will then start another Build which then deploys to Dev to QA (this time funtion and regression testing) as this could be a potential candidate for production.</p>\n<h3 id=\"Work-Item-is-DONE-but-the-testing-continues\"><a href=\"#Work-Item-is-DONE-but-the-testing-continues\" class=\"headerlink\" title=\"Work Item is DONE but the testing continues\"></a>Work Item is DONE but the testing continues</h3><p>In a previous post <a href=\"https://donaldonsoftware.azurewebsites.net/2016/04/Let-the-Test-Plan-Tell-the-Story/\" target=\"_blank\" rel=\"noopener\">Let the Test Plan Tell the Story</a> I explain how the test plan is the real tool that tells us if the build we are testing is ready for a release into Production.  This is the tool we use to verify that the functionality of the current new changes as well as the older features are working as expected through test cases.  We are not testing the Stories and Bugs directly those are DONE when the work is done.<br><img src=\"/images/TestResults.png\" class=\"center\" width=\"700\" height=\"700\" title=\"New to Done\"></p>\n"},{"title":"Who left the Developers in the Design Room","date":"2016-05-23T23:53:51.000Z","_content":"{% img left /images/SoapBox.jpg 100 100 \"My Opinion\" %}\nThis post is all about something that has been starting to bug me and it has been bugging me for quite a while.  I have been quiet about this and have started the conversation with different people at random and now it is finally time I just say my piece.  Yes this is soap box time and so I am just going to unload here.  If you don't like this kind of post, I promise to be more joyful and uplifting next month but this month I am going to lay it out there and it just might sound a bit harsh.\n\n## Developers are bad Designers\n{% img right /images/BadDesign.png 250 200 \"Not very practical\" %}\nI come from the development community with over 25 years I have spent on the craft and originally I got there because I was tired of the bad workflows and interfaces that people who thought they understood how accounting should work, just did not.  I implemented a system that changed my workload from 12 hour days plus some weekends to getting everything done in 10 normal days.  Needless to say I worked my way out of a job, but that was okay because that led me to opportunities that really allowed me to be creative.  You would think that with a track record like that I should be able to design very usable software and be a developer, right?\n\nTurns out that being a developer has given me developer characteristics and that is that we are a bit geeky.  As a geeky person, you tend to like having massive control and clicking lots of buttons, but this might not be the best experience for a user that is just trying to get their job done.  I once made the mistake of asking my wife, who was the Product Owner of a little product that we were building, what the message should be when they confirm that they want to Save a Student.  Her remarks threw me off guard for a moment when she asked why do I need a save button?  I made the change so just save it, don't have a button at all.  \n\n## Where's the Beef\nOkay, so far all I have enlightened you with is that I am not always the best designer and that is why I have gate keepers like my wife who remind me every so often that I am not thinking about the customer.  However, I have noticed that many businesses have been doing a revamping of their websites with what looks like a focus on mobile.  I get that but the end result is that it is harder for me to figure out how to use their site and somethings that I was able to do before are just not possible anymore.  You can tell right away that the changes were not based on how a customer might interact with the site, I don't think the customer was even considered.\n\nOne rule that I always try to follow and this is especially true for an eCommerce site is that you need to make it easy for the customer if you want them to buy.  Some of the experiences that I have had lately almost leave you convinced that they don't want to sell their products or do business with me.  For some of these I have sought out different vendors because the frustration level is just too high.\n\n## Who Tests this Stuff?\n{% img right /images/LackOfTesting.jpg 250 200 \"Anyone Testing This\" %}\nThat leads right into my second peeve in that no one seems to test this stuff.  Sure the developer probably tested their work for proper functionality and there might have even been a product owner who understood the steps he needed to take after talking to the developer and proved to him or herself that the feature was working properly.  That is not testing my friend, both of these groups of people test applications the very same way, it's called the Happy Path.  No one is thinking about all the ways that a customer may expect to interact with the new site.  Especially when you have gone from an older design to the new one, ah, no one thought of that and now your sales numbers are falling because no one knows how to buy from you.\n\n{% img left /images/NoTesting.jpg 150 150 \"No Testing Needed Here\" %}\nTesters have a special gene in their DNA that gives them the ability to think about all the ways that a user may interact with the application and even attempt to do evil things with it.  You want these kind of people on your side, it is best to find it while it is still under development than having a customer find it and worse yet you get hacked which could really cost you financially as well as trust.\n\nIn my previous post [\"Let the Test Plan tell the Story\"](/2016/04/Let-the-Test-Plan-Tell-the-Story/) I laid out the purpose of the test plan.  This is the report that we can always go back to and see what was tested and how much of it was tested and so on.  I feel that the rush to get a new design out the door is hurting the future of many of these companies because they are taking the short cuts of not designing these sites with the customer in mind and eliminating much of the much needed testing.  At least that is how it seems to me, my opinion.\n","source":"_posts/Who-left-the-Developers-in-the-design-room-and-who-is-testing-this-stuff.md","raw":"title: Who left the Developers in the Design Room\ndate: 2016-05-23 16:53:51\ntags:\n- Testing\n- Soap Box\n- User Experience (UX)\n---\n{% img left /images/SoapBox.jpg 100 100 \"My Opinion\" %}\nThis post is all about something that has been starting to bug me and it has been bugging me for quite a while.  I have been quiet about this and have started the conversation with different people at random and now it is finally time I just say my piece.  Yes this is soap box time and so I am just going to unload here.  If you don't like this kind of post, I promise to be more joyful and uplifting next month but this month I am going to lay it out there and it just might sound a bit harsh.\n\n## Developers are bad Designers\n{% img right /images/BadDesign.png 250 200 \"Not very practical\" %}\nI come from the development community with over 25 years I have spent on the craft and originally I got there because I was tired of the bad workflows and interfaces that people who thought they understood how accounting should work, just did not.  I implemented a system that changed my workload from 12 hour days plus some weekends to getting everything done in 10 normal days.  Needless to say I worked my way out of a job, but that was okay because that led me to opportunities that really allowed me to be creative.  You would think that with a track record like that I should be able to design very usable software and be a developer, right?\n\nTurns out that being a developer has given me developer characteristics and that is that we are a bit geeky.  As a geeky person, you tend to like having massive control and clicking lots of buttons, but this might not be the best experience for a user that is just trying to get their job done.  I once made the mistake of asking my wife, who was the Product Owner of a little product that we were building, what the message should be when they confirm that they want to Save a Student.  Her remarks threw me off guard for a moment when she asked why do I need a save button?  I made the change so just save it, don't have a button at all.  \n\n## Where's the Beef\nOkay, so far all I have enlightened you with is that I am not always the best designer and that is why I have gate keepers like my wife who remind me every so often that I am not thinking about the customer.  However, I have noticed that many businesses have been doing a revamping of their websites with what looks like a focus on mobile.  I get that but the end result is that it is harder for me to figure out how to use their site and somethings that I was able to do before are just not possible anymore.  You can tell right away that the changes were not based on how a customer might interact with the site, I don't think the customer was even considered.\n\nOne rule that I always try to follow and this is especially true for an eCommerce site is that you need to make it easy for the customer if you want them to buy.  Some of the experiences that I have had lately almost leave you convinced that they don't want to sell their products or do business with me.  For some of these I have sought out different vendors because the frustration level is just too high.\n\n## Who Tests this Stuff?\n{% img right /images/LackOfTesting.jpg 250 200 \"Anyone Testing This\" %}\nThat leads right into my second peeve in that no one seems to test this stuff.  Sure the developer probably tested their work for proper functionality and there might have even been a product owner who understood the steps he needed to take after talking to the developer and proved to him or herself that the feature was working properly.  That is not testing my friend, both of these groups of people test applications the very same way, it's called the Happy Path.  No one is thinking about all the ways that a customer may expect to interact with the new site.  Especially when you have gone from an older design to the new one, ah, no one thought of that and now your sales numbers are falling because no one knows how to buy from you.\n\n{% img left /images/NoTesting.jpg 150 150 \"No Testing Needed Here\" %}\nTesters have a special gene in their DNA that gives them the ability to think about all the ways that a user may interact with the application and even attempt to do evil things with it.  You want these kind of people on your side, it is best to find it while it is still under development than having a customer find it and worse yet you get hacked which could really cost you financially as well as trust.\n\nIn my previous post [\"Let the Test Plan tell the Story\"](/2016/04/Let-the-Test-Plan-Tell-the-Story/) I laid out the purpose of the test plan.  This is the report that we can always go back to and see what was tested and how much of it was tested and so on.  I feel that the rush to get a new design out the door is hurting the future of many of these companies because they are taking the short cuts of not designing these sites with the customer in mind and eliminating much of the much needed testing.  At least that is how it seems to me, my opinion.\n","slug":"Who-left-the-Developers-in-the-design-room-and-who-is-testing-this-stuff","published":1,"updated":"2020-01-05T00:36:05.108Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgge000ss4ufrz376ohw","content":"<img src=\"/images/SoapBox.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"My Opinion\">\n<p>This post is all about something that has been starting to bug me and it has been bugging me for quite a while.  I have been quiet about this and have started the conversation with different people at random and now it is finally time I just say my piece.  Yes this is soap box time and so I am just going to unload here.  If you don’t like this kind of post, I promise to be more joyful and uplifting next month but this month I am going to lay it out there and it just might sound a bit harsh.</p>\n<h2 id=\"Developers-are-bad-Designers\"><a href=\"#Developers-are-bad-Designers\" class=\"headerlink\" title=\"Developers are bad Designers\"></a>Developers are bad Designers</h2><img src=\"/images/BadDesign.png\" class=\"right\" width=\"250\" height=\"200\" title=\"Not very practical\">\n<p>I come from the development community with over 25 years I have spent on the craft and originally I got there because I was tired of the bad workflows and interfaces that people who thought they understood how accounting should work, just did not.  I implemented a system that changed my workload from 12 hour days plus some weekends to getting everything done in 10 normal days.  Needless to say I worked my way out of a job, but that was okay because that led me to opportunities that really allowed me to be creative.  You would think that with a track record like that I should be able to design very usable software and be a developer, right?</p>\n<p>Turns out that being a developer has given me developer characteristics and that is that we are a bit geeky.  As a geeky person, you tend to like having massive control and clicking lots of buttons, but this might not be the best experience for a user that is just trying to get their job done.  I once made the mistake of asking my wife, who was the Product Owner of a little product that we were building, what the message should be when they confirm that they want to Save a Student.  Her remarks threw me off guard for a moment when she asked why do I need a save button?  I made the change so just save it, don’t have a button at all.  </p>\n<h2 id=\"Where’s-the-Beef\"><a href=\"#Where’s-the-Beef\" class=\"headerlink\" title=\"Where’s the Beef\"></a>Where’s the Beef</h2><p>Okay, so far all I have enlightened you with is that I am not always the best designer and that is why I have gate keepers like my wife who remind me every so often that I am not thinking about the customer.  However, I have noticed that many businesses have been doing a revamping of their websites with what looks like a focus on mobile.  I get that but the end result is that it is harder for me to figure out how to use their site and somethings that I was able to do before are just not possible anymore.  You can tell right away that the changes were not based on how a customer might interact with the site, I don’t think the customer was even considered.</p>\n<p>One rule that I always try to follow and this is especially true for an eCommerce site is that you need to make it easy for the customer if you want them to buy.  Some of the experiences that I have had lately almost leave you convinced that they don’t want to sell their products or do business with me.  For some of these I have sought out different vendors because the frustration level is just too high.</p>\n<h2 id=\"Who-Tests-this-Stuff\"><a href=\"#Who-Tests-this-Stuff\" class=\"headerlink\" title=\"Who Tests this Stuff?\"></a>Who Tests this Stuff?</h2><img src=\"/images/LackOfTesting.jpg\" class=\"right\" width=\"250\" height=\"200\" title=\"Anyone Testing This\">\n<p>That leads right into my second peeve in that no one seems to test this stuff.  Sure the developer probably tested their work for proper functionality and there might have even been a product owner who understood the steps he needed to take after talking to the developer and proved to him or herself that the feature was working properly.  That is not testing my friend, both of these groups of people test applications the very same way, it’s called the Happy Path.  No one is thinking about all the ways that a customer may expect to interact with the new site.  Especially when you have gone from an older design to the new one, ah, no one thought of that and now your sales numbers are falling because no one knows how to buy from you.</p>\n<img src=\"/images/NoTesting.jpg\" class=\"left\" width=\"150\" height=\"150\" title=\"No Testing Needed Here\">\n<p>Testers have a special gene in their DNA that gives them the ability to think about all the ways that a user may interact with the application and even attempt to do evil things with it.  You want these kind of people on your side, it is best to find it while it is still under development than having a customer find it and worse yet you get hacked which could really cost you financially as well as trust.</p>\n<p>In my previous post <a href=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/\">“Let the Test Plan tell the Story”</a> I laid out the purpose of the test plan.  This is the report that we can always go back to and see what was tested and how much of it was tested and so on.  I feel that the rush to get a new design out the door is hurting the future of many of these companies because they are taking the short cuts of not designing these sites with the customer in mind and eliminating much of the much needed testing.  At least that is how it seems to me, my opinion.</p>\n","site":{"data":{}},"excerpt":"","more":"<img src=\"/images/SoapBox.jpg\" class=\"left\" width=\"100\" height=\"100\" title=\"My Opinion\">\n<p>This post is all about something that has been starting to bug me and it has been bugging me for quite a while.  I have been quiet about this and have started the conversation with different people at random and now it is finally time I just say my piece.  Yes this is soap box time and so I am just going to unload here.  If you don’t like this kind of post, I promise to be more joyful and uplifting next month but this month I am going to lay it out there and it just might sound a bit harsh.</p>\n<h2 id=\"Developers-are-bad-Designers\"><a href=\"#Developers-are-bad-Designers\" class=\"headerlink\" title=\"Developers are bad Designers\"></a>Developers are bad Designers</h2><img src=\"/images/BadDesign.png\" class=\"right\" width=\"250\" height=\"200\" title=\"Not very practical\">\n<p>I come from the development community with over 25 years I have spent on the craft and originally I got there because I was tired of the bad workflows and interfaces that people who thought they understood how accounting should work, just did not.  I implemented a system that changed my workload from 12 hour days plus some weekends to getting everything done in 10 normal days.  Needless to say I worked my way out of a job, but that was okay because that led me to opportunities that really allowed me to be creative.  You would think that with a track record like that I should be able to design very usable software and be a developer, right?</p>\n<p>Turns out that being a developer has given me developer characteristics and that is that we are a bit geeky.  As a geeky person, you tend to like having massive control and clicking lots of buttons, but this might not be the best experience for a user that is just trying to get their job done.  I once made the mistake of asking my wife, who was the Product Owner of a little product that we were building, what the message should be when they confirm that they want to Save a Student.  Her remarks threw me off guard for a moment when she asked why do I need a save button?  I made the change so just save it, don’t have a button at all.  </p>\n<h2 id=\"Where’s-the-Beef\"><a href=\"#Where’s-the-Beef\" class=\"headerlink\" title=\"Where’s the Beef\"></a>Where’s the Beef</h2><p>Okay, so far all I have enlightened you with is that I am not always the best designer and that is why I have gate keepers like my wife who remind me every so often that I am not thinking about the customer.  However, I have noticed that many businesses have been doing a revamping of their websites with what looks like a focus on mobile.  I get that but the end result is that it is harder for me to figure out how to use their site and somethings that I was able to do before are just not possible anymore.  You can tell right away that the changes were not based on how a customer might interact with the site, I don’t think the customer was even considered.</p>\n<p>One rule that I always try to follow and this is especially true for an eCommerce site is that you need to make it easy for the customer if you want them to buy.  Some of the experiences that I have had lately almost leave you convinced that they don’t want to sell their products or do business with me.  For some of these I have sought out different vendors because the frustration level is just too high.</p>\n<h2 id=\"Who-Tests-this-Stuff\"><a href=\"#Who-Tests-this-Stuff\" class=\"headerlink\" title=\"Who Tests this Stuff?\"></a>Who Tests this Stuff?</h2><img src=\"/images/LackOfTesting.jpg\" class=\"right\" width=\"250\" height=\"200\" title=\"Anyone Testing This\">\n<p>That leads right into my second peeve in that no one seems to test this stuff.  Sure the developer probably tested their work for proper functionality and there might have even been a product owner who understood the steps he needed to take after talking to the developer and proved to him or herself that the feature was working properly.  That is not testing my friend, both of these groups of people test applications the very same way, it’s called the Happy Path.  No one is thinking about all the ways that a customer may expect to interact with the new site.  Especially when you have gone from an older design to the new one, ah, no one thought of that and now your sales numbers are falling because no one knows how to buy from you.</p>\n<img src=\"/images/NoTesting.jpg\" class=\"left\" width=\"150\" height=\"150\" title=\"No Testing Needed Here\">\n<p>Testers have a special gene in their DNA that gives them the ability to think about all the ways that a user may interact with the application and even attempt to do evil things with it.  You want these kind of people on your side, it is best to find it while it is still under development than having a customer find it and worse yet you get hacked which could really cost you financially as well as trust.</p>\n<p>In my previous post <a href=\"/2016/04/Let-the-Test-Plan-Tell-the-Story/\">“Let the Test Plan tell the Story”</a> I laid out the purpose of the test plan.  This is the report that we can always go back to and see what was tested and how much of it was tested and so on.  I feel that the rush to get a new design out the door is hurting the future of many of these companies because they are taking the short cuts of not designing these sites with the customer in mind and eliminating much of the much needed testing.  At least that is how it seems to me, my opinion.</p>\n"},{"title":"Who's the Boss","date":"2008-06-21T07:00:00.000Z","_content":"For most of our lives we have a constant struggle to try to be the boss of ourselves. Does it ever happen?  When you grew up as a child I am sure you have memories similar to mine where at some point in your life you were struggling to gain control of your own life.  Could not wait to move out of the house and get out on your own, so that you could be the boss of you.  How’s that going for you?  Are you the boss of you yet? \n\nIt is not long after you move out that you find you have a whole bunch of new people that have stepped in to take over the boss position.  You have to pay rent so you have to answer to your landlord as he becomes a certain boss and when you can't pay the rent, he fires you by way of eviction.  Then in order to make some money to pay the rent you have to find a job and that usually leads to a boss and might even have a complete entourage of bosses.  You know what I mean, there is your manager, the assistant manager, then there is the shift manager and none of them are shy at giving you orders and commands.  Come to think of it, maybe living at home wasn't so bad after all. \n## Self Employed\nThen one day you wake up with this fantastic idea.  If you start your own company you could become your own boss.  Then you would truly have reached your goal of being the boss of your self.  Then as the company grows you could end up being the boss for lots of other people.  Yea, this is what you are going to do to be the boss of you.  Well it is never quite like that because if you want to remain in business you will need to listen to your customers.  You need to provide them with a service that they will value and will want to pay you for.  One of the very reasons why a small company has a good chance of competing against a larger competitor is the ability to deliver better quality customer service.  Wait a minute! If I have to listen to my customers and do what they want me to do, then they are my new boss?  That’s right and as your business grows and you attract more and more customers and you want to continue to be successful, the number of people you need to listen to increases as well. You could just ignore the requests of your customers and we all know how that is going to affect your newly formed company. Remember the last time you were fed up with a business that was ignoring your needs. Why, you found a new place of business who was more willing to listen to your needs and even provide you with that service you were looking for. \n## Going Public\nOkay, let’s take the self employed business a step farther. Let say that you do make a real honest effort in your new business and listen to your customers and follow through on many of their suggestions to improve the products and services that you provide. You make improvements’ in your goods and services for the benefit of your customers. The company grows and grows, you are the boss of hundreds maybe even thousands of employees, your customers love your products so you decide to take the business to the next level and go public. You know trade shares of your company on the stock market. This was of course in an effort to reach more customers and to expand to other geographical areas, expand your horizons and get your products and services into your new deserving customers. This changes things. All of a sudden you are hearing from a new group of people that want your attention and they keep talking about steady growth, make more profit and drive the share price up. These are your investors and it sounds like a new set of bosses to me. They don’t seem to share the same passion that you had with pleasing your customers, in fact they don’t seem to care about them other then to make them pay more money and anything to show growth and make the stock price go up. This can be a problem, if you grow too fast and the profits are a little slow at coming in you are going to be under pressure to increase profits somewhere and decline expenses in other areas. Both of these decisions could greatly affect your fine customer service that you have been able to provide in the past. \n## Politics\nLet us talk about one more area in this topic of bosses and that is in the area of politics. I think that sometimes politicians forget that there positions are in a role reversal of sorts. Politicians work for the people, I think the correct term is the servant of the people. Yes the highest ranked position in the country, that of the president is really a servant of the people and we expect them to serve the needs of its citizens and make decisions that are for the good of the people not themselves and the many friends that they have made to get to this fine position of servant hood. \n## Conclusion\nI think that having a boss and having to answer to someone is a fact of life. You can even get to be the president of the United States only to answer to the people, who are your bosses. So, in conclusion be the best boss that you can be to the people who look to you for leadership and threat those in a boss position to you with respect. If they do not deserve your respect, then maybe it is time to leave and find a new and better boss. There are a number of them out there, I know, I have worked for a few of them myself. ","source":"_posts/Who-s-the-Boss.md","raw":"title: \"Who's the Boss\"\ndate: 2008-06-21\ntags:\n- Politics\n---\nFor most of our lives we have a constant struggle to try to be the boss of ourselves. Does it ever happen?  When you grew up as a child I am sure you have memories similar to mine where at some point in your life you were struggling to gain control of your own life.  Could not wait to move out of the house and get out on your own, so that you could be the boss of you.  How’s that going for you?  Are you the boss of you yet? \n\nIt is not long after you move out that you find you have a whole bunch of new people that have stepped in to take over the boss position.  You have to pay rent so you have to answer to your landlord as he becomes a certain boss and when you can't pay the rent, he fires you by way of eviction.  Then in order to make some money to pay the rent you have to find a job and that usually leads to a boss and might even have a complete entourage of bosses.  You know what I mean, there is your manager, the assistant manager, then there is the shift manager and none of them are shy at giving you orders and commands.  Come to think of it, maybe living at home wasn't so bad after all. \n## Self Employed\nThen one day you wake up with this fantastic idea.  If you start your own company you could become your own boss.  Then you would truly have reached your goal of being the boss of your self.  Then as the company grows you could end up being the boss for lots of other people.  Yea, this is what you are going to do to be the boss of you.  Well it is never quite like that because if you want to remain in business you will need to listen to your customers.  You need to provide them with a service that they will value and will want to pay you for.  One of the very reasons why a small company has a good chance of competing against a larger competitor is the ability to deliver better quality customer service.  Wait a minute! If I have to listen to my customers and do what they want me to do, then they are my new boss?  That’s right and as your business grows and you attract more and more customers and you want to continue to be successful, the number of people you need to listen to increases as well. You could just ignore the requests of your customers and we all know how that is going to affect your newly formed company. Remember the last time you were fed up with a business that was ignoring your needs. Why, you found a new place of business who was more willing to listen to your needs and even provide you with that service you were looking for. \n## Going Public\nOkay, let’s take the self employed business a step farther. Let say that you do make a real honest effort in your new business and listen to your customers and follow through on many of their suggestions to improve the products and services that you provide. You make improvements’ in your goods and services for the benefit of your customers. The company grows and grows, you are the boss of hundreds maybe even thousands of employees, your customers love your products so you decide to take the business to the next level and go public. You know trade shares of your company on the stock market. This was of course in an effort to reach more customers and to expand to other geographical areas, expand your horizons and get your products and services into your new deserving customers. This changes things. All of a sudden you are hearing from a new group of people that want your attention and they keep talking about steady growth, make more profit and drive the share price up. These are your investors and it sounds like a new set of bosses to me. They don’t seem to share the same passion that you had with pleasing your customers, in fact they don’t seem to care about them other then to make them pay more money and anything to show growth and make the stock price go up. This can be a problem, if you grow too fast and the profits are a little slow at coming in you are going to be under pressure to increase profits somewhere and decline expenses in other areas. Both of these decisions could greatly affect your fine customer service that you have been able to provide in the past. \n## Politics\nLet us talk about one more area in this topic of bosses and that is in the area of politics. I think that sometimes politicians forget that there positions are in a role reversal of sorts. Politicians work for the people, I think the correct term is the servant of the people. Yes the highest ranked position in the country, that of the president is really a servant of the people and we expect them to serve the needs of its citizens and make decisions that are for the good of the people not themselves and the many friends that they have made to get to this fine position of servant hood. \n## Conclusion\nI think that having a boss and having to answer to someone is a fact of life. You can even get to be the president of the United States only to answer to the people, who are your bosses. So, in conclusion be the best boss that you can be to the people who look to you for leadership and threat those in a boss position to you with respect. If they do not deserve your respect, then maybe it is time to leave and find a new and better boss. There are a number of them out there, I know, I have worked for a few of them myself. ","slug":"Who-s-the-Boss","published":1,"updated":"2020-01-05T00:36:05.108Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck50aqgge000ts4uf3lr5fhme","content":"<p>For most of our lives we have a constant struggle to try to be the boss of ourselves. Does it ever happen?  When you grew up as a child I am sure you have memories similar to mine where at some point in your life you were struggling to gain control of your own life.  Could not wait to move out of the house and get out on your own, so that you could be the boss of you.  How’s that going for you?  Are you the boss of you yet? </p>\n<p>It is not long after you move out that you find you have a whole bunch of new people that have stepped in to take over the boss position.  You have to pay rent so you have to answer to your landlord as he becomes a certain boss and when you can’t pay the rent, he fires you by way of eviction.  Then in order to make some money to pay the rent you have to find a job and that usually leads to a boss and might even have a complete entourage of bosses.  You know what I mean, there is your manager, the assistant manager, then there is the shift manager and none of them are shy at giving you orders and commands.  Come to think of it, maybe living at home wasn’t so bad after all. </p>\n<h2 id=\"Self-Employed\"><a href=\"#Self-Employed\" class=\"headerlink\" title=\"Self Employed\"></a>Self Employed</h2><p>Then one day you wake up with this fantastic idea.  If you start your own company you could become your own boss.  Then you would truly have reached your goal of being the boss of your self.  Then as the company grows you could end up being the boss for lots of other people.  Yea, this is what you are going to do to be the boss of you.  Well it is never quite like that because if you want to remain in business you will need to listen to your customers.  You need to provide them with a service that they will value and will want to pay you for.  One of the very reasons why a small company has a good chance of competing against a larger competitor is the ability to deliver better quality customer service.  Wait a minute! If I have to listen to my customers and do what they want me to do, then they are my new boss?  That’s right and as your business grows and you attract more and more customers and you want to continue to be successful, the number of people you need to listen to increases as well. You could just ignore the requests of your customers and we all know how that is going to affect your newly formed company. Remember the last time you were fed up with a business that was ignoring your needs. Why, you found a new place of business who was more willing to listen to your needs and even provide you with that service you were looking for. </p>\n<h2 id=\"Going-Public\"><a href=\"#Going-Public\" class=\"headerlink\" title=\"Going Public\"></a>Going Public</h2><p>Okay, let’s take the self employed business a step farther. Let say that you do make a real honest effort in your new business and listen to your customers and follow through on many of their suggestions to improve the products and services that you provide. You make improvements’ in your goods and services for the benefit of your customers. The company grows and grows, you are the boss of hundreds maybe even thousands of employees, your customers love your products so you decide to take the business to the next level and go public. You know trade shares of your company on the stock market. This was of course in an effort to reach more customers and to expand to other geographical areas, expand your horizons and get your products and services into your new deserving customers. This changes things. All of a sudden you are hearing from a new group of people that want your attention and they keep talking about steady growth, make more profit and drive the share price up. These are your investors and it sounds like a new set of bosses to me. They don’t seem to share the same passion that you had with pleasing your customers, in fact they don’t seem to care about them other then to make them pay more money and anything to show growth and make the stock price go up. This can be a problem, if you grow too fast and the profits are a little slow at coming in you are going to be under pressure to increase profits somewhere and decline expenses in other areas. Both of these decisions could greatly affect your fine customer service that you have been able to provide in the past. </p>\n<h2 id=\"Politics\"><a href=\"#Politics\" class=\"headerlink\" title=\"Politics\"></a>Politics</h2><p>Let us talk about one more area in this topic of bosses and that is in the area of politics. I think that sometimes politicians forget that there positions are in a role reversal of sorts. Politicians work for the people, I think the correct term is the servant of the people. Yes the highest ranked position in the country, that of the president is really a servant of the people and we expect them to serve the needs of its citizens and make decisions that are for the good of the people not themselves and the many friends that they have made to get to this fine position of servant hood. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>I think that having a boss and having to answer to someone is a fact of life. You can even get to be the president of the United States only to answer to the people, who are your bosses. So, in conclusion be the best boss that you can be to the people who look to you for leadership and threat those in a boss position to you with respect. If they do not deserve your respect, then maybe it is time to leave and find a new and better boss. There are a number of them out there, I know, I have worked for a few of them myself. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>For most of our lives we have a constant struggle to try to be the boss of ourselves. Does it ever happen?  When you grew up as a child I am sure you have memories similar to mine where at some point in your life you were struggling to gain control of your own life.  Could not wait to move out of the house and get out on your own, so that you could be the boss of you.  How’s that going for you?  Are you the boss of you yet? </p>\n<p>It is not long after you move out that you find you have a whole bunch of new people that have stepped in to take over the boss position.  You have to pay rent so you have to answer to your landlord as he becomes a certain boss and when you can’t pay the rent, he fires you by way of eviction.  Then in order to make some money to pay the rent you have to find a job and that usually leads to a boss and might even have a complete entourage of bosses.  You know what I mean, there is your manager, the assistant manager, then there is the shift manager and none of them are shy at giving you orders and commands.  Come to think of it, maybe living at home wasn’t so bad after all. </p>\n<h2 id=\"Self-Employed\"><a href=\"#Self-Employed\" class=\"headerlink\" title=\"Self Employed\"></a>Self Employed</h2><p>Then one day you wake up with this fantastic idea.  If you start your own company you could become your own boss.  Then you would truly have reached your goal of being the boss of your self.  Then as the company grows you could end up being the boss for lots of other people.  Yea, this is what you are going to do to be the boss of you.  Well it is never quite like that because if you want to remain in business you will need to listen to your customers.  You need to provide them with a service that they will value and will want to pay you for.  One of the very reasons why a small company has a good chance of competing against a larger competitor is the ability to deliver better quality customer service.  Wait a minute! If I have to listen to my customers and do what they want me to do, then they are my new boss?  That’s right and as your business grows and you attract more and more customers and you want to continue to be successful, the number of people you need to listen to increases as well. You could just ignore the requests of your customers and we all know how that is going to affect your newly formed company. Remember the last time you were fed up with a business that was ignoring your needs. Why, you found a new place of business who was more willing to listen to your needs and even provide you with that service you were looking for. </p>\n<h2 id=\"Going-Public\"><a href=\"#Going-Public\" class=\"headerlink\" title=\"Going Public\"></a>Going Public</h2><p>Okay, let’s take the self employed business a step farther. Let say that you do make a real honest effort in your new business and listen to your customers and follow through on many of their suggestions to improve the products and services that you provide. You make improvements’ in your goods and services for the benefit of your customers. The company grows and grows, you are the boss of hundreds maybe even thousands of employees, your customers love your products so you decide to take the business to the next level and go public. You know trade shares of your company on the stock market. This was of course in an effort to reach more customers and to expand to other geographical areas, expand your horizons and get your products and services into your new deserving customers. This changes things. All of a sudden you are hearing from a new group of people that want your attention and they keep talking about steady growth, make more profit and drive the share price up. These are your investors and it sounds like a new set of bosses to me. They don’t seem to share the same passion that you had with pleasing your customers, in fact they don’t seem to care about them other then to make them pay more money and anything to show growth and make the stock price go up. This can be a problem, if you grow too fast and the profits are a little slow at coming in you are going to be under pressure to increase profits somewhere and decline expenses in other areas. Both of these decisions could greatly affect your fine customer service that you have been able to provide in the past. </p>\n<h2 id=\"Politics\"><a href=\"#Politics\" class=\"headerlink\" title=\"Politics\"></a>Politics</h2><p>Let us talk about one more area in this topic of bosses and that is in the area of politics. I think that sometimes politicians forget that there positions are in a role reversal of sorts. Politicians work for the people, I think the correct term is the servant of the people. Yes the highest ranked position in the country, that of the president is really a servant of the people and we expect them to serve the needs of its citizens and make decisions that are for the good of the people not themselves and the many friends that they have made to get to this fine position of servant hood. </p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>I think that having a boss and having to answer to someone is a fact of life. You can even get to be the president of the United States only to answer to the people, who are your bosses. So, in conclusion be the best boss that you can be to the people who look to you for leadership and threat those in a boss position to you with respect. If they do not deserve your respect, then maybe it is time to leave and find a new and better boss. There are a number of them out there, I know, I have worked for a few of them myself. </p>\n"}],"PostAsset":[{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image1.png","slug":"image1.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image5.png","slug":"image5.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image3.png","slug":"image3.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image1.png","slug":"image1.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image3.png","slug":"image3.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image2.png","slug":"image2.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image6.png","slug":"image6.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image8.png","slug":"image8.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/IISWebAppDeployTask.png","slug":"IISWebAppDeployTask.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/variables.png","slug":"variables.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Let-the-Test-Plan-Tell-the-Story/TestPlanResults.png","slug":"TestPlanResults.png","post":"ck50aqgfh0009s4uf69g3i0lu","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/1.png","slug":"1.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/5.png","slug":"5.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/BackToTeams.jpg","slug":"BackToTeams.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewParentTeam.jpg","slug":"CreateNewParentTeam.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-2.jpg","slug":"CreateNewTeam-2.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateNewTeam-1.jpg","slug":"CreateNewTeam-1.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/SelectWebAreaPath.jpg","slug":"SelectWebAreaPath.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/The-Power-of-Time-Tracking/image.jpg","slug":"image.jpg","post":"ck50aqgg9000ns4uf2xflmle5","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/web.config.png","slug":"web.config.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Goal-Tracking/image.png","slug":"image.png","post":"ck50aqgff0007s4uf4oljd5i7","modified":0,"renderable":0},{"_id":"source/_posts/How-I-Use-Chocolatey-in-my-Releases/FileStructure.png","slug":"FileStructure.png","post":"ck50aqgfg0008s4ufuywftqh5","modified":0,"renderable":0},{"_id":"source/_posts/My-New-3-Rules-for-Releases/MasterBranchOnly.png","slug":"MasterBranchOnly.png","post":"ck50aqgfp000fs4uf9onpilok","modified":0,"renderable":0},{"_id":"source/_posts/No-no-he-s-not-dead-he-s-he-s-restin/blog_grim_reaper.gif","slug":"blog_grim_reaper.gif","post":"ck50aqgg0000hs4uf33488baz","modified":0,"renderable":0},{"_id":"source/_posts/One-Build-Definition-to-Support-Multiple-Branches/CIBranchFilters.png","slug":"CIBranchFilters.png","post":"ck50aqgg4000is4ufvbcnl4is","modified":0,"renderable":0},{"_id":"source/_posts/A-New-Start-on-an-Old-Blog/V__FD25.jpg","slug":"V__FD25.jpg","post":"ck50aqgf60001s4ufseyh4c3s","modified":0,"renderable":0},{"_id":"source/_posts/A-New-Start-on-an-Old-Blog/hexo.jpg","slug":"hexo.jpg","post":"ck50aqgf60001s4ufseyh4c3s","modified":0,"renderable":0},{"_id":"source/_posts/Let-the-Test-Plan-Tell-the-Story/testImpactResults.jpg","slug":"testImpactResults.jpg","post":"ck50aqgfh0009s4uf69g3i0lu","modified":0,"renderable":0},{"_id":"source/_posts/Some-MSDeploy-Tricks-I-ve-Learned/CreatePackageFromManifest.png","slug":"CreatePackageFromManifest.png","post":"ck50aqgg8000ls4uf2kyp1tbo","modified":0,"renderable":0},{"_id":"source/_posts/Some-MSDeploy-Tricks-I-ve-Learned/DeployWebsiteAzure.png","slug":"DeployWebsiteAzure.png","post":"ck50aqgg8000ls4uf2kyp1tbo","modified":0,"renderable":0},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/FinishResults.png","slug":"FinishResults.png","post":"ck50aqgfk000bs4ufh1deno84","modified":0,"renderable":0},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/NewRepoDialog.png","slug":"NewRepoDialog.png","post":"ck50aqgfk000bs4ufh1deno84","modified":0,"renderable":0},{"_id":"source/_posts/Migrate-from-TFVC-to-Git-in-TFS-with-Full-History/TFSNewRepo.png","slug":"TFSNewRepo.png","post":"ck50aqgfk000bs4ufh1deno84","modified":0,"renderable":0},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/ConditionOption.png","slug":"ConditionOption.png","post":"ck50aqgfm000ds4ufqgbp44zl","modified":0,"renderable":0},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/ConfigureScreen.png","slug":"ConfigureScreen.png","post":"ck50aqgfm000ds4ufqgbp44zl","modified":0,"renderable":0},{"_id":"source/_posts/Master-Only-in-Production-an-Improvement/NewReleaseEditor.png","slug":"NewReleaseEditor.png","post":"ck50aqgfm000ds4ufqgbp44zl","modified":0,"renderable":0},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/DLMBuildTask.png","slug":"DLMBuildTask.png","post":"ck50aqgg5000js4ufwybzfnwq","modified":0,"renderable":0},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/ImportDb.png","slug":"ImportDb.png","post":"ck50aqgg5000js4ufwybzfnwq","modified":0,"renderable":0},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/NewWidgitRepo.png","slug":"NewWidgitRepo.png","post":"ck50aqgg5000js4ufwybzfnwq","modified":0,"renderable":0},{"_id":"source/_posts/Red-Gate-tools-vs-SQL-Server-Data-Tools/pushRepoCopyButton.png","slug":"pushRepoCopyButton.png","post":"ck50aqgg5000js4ufwybzfnwq","modified":0,"renderable":0},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/GearIcon.png","slug":"GearIcon.png","post":"ck50aqgg6000ks4ufqk1lsqql","modified":0,"renderable":0},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NewNotification.png","slug":"NewNotification.png","post":"ck50aqgg6000ks4ufqk1lsqql","modified":0,"renderable":0},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/NotificationDetails.png","slug":"NotificationDetails.png","post":"ck50aqgg6000ks4ufqk1lsqql","modified":0,"renderable":0},{"_id":"source/_posts/Sending-an-Email-to-the-Developer-when-the-Build-Failed/Notifications.png","slug":"Notifications.png","post":"ck50aqgg6000ks4ufqk1lsqql","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image11.png","slug":"image11.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/IISWebAppDeployJsonTask.png","slug":"IISWebAppDeployJsonTask.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/Json.png","slug":"Json.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/variable-json.png","slug":"variable-json.png","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Easy-Configuration-updates-during-Deployment/webconfig.jpg","slug":"webconfig.jpg","post":"ck50aqgfe0006s4uf2jrf7rm8","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image10.png","slug":"image10.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image2.png","slug":"image2.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image4.png","slug":"image4.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image6.png","slug":"image6.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image7.png","slug":"image7.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image8.png","slug":"image8.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Schema-Compare-where-Visual-Studio-goes-that-extra-mile/image9.png","slug":"image9.png","post":"ck50aqgfa0002s4uf4kff0ath","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image10.png","slug":"image10.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image11.png","slug":"image11.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image12.png","slug":"image12.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image13.png","slug":"image13.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image14.png","slug":"image14.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image15.png","slug":"image15.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image4.png","slug":"image4.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image5.png","slug":"image5.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image7.png","slug":"image7.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Database-Unit-Testing-from-the-Beginning/image9.png","slug":"image9.png","post":"ck50aqgfd0005s4ufj97srk3k","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/10.png","slug":"10.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/11.png","slug":"11.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/12.png","slug":"12.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/13.png","slug":"13.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/14.png","slug":"14.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/15.png","slug":"15.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/16.png","slug":"16.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/17.png","slug":"17.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/18.png","slug":"18.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/19.png","slug":"19.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/2.png","slug":"2.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/20.png","slug":"20.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/21.png","slug":"21.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/22.png","slug":"22.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/23.png","slug":"23.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/3.png","slug":"3.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/4.png","slug":"4.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/6.png","slug":"6.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/7.png","slug":"7.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/8.png","slug":"8.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Security-Configuration-for-Teams/9.png","slug":"9.png","post":"ck50aqgfr000gs4uf0uzqjis4","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AddTeamButton.jpg","slug":"AddTeamButton.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AreaDropDown.jpg","slug":"AreaDropDown.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/AreasLink.jpg","slug":"AreasLink.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/BoardsToWorkItems.jpg","slug":"BoardsToWorkItems.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ChooseBug.jpg","slug":"ChooseBug.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickAreas.jpg","slug":"ClickAreas.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickSaveAndClose.jpg","slug":"ClickSaveAndClose.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickSelectAreasButton.jpg","slug":"ClickSelectAreasButton.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickTeam-1.jpg","slug":"ClickTeam-1.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ClickTeam-1Area.jpg","slug":"ClickTeam-1Area.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/CreateTheBug.jpg","slug":"CreateTheBug.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/DeleteButton.jpg","slug":"DeleteButton.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/DeleteTeam.jpg","slug":"DeleteTeam.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ErrorMessage.jpg","slug":"ErrorMessage.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/FilterToolbar.jpg","slug":"FilterToolbar.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/GetToWebBacklog.jpg","slug":"GetToWebBacklog.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/IncludeSubAreas.jpg","slug":"IncludeSubAreas.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewChildButton.jpg","slug":"NewChildButton.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewSecondChildProcess.jpg","slug":"NewSecondChildProcess.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-1.jpg","slug":"NewTeamButtonForTeam-1.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewTeamButtonForTeam-2.jpg","slug":"NewTeamButtonForTeam-2.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/NewWebTeam-1.jpg","slug":"NewWebTeam-1.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/OnlyTeam-2.jpg","slug":"OnlyTeam-2.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ProjectConfigurationButton.jpg","slug":"ProjectConfigurationButton.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/ProjectSettings.jpg","slug":"ProjectSettings.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/Team-1AreaPath.jpg","slug":"Team-1AreaPath.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/Team-2AreaPath.jpg","slug":"Team-2AreaPath.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TeamConfiguration.jpg","slug":"TeamConfiguration.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TeamConfigurationClick.jpg","slug":"TeamConfigurationClick.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/TwoBugs.jpg","slug":"TwoBugs.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0},{"_id":"source/_posts/Teams-SubTeams-and-Area-Path-s/subareaincludedmessage.jpg","slug":"subareaincludedmessage.jpg","post":"ck50aqgg8000ms4uf1u947z6f","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ck50aqgf60001s4ufseyh4c3s","tag_id":"ck50aqgke000us4uf4o17cwjb","_id":"ck50aqgm3002ts4uf57tflhur"},{"post_id":"ck50aqgf60001s4ufseyh4c3s","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm4002us4ufbzno1r52"},{"post_id":"ck50aqgfa0002s4uf4kff0ath","tag_id":"ck50aqgkn000ws4uf6oujiq8j","_id":"ck50aqgm5002vs4ufgj4hos2u"},{"post_id":"ck50aqgfa0002s4uf4kff0ath","tag_id":"ck50aqgko000xs4uf8k2vds1a","_id":"ck50aqgm5002ws4ufg480gg08"},{"post_id":"ck50aqgfa0002s4uf4kff0ath","tag_id":"ck50aqgkr000ys4uft9hulesh","_id":"ck50aqgm5002xs4uf2ye7z9gr"},{"post_id":"ck50aqgfa0002s4uf4kff0ath","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm5002ys4ufw6mcd3c1"},{"post_id":"ck50aqgfb0003s4uf88twwpd0","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm5002zs4ufp4xuutgd"},{"post_id":"ck50aqgfb0003s4uf88twwpd0","tag_id":"ck50aqgkw0011s4uf2rov1kex","_id":"ck50aqgm60030s4ufk7bc85b9"},{"post_id":"ck50aqgfc0004s4ufyly67iww","tag_id":"ck50aqgky0012s4uf34d5f1c6","_id":"ck50aqgm60031s4uf8dyq7yaw"},{"post_id":"ck50aqgfc0004s4ufyly67iww","tag_id":"ck50aqgkz0013s4ufff5yszim","_id":"ck50aqgm60032s4ufav8w9lcl"},{"post_id":"ck50aqgfc0004s4ufyly67iww","tag_id":"ck50aqgl00014s4ufb8bsoarx","_id":"ck50aqgm60033s4ufvc2yek2f"},{"post_id":"ck50aqgfd0005s4ufj97srk3k","tag_id":"ck50aqgko000xs4uf8k2vds1a","_id":"ck50aqgm60034s4ufhvaim2vl"},{"post_id":"ck50aqgfd0005s4ufj97srk3k","tag_id":"ck50aqgkn000ws4uf6oujiq8j","_id":"ck50aqgm60035s4uf7635enfx"},{"post_id":"ck50aqgfd0005s4ufj97srk3k","tag_id":"ck50aqgl10017s4ufr15d4p22","_id":"ck50aqgm60036s4ufhls7wvd5"},{"post_id":"ck50aqgfd0005s4ufj97srk3k","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm70037s4uf7lhcel10"},{"post_id":"ck50aqgfe0006s4uf2jrf7rm8","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgm70038s4ufrdrgg70b"},{"post_id":"ck50aqgfe0006s4uf2jrf7rm8","tag_id":"ck50aqgl2001as4ufovuf9qyo","_id":"ck50aqgm70039s4ufxwl2w21s"},{"post_id":"ck50aqgfe0006s4uf2jrf7rm8","tag_id":"ck50aqgky0012s4uf34d5f1c6","_id":"ck50aqgm8003as4ufnj28st98"},{"post_id":"ck50aqgff0007s4uf4oljd5i7","tag_id":"ck50aqgl3001cs4uf03qiw8al","_id":"ck50aqgm8003bs4ufm4bkqxey"},{"post_id":"ck50aqgff0007s4uf4oljd5i7","tag_id":"ck50aqgkw0011s4uf2rov1kex","_id":"ck50aqgm8003cs4uf0v5qsr21"},{"post_id":"ck50aqgfg0008s4ufuywftqh5","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm8003ds4ufqyqtq7bq"},{"post_id":"ck50aqgfg0008s4ufuywftqh5","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgm9003es4ufirctkvfs"},{"post_id":"ck50aqgfg0008s4ufuywftqh5","tag_id":"ck50aqgl4001gs4ufgn4o1jz1","_id":"ck50aqgm9003fs4ufc4w2en3v"},{"post_id":"ck50aqgfg0008s4ufuywftqh5","tag_id":"ck50aqgl5001hs4ufhm3d93oy","_id":"ck50aqgm9003gs4uf2wf26nyx"},{"post_id":"ck50aqgfh0009s4uf69g3i0lu","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgm9003hs4ufadar9ab0"},{"post_id":"ck50aqgfh0009s4uf69g3i0lu","tag_id":"ck50aqgl10017s4ufr15d4p22","_id":"ck50aqgm9003is4uf48t9mnd8"},{"post_id":"ck50aqgfh0009s4uf69g3i0lu","tag_id":"ck50aqgky0012s4uf34d5f1c6","_id":"ck50aqgm9003js4uf8bseixpr"},{"post_id":"ck50aqgfj000as4uf44i7dyoa","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgma003ks4ufpdwoyi6y"},{"post_id":"ck50aqgfj000as4uf44i7dyoa","tag_id":"ck50aqgl5001hs4ufhm3d93oy","_id":"ck50aqgma003ls4ufw2om3z9k"},{"post_id":"ck50aqgfj000as4uf44i7dyoa","tag_id":"ck50aqgl2001as4ufovuf9qyo","_id":"ck50aqgma003ms4ufrqkzoixg"},{"post_id":"ck50aqgfk000bs4ufh1deno84","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgma003ns4ufrhvxzbax"},{"post_id":"ck50aqgfk000bs4ufh1deno84","tag_id":"ck50aqgl8001ps4ufnvpeqrjc","_id":"ck50aqgma003os4uffuxwk73f"},{"post_id":"ck50aqgfk000bs4ufh1deno84","tag_id":"ck50aqgl8001qs4uf1s1sdy89","_id":"ck50aqgma003ps4uf0qgxekad"},{"post_id":"ck50aqgfl000cs4uf3mq3og8i","tag_id":"ck50aqgl9001rs4uf2sgxanpm","_id":"ck50aqgma003qs4ufh4p8x32i"},{"post_id":"ck50aqgfl000cs4uf3mq3og8i","tag_id":"ck50aqgl9001ss4uf09h2ux1u","_id":"ck50aqgmb003rs4ufz47j65hl"},{"post_id":"ck50aqgfl000cs4uf3mq3og8i","tag_id":"ck50aqgl9001ts4ufff7ssybd","_id":"ck50aqgmb003ss4ufeg4p6e6l"},{"post_id":"ck50aqgfm000ds4ufqgbp44zl","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgmb003ts4ufcmn8cjmk"},{"post_id":"ck50aqgfm000ds4ufqgbp44zl","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmb003us4uf6hffvwvb"},{"post_id":"ck50aqgfo000es4ufuenn9q28","tag_id":"ck50aqgl8001ps4ufnvpeqrjc","_id":"ck50aqgmb003vs4uf1ogm3m3w"},{"post_id":"ck50aqgfo000es4ufuenn9q28","tag_id":"ck50aqgke000us4uf4o17cwjb","_id":"ck50aqgmb003ws4ufeoenj8br"},{"post_id":"ck50aqgfp000fs4uf9onpilok","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgmc003xs4ufgqajdjz5"},{"post_id":"ck50aqgfp000fs4uf9onpilok","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmc003ys4ufbevd7u6d"},{"post_id":"ck50aqgfp000fs4uf9onpilok","tag_id":"ck50aqgl5001hs4ufhm3d93oy","_id":"ck50aqgmc003zs4ufm30jbxzv"},{"post_id":"ck50aqgfr000gs4uf0uzqjis4","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmc0040s4ufqfys8akq"},{"post_id":"ck50aqgfr000gs4uf0uzqjis4","tag_id":"ck50aqgle0022s4ufbgczm27j","_id":"ck50aqgmc0041s4uf3lz3fxvy"},{"post_id":"ck50aqgg0000hs4uf33488baz","tag_id":"ck50aqgke000us4uf4o17cwjb","_id":"ck50aqgmc0042s4uf30p6ce44"},{"post_id":"ck50aqgg4000is4ufvbcnl4is","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmc0043s4ufytqgx6sr"},{"post_id":"ck50aqgg4000is4ufvbcnl4is","tag_id":"ck50aqgl8001ps4ufnvpeqrjc","_id":"ck50aqgmd0044s4uf8dow2g90"},{"post_id":"ck50aqgg4000is4ufvbcnl4is","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgmd0045s4ufpmf394f7"},{"post_id":"ck50aqgg5000js4ufwybzfnwq","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgmd0046s4ufo6ll2trb"},{"post_id":"ck50aqgg5000js4ufwybzfnwq","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmd0047s4ufplq04uil"},{"post_id":"ck50aqgg6000ks4ufqk1lsqql","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgme0048s4uflbfepzqw"},{"post_id":"ck50aqgg8000ls4uf2kyp1tbo","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgme0049s4ufis2tvgny"},{"post_id":"ck50aqgg8000ls4uf2kyp1tbo","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgme004as4ufuu44yvit"},{"post_id":"ck50aqgg8000ms4uf1u947z6f","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgme004bs4uftqie68z9"},{"post_id":"ck50aqgg8000ms4uf1u947z6f","tag_id":"ck50aqgl2001as4ufovuf9qyo","_id":"ck50aqgme004cs4ufdwx3h8w5"},{"post_id":"ck50aqgg9000ns4uf2xflmle5","tag_id":"ck50aqgll002es4ufw1okzlnj","_id":"ck50aqgmf004ds4ufqlk3b0tt"},{"post_id":"ck50aqgg9000ns4uf2xflmle5","tag_id":"ck50aqgkw0011s4uf2rov1kex","_id":"ck50aqgmf004es4ufdnze7n5k"},{"post_id":"ck50aqgga000os4ufdjubhwln","tag_id":"ck50aqgll002gs4uffmel6zem","_id":"ck50aqgmf004fs4ufjwjzxj2h"},{"post_id":"ck50aqggb000ps4ufum1ti0mg","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmg004gs4uf54izqa33"},{"post_id":"ck50aqggb000ps4ufum1ti0mg","tag_id":"ck50aqgkr000ys4uft9hulesh","_id":"ck50aqgmg004hs4uf2olauihp"},{"post_id":"ck50aqggc000qs4ufy5ymzwcf","tag_id":"ck50aqgln002js4ufiqtipnw3","_id":"ck50aqgmg004is4uffs06pyev"},{"post_id":"ck50aqggc000qs4ufy5ymzwcf","tag_id":"ck50aqgln002ks4ufbbexwgmu","_id":"ck50aqgmg004js4ufl5pj0fn4"},{"post_id":"ck50aqggc000qs4ufy5ymzwcf","tag_id":"ck50aqgkw0011s4uf2rov1kex","_id":"ck50aqgmg004ks4ufmwsfgahb"},{"post_id":"ck50aqggd000rs4ufcy5tohmj","tag_id":"ck50aqgkm000vs4ufatb2ubgk","_id":"ck50aqgmh004ls4ufyzz7b3mt"},{"post_id":"ck50aqggd000rs4ufcy5tohmj","tag_id":"ck50aqgl20019s4ufudlqyahg","_id":"ck50aqgmh004ms4ufhhro65s2"},{"post_id":"ck50aqggd000rs4ufcy5tohmj","tag_id":"ck50aqgl2001as4ufovuf9qyo","_id":"ck50aqgmh004ns4ufqyz8wyoq"},{"post_id":"ck50aqgge000ss4ufrz376ohw","tag_id":"ck50aqgl10017s4ufr15d4p22","_id":"ck50aqgmh004os4ufkls9bhk3"},{"post_id":"ck50aqgge000ss4ufrz376ohw","tag_id":"ck50aqglp002qs4uft3mb7ia0","_id":"ck50aqgmh004ps4ufknoz43wv"},{"post_id":"ck50aqgge000ss4ufrz376ohw","tag_id":"ck50aqglq002rs4ufj0jcrccb","_id":"ck50aqgmh004qs4ufa6s2cxc7"},{"post_id":"ck50aqgge000ts4uf3lr5fhme","tag_id":"ck50aqgll002gs4uffmel6zem","_id":"ck50aqgmh004rs4ufo9djzbl7"}],"Tag":[{"name":"Blogs","_id":"ck50aqgke000us4uf4o17cwjb"},{"name":"ALM","_id":"ck50aqgkm000vs4ufatb2ubgk"},{"name":"Database","_id":"ck50aqgkn000ws4uf6oujiq8j"},{"name":"SQL","_id":"ck50aqgko000xs4uf8k2vds1a"},{"name":"Compare","_id":"ck50aqgkr000ys4uft9hulesh"},{"name":"Products","_id":"ck50aqgkw0011s4uf2rov1kex"},{"name":"dotNet","_id":"ck50aqgky0012s4uf34d5f1c6"},{"name":"c#","_id":"ck50aqgkz0013s4ufff5yszim"},{"name":"vb","_id":"ck50aqgl00014s4ufb8bsoarx"},{"name":"Testing","_id":"ck50aqgl10017s4ufr15d4p22"},{"name":"DevOps","_id":"ck50aqgl20019s4ufudlqyahg"},{"name":"TFS","_id":"ck50aqgl2001as4ufovuf9qyo"},{"name":"Goal Tracker","_id":"ck50aqgl3001cs4uf03qiw8al"},{"name":"NuGet","_id":"ck50aqgl4001gs4ufgn4o1jz1"},{"name":"PowerShell","_id":"ck50aqgl5001hs4ufhm3d93oy"},{"name":"git","_id":"ck50aqgl8001ps4ufnvpeqrjc"},{"name":"git-tf","_id":"ck50aqgl8001qs4uf1s1sdy89"},{"name":"Lifestyle","_id":"ck50aqgl9001rs4uf2sgxanpm"},{"name":"Vegan","_id":"ck50aqgl9001ss4uf09h2ux1u"},{"name":"Health","_id":"ck50aqgl9001ts4ufff7ssybd"},{"name":"Security","_id":"ck50aqgle0022s4ufbgczm27j"},{"name":"Time Tracker","_id":"ck50aqgll002es4ufw1okzlnj"},{"name":"Politics","_id":"ck50aqgll002gs4uffmel6zem"},{"name":"3WInc","_id":"ck50aqgln002js4ufiqtipnw3"},{"name":"Corporation","_id":"ck50aqgln002ks4ufbbexwgmu"},{"name":"Soap Box","_id":"ck50aqglp002qs4uft3mb7ia0"},{"name":"User Experience (UX)","_id":"ck50aqglq002rs4ufj0jcrccb"}]}}